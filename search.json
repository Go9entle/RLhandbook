[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "强化学习基础",
    "section": "",
    "text": "在马尔可夫过程的基础上加入奖励函数r和折扣因子\\gamma,就可以得到马尔可夫奖励过程（Markov Reward Process, MRP）。一个马尔可夫奖励过程由\\langle S,\\mathcal{P},r,\\gamma\\rangle构成。\n\n\n在一个MRP中，从第t时刻状态S_t开始，直到终止状态时，所有奖励的衰减之和称为回报G_t（Return），公式如下：\n\nG_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\dots=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k},\n 其中，R_t表示在时刻t获得的奖励。\n\n\n\n在MRP中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（Value）。所有状态的价值就组成了价值函数（Value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成\n\n\\begin{aligned}\nV(s)&=\\mathbb{E}\\left[G_t\\mid S_t=s\\right]\\\\\n&= \\mathbb{E}\\left[ R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} +\\dots\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma (R_{t+1}+\\gamma R_{t+2} +\\dots)\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma G_{t+1}\\mid S_t=s \\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma V(S_{t+1})\\mid S_t=s \\right]\n\\end{aligned}\n\n在上式的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即\\mathbb{E}\\left[ R_t\\mid S_t=s \\right]=r(s);另一方面，等式中剩余部分\\mathbb{E}\\left[\\gamma V(S_{t+1})\\mid S_t=s\\right]可以根据从状态s出发的转移概率得到，即\n\nV(s)=r(s)+\\gamma \\sum_{s'\\in \\mathcal{S}}p(s'\\mid s)V(s')\n\n上式就是MRP中非常有名的贝尔曼方程（Bellman equation），对每一个状态都成立。若通过矩阵运算可以得到以下解析解：\n\n\\begin{aligned}\n\\mathcal{V}&=\\mathcal{R}+\\gamma\\mathcal{P}\\mathcal{V}\\\\\n\\mathcal{V}&=(I-\\gamma\\mathcal{P})^{-1}\\mathcal{R}\n\\end{aligned}\n 其中\\mathcal{V,R,P}均为向量或矩阵。以上解析解的计算复杂度是O(n^3),其中n是状态个数，因此这种方法只适用于很小的MRP。求解较大规模的MRP奖励过程中的价值函数时，可以使用动态规划、蒙特卡洛、时序差分，这些方法将在之后的章节介绍。\n接下来编写代码来实现求解价值函数的解析解方法，并据此计算该马尔可夫奖励过程中所有状态的价值。\n\ndef compute(P, rewards, gamma, states_num):\n    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n                   rewards)\n    return value\n\n\n\n\n\n如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（MDP）。我们将这个来自外界的刺激称为智能体（Agent）的动作，在MRP的基础上加入动作，就得到了MDP。MDP有元组\\langle \\mathcal{S,A,P},r,\\gamma\\rangle构成。\n\n\n智能体的策略（Policy）通常用字母\\pi表示。策略\\pi(a|s)=P(A_t=a|S_t=s)是一个函数，表示在输入状态s情况下采取动作a的概率。当一个策略是确定性策略（Deterministic policy）时，它在每个状态只输出一个确定性的动作，即只有该动作的概率为1,其他动作的概率为0; 当一个策略是随机性策略（Stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。\n在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。回顾一下在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。这很好理解，因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。\n\n\n\n我们用V^{\\pi}(s)表示在MDP中基于策略\\pi的状态价值函数（State-value function），定义在从状态s出发遵循策略\\pi能获得的期望回报，数学表达为\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]\n\n\n\n\n不同于MRP，MDP中由于动作的存在我们定义一个动作价值函数（Action-value function）。我们用Q^{\\pi}(s,a)表示在MDP遵循策略\\pi时，对当前状态s执行动作a得到的期望回报：\n\nQ^{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]\n\n状态价值函数和动作价值函数之间的关系：\n在使用策略\\pi时，状态s的价值等于在该状态下基于策略\\pi采取所有动作的概率与相应价值相乘再求和的结果：\n\nV^{\\pi}(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)Q^{\\pi}(s,a)\n\n在使用策略\\pi时，状态s下采取动作a的价值等于即时奖励加上经过衰减后的所有可能的下一个状态转移概率与相应价值的乘积：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n\n\n\n在贝尔曼方程中加上期望二字是为了与接下来的贝尔曼最优方程进行区分。我们通过加单推到就可以分别得到两个价值函数的贝尔曼期望方程（Bellman Expect Equation）：\n\n\\begin{aligned}\nV^{\\pi}(s)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma V^{\\pi}(S_{t+1})|S_t=s\\right]\\\\\n&=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left( r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi}(s')\\right)\\\\\n\\\\\nQ^{\\pi}(s,a)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma Q^{\\pi}(S_{t+1},A_{t+1}|S_t=s, A_t=a)\\right]\\\\\n&=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')Q^{\\pi}(s',a)\n\\end{aligned}\n\n价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的。\n现在我们考虑一个MDP的简单例子如 图 1 ，其中每个绿色圆圈代表一个状态，一共有s_1\\sim s_5这5个状态。黑色实线箭头代表可以采取的动作，黄色小圆圈代表动作。需要注意的是，并非在每个状态都能采取所有动作，例如在s_1,智能体只能采取”保持s_1“和”前往s_2“这两个动作，无法采取其他动作。\n每个黄色小圆圈旁的红色数字代表在某个状态下采取某个动作能获得的奖励。虚线箭头代表采取动作后可能转移到的状态，箭头边上的带方框的数字代表转移概率，如果没有数字则表示转移概率为1.例如，在s_2下，如果采取动作”前往s_3“就能得到奖励-2,并且以概率1转移到s_3;在s_4下，如果采取”概率前往”这个动作，就能得到奖励1,并且会分别以概率0.2,0.4,0.4转移到s_2,s_3或s_4.\n\n\n\n\n\n\n图 1： MDP一个简单例子\n\n\n\n接下来我们编写代码来表示 图 1 中的MDP，并定义两个策略，第一个策略是一个完全随机策略，即在每个状态下，智能体会以同样的概率选取它可能采取的动作。例如，在s_1下智能体会以0.5,0.5的概率选取动作”保持s_1“和”前往s_2“.第二个策略是一个提前设定的策略。\n\nimport numpy as np\nS = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\nA = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n# 状态转移函数\nP = {\n    \"s1-保持s1-s1\": 1.0,\n    \"s1-前往s2-s2\": 1.0,\n    \"s2-前往s1-s1\": 1.0,\n    \"s2-前往s3-s3\": 1.0,\n    \"s3-前往s4-s4\": 1.0,\n    \"s3-前往s5-s5\": 1.0,\n    \"s4-前往s5-s5\": 1.0,\n    \"s4-概率前往-s2\": 0.2,\n    \"s4-概率前往-s3\": 0.4,\n    \"s4-概率前往-s4\": 0.4,\n}\n# 奖励函数\nR = {\n    \"s1-保持s1\": -1,\n    \"s1-前往s2\": 0,\n    \"s2-前往s1\": -1,\n    \"s2-前往s3\": -2,\n    \"s3-前往s4\": -2,\n    \"s3-前往s5\": 0,\n    \"s4-前往s5\": 10,\n    \"s4-概率前往\": 1,\n}\ngamma = 0.5  # 折扣因子\nMDP = (S, A, P, R, gamma)\n\n# 策略1,随机策略\nPi_1 = {\n    \"s1-保持s1\": 0.5,\n    \"s1-前往s2\": 0.5,\n    \"s2-前往s1\": 0.5,\n    \"s2-前往s3\": 0.5,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.5,\n    \"s4-概率前往\": 0.5,\n}\n# 策略2\nPi_2 = {\n    \"s1-保持s1\": 0.6,\n    \"s1-前往s2\": 0.4,\n    \"s2-前往s1\": 0.3,\n    \"s2-前往s3\": 0.7,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.1,\n    \"s4-概率前往\": 0.9,\n}\n\n\n# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\ndef join(str1, str2):\n    return str1 + '-' + str2\n\n接下来我们想要计算该MDP下，一个策略\\pi的状态价值函数。我们现有的工具是MRP的解析解方法，一个自然的想法是给定一个MDP和一个策略\\pi,我们是否可以将其转化为一个MRP？答案是肯定的，我们可以将策略的动作选择进行边缘化（Marginalization），就可以得到没有动作的MRP了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个MRP在该状态下的奖励，即： \nr'(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)r(s,a)\n\n同理，我们计算采取动作的概率与使s转移到s'的概率的乘积，再将这些乘积相加，其和就是一个MRP的状态从s转移至s'的概率：\n\nP'(s'|s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\n\n于是，我们构建得到了一个MRP:\\langle \\mathcal{S},P',r',\\gamma\\rangle.根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。\n接下来，我们用代码实现该方法，计算用随即策略Pi_1时的状态价值函数，为了简单起见，我们将直接给出转化后的MRP的状态转移矩阵和奖励函数。\n\ngamma = 0.5\n# 转化后的MRP的状态转移矩阵\nP_from_mdp_to_mrp = [\n    [0.5, 0.5, 0.0, 0.0, 0.0],\n    [0.5, 0.0, 0.5, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.5, 0.5],\n    [0.0, 0.1, 0.2, 0.2, 0.5],\n    [0.0, 0.0, 0.0, 0.0, 1.0],\n]\nP_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\nR_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n\nV = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\nprint(\"MDP中每个状态价值分别为\\n\", V)\n\nMDP中每个状态价值分别为\n [[-1.22555411]\n [-1.67666232]\n [ 0.51890482]\n [ 6.0756193 ]\n [ 0.        ]]\n\n\n知道了状态价值函数V^{\\pi}(s)后，我们可以计算动作价值函数Q^{\\pi}(s,a).例如(s_4,\\text{概率前往})的动作价值为2.152，根据以下公式可以计算得到：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n这个 MRP 解析解的方法在状态动作集合比较大的时候不是很适用，那有没有其他的方法呢？ 章节 2 将介绍用动态规划算法来计算得到价值函数。 章节 1.3 将介绍用蒙特卡洛方法来近似估计这个价值函数，用蒙特卡洛方法的好处在于我们不需要知道 MDP 的状态转移函数和奖励函数，它可以得到一个近似值，并且采样数越多越准确。\n\n\n\n\n蒙特卡洛方法（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用MC方法时我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。一个简单的例子是用蒙特卡洛方法来计算圆的面积。例如，在正方形内部随机产生若干个点，细数落在圆中点的个数，圆的面积与正方形面积之比就等于圆中点的个数与正方形中点的个数之比。如果我们随机产生的点的个数越多，计算得到圆的面积就越接近于真实的圆的面积。\n我们现在介绍如何用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数。回忆一下，一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望就可以了，公式如下：\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[ G_t|S_t=s \\right]\\approx \\frac{1}{N}\\sum_{i=1}^N G_t^{(i)}\n\n在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。我们介绍的蒙特卡洛价值估计方法会在该状态每一次出现时计算它的回报。还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。假设我们现在用策略\\pi从s开始采样序列，据此来计算状态价值。我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示。\n\n使用策略\\pi 采样若干条序列： \ns_0^{(i)}\\overset{a_0^{(i)}}{\\rightarrow} r_0^{(i)},s_1^{(i)}\\overset{a_1^{(i)}}{\\rightarrow} r_1^{(i)},\ns_2^{(i)}\\overset{a_2^{(i)}}{\\rightarrow},...,\\overset{a_{T-1}^{(i)}}{\\rightarrow} r_{T-1}^{(i)},s_T^{(i)}\n\n对每一条序列中的每一时间步t的状态s进行以下操作：\n\n更新状态s的计数器N(s)\\leftarrow N(s)+1;\n更新状态s的总回报M(s)\\leftarrow M(s)+G_t;\n\n每一个状态的价值被估计为回报的平均值V(s)=\\=M(s)/N(s).\n\n根据大数定律，当N(s)\\rightarrow \\infty,有V(s)\\rightarrow V^{\\pi}(s). 计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态s和对应回报G,进行如下计算：\n\nN(s)\\leftarrow N(s)+1\nV(s)\\leftarrow V(s)+\\frac{G-V(s)}{N(s)}\n\n接下来我们用代码定义一个采样函数，采样函数需要遵守状态转移矩阵和相应策略，每次将(s,a,r,s_next)元组放入序列中，直到到达终止序列。然后我们通过该函数，用随即策略在 图 1 的MDP中随机采样几条序列。\n\ndef sample(MDP, Pi, timestep_max, number):\n    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n    S, A, P, R, gamma = MDP\n    episodes = []\n    for _ in range(number):\n        episode = []\n        timestep = 0\n        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n        # 当前状态为终止状态或者时间步太长时,一次采样结束\n        while s != \"s5\" and timestep &lt;= timestep_max:\n            timestep += 1\n            rand, temp = np.random.rand(), 0\n            # 在状态s下根据策略选择动作\n            for a_opt in A:\n                temp += Pi.get(join(s, a_opt), 0)\n                if temp &gt; rand:\n                    a = a_opt\n                    r = R.get(join(s, a), 0)\n                    break\n            rand, temp = np.random.rand(), 0\n            # 根据状态转移概率得到下一个状态s_next\n            for s_opt in S:\n                temp += P.get(join(join(s, a), s_opt), 0)\n                if temp &gt; rand:\n                    s_next = s_opt\n                    break\n            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n            s = s_next  # s_next变成当前状态,开始接下来的循环\n        episodes.append(episode)\n    return episodes\n\n\n# 采样5次,每个序列最长不超过20步\nepisodes = sample(MDP, Pi_1, 20, 5)\nprint('第一条序列\\n', episodes[0])\nprint('第二条序列\\n', episodes[1])\nprint('第五条序列\\n', episodes[4])\n\n第一条序列\n [('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n第二条序列\n [('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's3'), ('s3', '前往s5', 0, 's5')]\n第五条序列\n [('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '前往s5', 10, 's5')]\n\n\n\n# 对所有采样序列计算所有状态的价值\ndef MC(episodes, V, N, gamma):\n    for episode in episodes:\n        G = 0\n        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算\n            (s, a, r, s_next) = episode[i]\n            G = r + gamma * G\n            N[s] = N[s] + 1\n            V[s] = V[s] + (G - V[s]) / N[s]\n\n\ntimestep_max = 20\n# 采样1000次,可以自行修改\nepisodes = sample(MDP, Pi_1, timestep_max, 1000)\ngamma = 0.5\nV = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nN = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nMC(episodes, V, N, gamma)\nprint(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)\n\n使用蒙特卡洛方法计算MDP的状态价值为\n {'s1': -1.2433556502656373, 's2': -1.6687838281541936, 's3': 0.5172596878892808, 's4': 6.24924512869253, 's5': 0}\n\n\n可以看到用蒙特卡洛方法估计得到的状态价值和我们用 MRP 解析解得到的状态价值是很接近的。这得益于我们采样了比较多的序列，感兴趣的读者可以尝试修改采样次数，然后观察蒙特卡洛方法的结果。\n\n\n\n强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。我们首先定义策略之间的偏序关系：当且仅当对于任意状态s都有V^{\\pi}(s)&gt; V^{\\pi'}(s), 记\\pi&gt;\\pi'. 于是在有限状态和动作集合的MDP中至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是最优策略（Optimal policy）。最优策略可能有很多个，我们都将其表示为\\pi^*(s).\n最优策略都有相同的状态价值函数，我们称之为最有状态价值函数，表示为： \nV^*(s)=\\max_{\\pi} V^{\\pi}(s),\\forall s\\in\\mathcal{S}\n\n同理我们定义最优动作价值函数： \nQ^*(s,a)=\\max_{\\pi}Q^{\\pi}(s,a),\\forall s\\in\\mathcal{S},a\\in\\mathcal{A}\n\n为了使Q^*(s,a)最大，我们需要在当前状态动作对(s,a)之后都执行最优策略，于是我们得到了最有状态价值函数和最优动作价值函数之间的关系：\n\nQ^\\pi{s,a}=r(s,a)+\\gamma\\sum_{s\\in\\mathcal{S}}P(s'|s,a)V^*(s')\n\n这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值： \nV^*(s)=\\max_{a\\in\\mathcal{A}}Q^*(s,a)\n\n\n\n根据V^*(s)和Q^*(s,a)的关系，我们可以得到贝尔曼最优方程（Bellman optimality equation）：\n\n\\begin{aligned}\nV^*(s)&=\\max_{a\\in\\mathcal{A}}\\left\\{ r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^*(s')\\right\\}\\\\\nQ^*(s,a)=r(s,a)+\\gamma\\sum_{s'\\in \\mathcal{S}}p(s'|s,a)\\max_{a'\\in\\mathcal{A}}Q^*(s',a')\n\\end{aligned}\n\n章节 2 将介绍如何用动态规划算法得到最优策略。\n\n\n\n\n马尔可夫决策过程是强化学习中的基础概念，强化学习中的环境就是一个马尔可夫决策过程。我们接下来将要介绍的强化学习算法通常都是在求解马尔可夫决策过程中的最优策略。"
  },
  {
    "objectID": "index.html#马尔可夫决策过程",
    "href": "index.html#马尔可夫决策过程",
    "title": "强化学习基础",
    "section": "",
    "text": "在马尔可夫过程的基础上加入奖励函数r和折扣因子\\gamma,就可以得到马尔可夫奖励过程（Markov Reward Process, MRP）。一个马尔可夫奖励过程由\\langle S,\\mathcal{P},r,\\gamma\\rangle构成。\n\n\n在一个MRP中，从第t时刻状态S_t开始，直到终止状态时，所有奖励的衰减之和称为回报G_t（Return），公式如下：\n\nG_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\dots=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k},\n 其中，R_t表示在时刻t获得的奖励。\n\n\n\n在MRP中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（Value）。所有状态的价值就组成了价值函数（Value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成\n\n\\begin{aligned}\nV(s)&=\\mathbb{E}\\left[G_t\\mid S_t=s\\right]\\\\\n&= \\mathbb{E}\\left[ R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} +\\dots\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma (R_{t+1}+\\gamma R_{t+2} +\\dots)\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma G_{t+1}\\mid S_t=s \\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma V(S_{t+1})\\mid S_t=s \\right]\n\\end{aligned}\n\n在上式的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即\\mathbb{E}\\left[ R_t\\mid S_t=s \\right]=r(s);另一方面，等式中剩余部分\\mathbb{E}\\left[\\gamma V(S_{t+1})\\mid S_t=s\\right]可以根据从状态s出发的转移概率得到，即\n\nV(s)=r(s)+\\gamma \\sum_{s'\\in \\mathcal{S}}p(s'\\mid s)V(s')\n\n上式就是MRP中非常有名的贝尔曼方程（Bellman equation），对每一个状态都成立。若通过矩阵运算可以得到以下解析解：\n\n\\begin{aligned}\n\\mathcal{V}&=\\mathcal{R}+\\gamma\\mathcal{P}\\mathcal{V}\\\\\n\\mathcal{V}&=(I-\\gamma\\mathcal{P})^{-1}\\mathcal{R}\n\\end{aligned}\n 其中\\mathcal{V,R,P}均为向量或矩阵。以上解析解的计算复杂度是O(n^3),其中n是状态个数，因此这种方法只适用于很小的MRP。求解较大规模的MRP奖励过程中的价值函数时，可以使用动态规划、蒙特卡洛、时序差分，这些方法将在之后的章节介绍。\n接下来编写代码来实现求解价值函数的解析解方法，并据此计算该马尔可夫奖励过程中所有状态的价值。\n\ndef compute(P, rewards, gamma, states_num):\n    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n                   rewards)\n    return value\n\n\n\n\n\n如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（MDP）。我们将这个来自外界的刺激称为智能体（Agent）的动作，在MRP的基础上加入动作，就得到了MDP。MDP有元组\\langle \\mathcal{S,A,P},r,\\gamma\\rangle构成。\n\n\n智能体的策略（Policy）通常用字母\\pi表示。策略\\pi(a|s)=P(A_t=a|S_t=s)是一个函数，表示在输入状态s情况下采取动作a的概率。当一个策略是确定性策略（Deterministic policy）时，它在每个状态只输出一个确定性的动作，即只有该动作的概率为1,其他动作的概率为0; 当一个策略是随机性策略（Stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。\n在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。回顾一下在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。这很好理解，因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。\n\n\n\n我们用V^{\\pi}(s)表示在MDP中基于策略\\pi的状态价值函数（State-value function），定义在从状态s出发遵循策略\\pi能获得的期望回报，数学表达为\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]\n\n\n\n\n不同于MRP，MDP中由于动作的存在我们定义一个动作价值函数（Action-value function）。我们用Q^{\\pi}(s,a)表示在MDP遵循策略\\pi时，对当前状态s执行动作a得到的期望回报：\n\nQ^{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]\n\n状态价值函数和动作价值函数之间的关系：\n在使用策略\\pi时，状态s的价值等于在该状态下基于策略\\pi采取所有动作的概率与相应价值相乘再求和的结果：\n\nV^{\\pi}(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)Q^{\\pi}(s,a)\n\n在使用策略\\pi时，状态s下采取动作a的价值等于即时奖励加上经过衰减后的所有可能的下一个状态转移概率与相应价值的乘积：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n\n\n\n在贝尔曼方程中加上期望二字是为了与接下来的贝尔曼最优方程进行区分。我们通过加单推到就可以分别得到两个价值函数的贝尔曼期望方程（Bellman Expect Equation）：\n\n\\begin{aligned}\nV^{\\pi}(s)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma V^{\\pi}(S_{t+1})|S_t=s\\right]\\\\\n&=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left( r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi}(s')\\right)\\\\\n\\\\\nQ^{\\pi}(s,a)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma Q^{\\pi}(S_{t+1},A_{t+1}|S_t=s, A_t=a)\\right]\\\\\n&=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')Q^{\\pi}(s',a)\n\\end{aligned}\n\n价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的。\n现在我们考虑一个MDP的简单例子如@fig-mdp ，其中每个绿色圆圈代表一个状态，一共有s_1\\sim s_5这5个状态。黑色实线箭头代表可以采取的动作，黄色小圆圈代表动作。需要注意的是，并非在每个状态都能采取所有动作，例如在s_1,智能体只能采取”保持s_1“和”前往s_2“这两个动作，无法采取其他动作。\n每个黄色小圆圈旁的红色数字代表在某个状态下采取某个动作能获得的奖励。虚线箭头代表采取动作后可能转移到的状态，箭头边上的带方框的数字代表转移概率，如果没有数字则表示转移概率为1.例如，在s_2下，如果采取动作”前往s_3“就能得到奖励-2,并且以概率1转移到s_3;在s_4下，如果采取”概率前往”这个动作，就能得到奖励1,并且会分别以概率0.2,0.4,0.4转移到s_2,s_3或s_4.\n\n\n\n\n\n\n图 1： MDP一个简单例子\n\n\n\n接下来我们编写代码来表示@fig-mdp 中的MDP，并定义两个策略，第一个策略是一个完全随机策略，即在每个状态下，智能体会以同样的概率选取它可能采取的动作。例如，在s_1下智能体会以0.5,0.5的概率选取动作”保持s_1“和”前往s_2“.第二个策略是一个提前设定的策略。\n\nimport numpy as np\nS = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\nA = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n# 状态转移函数\nP = {\n    \"s1-保持s1-s1\": 1.0,\n    \"s1-前往s2-s2\": 1.0,\n    \"s2-前往s1-s1\": 1.0,\n    \"s2-前往s3-s3\": 1.0,\n    \"s3-前往s4-s4\": 1.0,\n    \"s3-前往s5-s5\": 1.0,\n    \"s4-前往s5-s5\": 1.0,\n    \"s4-概率前往-s2\": 0.2,\n    \"s4-概率前往-s3\": 0.4,\n    \"s4-概率前往-s4\": 0.4,\n}\n# 奖励函数\nR = {\n    \"s1-保持s1\": -1,\n    \"s1-前往s2\": 0,\n    \"s2-前往s1\": -1,\n    \"s2-前往s3\": -2,\n    \"s3-前往s4\": -2,\n    \"s3-前往s5\": 0,\n    \"s4-前往s5\": 10,\n    \"s4-概率前往\": 1,\n}\ngamma = 0.5  # 折扣因子\nMDP = (S, A, P, R, gamma)\n\n# 策略1,随机策略\nPi_1 = {\n    \"s1-保持s1\": 0.5,\n    \"s1-前往s2\": 0.5,\n    \"s2-前往s1\": 0.5,\n    \"s2-前往s3\": 0.5,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.5,\n    \"s4-概率前往\": 0.5,\n}\n# 策略2\nPi_2 = {\n    \"s1-保持s1\": 0.6,\n    \"s1-前往s2\": 0.4,\n    \"s2-前往s1\": 0.3,\n    \"s2-前往s3\": 0.7,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.1,\n    \"s4-概率前往\": 0.9,\n}\n\n\n# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\ndef join(str1, str2):\n    return str1 + '-' + str2\n\n接下来我们想要计算该MDP下，一个策略\\pi的状态价值函数。我们现有的工具是MRP的解析解方法，一个自然的想法是给定一个MDP和一个策略\\pi,我们是否可以将其转化为一个MRP？答案是肯定的，我们可以将策略的动作选择进行边缘化（Marginalization），就可以得到没有动作的MRP了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个MRP在该状态下的奖励，即： \nr'(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)r(s,a)\n\n同理，我们计算采取动作的概率与使s转移到s'的概率的乘积，再将这些乘积相加，其和就是一个MRP的状态从s转移至s'的概率：\n\nP'(s'|s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\n\n于是，我们构建得到了一个MRP:\\langle \\mathcal{S},P',r',\\gamma\\rangle.根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。\n接下来，我们用代码实现该方法，计算用随即策略Pi_1时的状态价值函数，为了简单起见，我们将直接给出转化后的MRP的状态转移矩阵和奖励函数。\n\ngamma = 0.5\n# 转化后的MRP的状态转移矩阵\nP_from_mdp_to_mrp = [\n    [0.5, 0.5, 0.0, 0.0, 0.0],\n    [0.5, 0.0, 0.5, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.5, 0.5],\n    [0.0, 0.1, 0.2, 0.2, 0.5],\n    [0.0, 0.0, 0.0, 0.0, 1.0],\n]\nP_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\nR_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n\nV = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\nprint(\"MDP中每个状态价值分别为\\n\", V)\n\nMDP中每个状态价值分别为\n [[-1.22555411]\n [-1.67666232]\n [ 0.51890482]\n [ 6.0756193 ]\n [ 0.        ]]\n\n\n知道了状态价值函数V^{\\pi}(s)后，我们可以计算动作价值函数Q^{\\pi}(s,a).例如(s_4,\\text{概率前往})的动作价值为2.152，根据以下公式可以计算得到：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n这个 MRP 解析解的方法在状态动作集合比较大的时候不是很适用，那有没有其他的方法呢？第 章节 1.2 章将介绍用动态规划算法来计算得到价值函数。章节 1.1.3 节将介绍用蒙特卡洛方法来近似估计这个价值函数，用蒙特卡洛方法的好处在于我们不需要知道 MDP 的状态转移函数和奖励函数，它可以得到一个近似值，并且采样数越多越准确。\n\n\n\n\n蒙特卡洛方法（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用MC方法时我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。一个简单的例子是用蒙特卡洛方法来计算圆的面积。例如，在正方形内部随机产生若干个点，细数落在圆中点的个数，圆的面积与正方形面积之比就等于圆中点的个数与正方形中点的个数之比。如果我们随机产生的点的个数越多，计算得到圆的面积就越接近于真实的圆的面积。\n我们现在介绍如何用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数。回忆一下，一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望就可以了，公式如下：\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[ G_t|S_t=s \\right]\\approx \\frac{1}{N}\\sum_{i=1}^N G_t^{(i)}\n\n在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。我们介绍的蒙特卡洛价值估计方法会在该状态每一次出现时计算它的回报。还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。假设我们现在用策略\\pi从s开始采样序列，据此来计算状态价值。我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示。\n\n使用策略\\pi 采样若干条序列： \ns_0^{(i)}\\overset{a_0^{(i)}}{\\rightarrow} r_0^{(i)},s_1^{(i)}\\overset{a_1^{(i)}}{\\rightarrow} r_1^{(i)},\ns_2^{(i)}\\overset{a_2^{(i)}}{\\rightarrow},...,\\overset{a_{T-1}^{(i)}}{\\rightarrow} r_{T-1}^{(i)},s_T^{(i)}\n\n对每一条序列中的每一时间步t的状态s进行以下操作：\n\n更新状态s的计数器N(s)\\leftarrow N(s)+1;\n更新状态s的总回报M(s)\\leftarrow M(s)+G_t;\n\n每一个状态的价值被估计为回报的平均值V(s)=\\=M(s)/N(s).\n\n根据大数定律，当N(s)\\rightarrow \\infty,有V(s)\\rightarrow V^{\\pi}(s). 计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态s和对应回报G,进行如下计算：\n\nN(s)\\leftarrow N(s)+1\nV(s)\\leftarrow V(s)+\\frac{G-V(s)}{N(s)}\n\n接下来我们用代码定义一个采样函数，采样函数需要遵守状态转移矩阵和相应策略，每次将(s,a,r,s_next)元组放入序列中，直到到达终止序列。然后我们通过该函数，用随即策略在@fig-mdp 的MDP中随机采样几条序列。\n\ndef sample(MDP, Pi, timestep_max, number):\n    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n    S, A, P, R, gamma = MDP\n    episodes = []\n    for _ in range(number):\n        episode = []\n        timestep = 0\n        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n        # 当前状态为终止状态或者时间步太长时,一次采样结束\n        while s != \"s5\" and timestep &lt;= timestep_max:\n            timestep += 1\n            rand, temp = np.random.rand(), 0\n            # 在状态s下根据策略选择动作\n            for a_opt in A:\n                temp += Pi.get(join(s, a_opt), 0)\n                if temp &gt; rand:\n                    a = a_opt\n                    r = R.get(join(s, a), 0)\n                    break\n            rand, temp = np.random.rand(), 0\n            # 根据状态转移概率得到下一个状态s_next\n            for s_opt in S:\n                temp += P.get(join(join(s, a), s_opt), 0)\n                if temp &gt; rand:\n                    s_next = s_opt\n                    break\n            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n            s = s_next  # s_next变成当前状态,开始接下来的循环\n        episodes.append(episode)\n    return episodes\n\n\n# 采样5次,每个序列最长不超过20步\nepisodes = sample(MDP, Pi_1, 20, 5)\nprint('第一条序列\\n', episodes[0])\nprint('第二条序列\\n', episodes[1])\nprint('第五条序列\\n', episodes[4])\n\n第一条序列\n [('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n第二条序列\n [('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n第五条序列\n [('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n\n\n\n# 对所有采样序列计算所有状态的价值\ndef MC(episodes, V, N, gamma):\n    for episode in episodes:\n        G = 0\n        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算\n            (s, a, r, s_next) = episode[i]\n            G = r + gamma * G\n            N[s] = N[s] + 1\n            V[s] = V[s] + (G - V[s]) / N[s]\n\n\ntimestep_max = 20\n# 采样1000次,可以自行修改\nepisodes = sample(MDP, Pi_1, timestep_max, 1000)\ngamma = 0.5\nV = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nN = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nMC(episodes, V, N, gamma)\nprint(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)\n\n使用蒙特卡洛方法计算MDP的状态价值为\n {'s1': -1.2199909859035727, 's2': -1.6944299913955496, 's3': 0.4373151566032693, 's4': 5.962755391182896, 's5': 0}\n\n\n可以看到用蒙特卡洛方法估计得到的状态价值和我们用 MRP 解析解得到的状态价值是很接近的。这得益于我们采样了比较多的序列，感兴趣的读者可以尝试修改采样次数，然后观察蒙特卡洛方法的结果。\n\n\n\n强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。我们首先定义策略之间的偏序关系：当且仅当对于任意状态s都有V^{\\pi}(s)&gt; V^{\\pi'}(s), 记\\pi&gt;\\pi'. 于是在有限状态和动作集合的MDP中至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是最优策略（Optimal policy）。最优策略可能有很多个，我们都将其表示为\\pi^*(s).\n最优策略都有相同的状态价值函数，我们称之为最有状态价值函数，表示为： \nV^*(s)=\\max_{\\pi} V^{\\pi}(s),\\forall s\\in\\mathcal{S}\n\n同理我们定义最优动作价值函数： \nQ^*(s,a)=\\max_{\\pi}Q^{\\pi}(s,a),\\forall s\\in\\mathcal{S},a\\in\\mathcal{A}\n\n为了使Q^*(s,a)最大，我们需要在当前状态动作对(s,a)之后都执行最优策略，于是我们得到了最有状态价值函数和最优动作价值函数之间的关系：\n\nQ^\\pi{s,a}=r(s,a)+\\gamma\\sum_{s\\in\\mathcal{S}}P(s'|s,a)V^*(s')\n\n这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值： \nV^*(s)=\\max_{a\\in\\mathcal{A}}Q^*(s,a)\n\n\n\n根据V^*(s)和Q^*(s,a)的关系，我们可以得到贝尔曼最优方程（Bellman optimality equation）：\n\n\\begin{aligned}\nV^*(s)&=\\max_{a\\in\\mathcal{A}}\\left\\{ r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^*(s')\\right\\}\\\\\nQ^*(s,a)=r(s,a)+\\gamma\\sum_{s'\\in \\mathcal{S}}p(s'|s,a)\\max_{a'\\in\\mathcal{A}}Q^*(s',a')\n\\end{aligned}\n\n第@sec-dynpa 章将介绍如何用动态规划算法得到最优策略。\n\n\n\n\n马尔可夫决策过程是强化学习中的基础概念，强化学习中的环境就是一个马尔可夫决策过程。我们接下来将要介绍的强化学习算法通常都是在求解马尔可夫决策过程中的最优策略。"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, everyone!"
  },
  {
    "objectID": "index.html#马尔可夫奖励过程",
    "href": "index.html#马尔可夫奖励过程",
    "title": "强化学习基础",
    "section": "",
    "text": "在马尔可夫过程的基础上加入奖励函数r和折扣因子\\gamma,就可以得到马尔可夫奖励过程（Markov Reward Process, MRP）。一个马尔可夫奖励过程由\\langle S,\\mathcal{P},r,\\gamma\\rangle构成。\n\n\n在一个MRP中，从第t时刻状态S_t开始，直到终止状态时，所有奖励的衰减之和称为回报G_t（Return），公式如下：\n\nG_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\dots=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k},\n 其中，R_t表示在时刻t获得的奖励。\n\n\n\n在MRP中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（Value）。所有状态的价值就组成了价值函数（Value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成\n\n\\begin{aligned}\nV(s)&=\\mathbb{E}\\left[G_t\\mid S_t=s\\right]\\\\\n&= \\mathbb{E}\\left[ R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} +\\dots\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma (R_{t+1}+\\gamma R_{t+2} +\\dots)\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma G_{t+1}\\mid S_t=s \\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma V(S_{t+1})\\mid S_t=s \\right]\n\\end{aligned}\n\n在上式的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即\\mathbb{E}\\left[ R_t\\mid S_t=s \\right]=r(s);另一方面，等式中剩余部分\\mathbb{E}\\left[\\gamma V(S_{t+1})\\mid S_t=s\\right]可以根据从状态s出发的转移概率得到，即\n\nV(s)=r(s)+\\gamma \\sum_{s'\\in \\mathcal{S}}p(s'\\mid s)V(s')\n\n上式就是MRP中非常有名的贝尔曼方程（Bellman equation），对每一个状态都成立。若通过矩阵运算可以得到以下解析解：\n\n\\begin{aligned}\n\\mathcal{V}&=\\mathcal{R}+\\gamma\\mathcal{P}\\mathcal{V}\\\\\n\\mathcal{V}&=(I-\\gamma\\mathcal{P})^{-1}\\mathcal{R}\n\\end{aligned}\n 其中\\mathcal{V,R,P}均为向量或矩阵。以上解析解的计算复杂度是O(n^3),其中n是状态个数，因此这种方法只适用于很小的MRP。求解较大规模的MRP奖励过程中的价值函数时，可以使用动态规划、蒙特卡洛、时序差分，这些方法将在之后的章节介绍。\n接下来编写代码来实现求解价值函数的解析解方法，并据此计算该马尔可夫奖励过程中所有状态的价值。\n\ndef compute(P, rewards, gamma, states_num):\n    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n                   rewards)\n    return value"
  },
  {
    "objectID": "index.html#马尔可夫决策过程-1",
    "href": "index.html#马尔可夫决策过程-1",
    "title": "强化学习基础",
    "section": "",
    "text": "如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（MDP）。我们将这个来自外界的刺激称为智能体（Agent）的动作，在MRP的基础上加入动作，就得到了MDP。MDP有元组\\langle \\mathcal{S,A,P},r,\\gamma\\rangle构成。\n\n\n智能体的策略（Policy）通常用字母\\pi表示。策略\\pi(a|s)=P(A_t=a|S_t=s)是一个函数，表示在输入状态s情况下采取动作a的概率。当一个策略是确定性策略（Deterministic policy）时，它在每个状态只输出一个确定性的动作，即只有该动作的概率为1,其他动作的概率为0; 当一个策略是随机性策略（Stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。\n在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。回顾一下在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。这很好理解，因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。\n\n\n\n我们用V^{\\pi}(s)表示在MDP中基于策略\\pi的状态价值函数（State-value function），定义在从状态s出发遵循策略\\pi能获得的期望回报，数学表达为\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]\n\n\n\n\n不同于MRP，MDP中由于动作的存在我们定义一个动作价值函数（Action-value function）。我们用Q^{\\pi}(s,a)表示在MDP遵循策略\\pi时，对当前状态s执行动作a得到的期望回报：\n\nQ^{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]\n\n状态价值函数和动作价值函数之间的关系：\n在使用策略\\pi时，状态s的价值等于在该状态下基于策略\\pi采取所有动作的概率与相应价值相乘再求和的结果：\n\nV^{\\pi}(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)Q^{\\pi}(s,a)\n\n在使用策略\\pi时，状态s下采取动作a的价值等于即时奖励加上经过衰减后的所有可能的下一个状态转移概率与相应价值的乘积：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n\n\n\n在贝尔曼方程中加上期望二字是为了与接下来的贝尔曼最优方程进行区分。我们通过加单推到就可以分别得到两个价值函数的贝尔曼期望方程（Bellman Expect Equation）：\n\n\\begin{aligned}\nV^{\\pi}(s)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma V^{\\pi}(S_{t+1})|S_t=s\\right]\\\\\n&=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left( r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi}(s')\\right)\\\\\n\\\\\nQ^{\\pi}(s,a)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma Q^{\\pi}(S_{t+1},A_{t+1}|S_t=s, A_t=a)\\right]\\\\\n&=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')Q^{\\pi}(s',a)\n\\end{aligned}\n\n价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的。\n现在我们考虑一个MDP的简单例子如 图 1 ，其中每个绿色圆圈代表一个状态，一共有s_1\\sim s_5这5个状态。黑色实线箭头代表可以采取的动作，黄色小圆圈代表动作。需要注意的是，并非在每个状态都能采取所有动作，例如在s_1,智能体只能采取”保持s_1“和”前往s_2“这两个动作，无法采取其他动作。\n每个黄色小圆圈旁的红色数字代表在某个状态下采取某个动作能获得的奖励。虚线箭头代表采取动作后可能转移到的状态，箭头边上的带方框的数字代表转移概率，如果没有数字则表示转移概率为1.例如，在s_2下，如果采取动作”前往s_3“就能得到奖励-2,并且以概率1转移到s_3;在s_4下，如果采取”概率前往”这个动作，就能得到奖励1,并且会分别以概率0.2,0.4,0.4转移到s_2,s_3或s_4.\n\n\n\n\n\n\n图 1： MDP一个简单例子\n\n\n\n接下来我们编写代码来表示 图 1 中的MDP，并定义两个策略，第一个策略是一个完全随机策略，即在每个状态下，智能体会以同样的概率选取它可能采取的动作。例如，在s_1下智能体会以0.5,0.5的概率选取动作”保持s_1“和”前往s_2“.第二个策略是一个提前设定的策略。\n\nimport numpy as np\nS = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\nA = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n# 状态转移函数\nP = {\n    \"s1-保持s1-s1\": 1.0,\n    \"s1-前往s2-s2\": 1.0,\n    \"s2-前往s1-s1\": 1.0,\n    \"s2-前往s3-s3\": 1.0,\n    \"s3-前往s4-s4\": 1.0,\n    \"s3-前往s5-s5\": 1.0,\n    \"s4-前往s5-s5\": 1.0,\n    \"s4-概率前往-s2\": 0.2,\n    \"s4-概率前往-s3\": 0.4,\n    \"s4-概率前往-s4\": 0.4,\n}\n# 奖励函数\nR = {\n    \"s1-保持s1\": -1,\n    \"s1-前往s2\": 0,\n    \"s2-前往s1\": -1,\n    \"s2-前往s3\": -2,\n    \"s3-前往s4\": -2,\n    \"s3-前往s5\": 0,\n    \"s4-前往s5\": 10,\n    \"s4-概率前往\": 1,\n}\ngamma = 0.5  # 折扣因子\nMDP = (S, A, P, R, gamma)\n\n# 策略1,随机策略\nPi_1 = {\n    \"s1-保持s1\": 0.5,\n    \"s1-前往s2\": 0.5,\n    \"s2-前往s1\": 0.5,\n    \"s2-前往s3\": 0.5,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.5,\n    \"s4-概率前往\": 0.5,\n}\n# 策略2\nPi_2 = {\n    \"s1-保持s1\": 0.6,\n    \"s1-前往s2\": 0.4,\n    \"s2-前往s1\": 0.3,\n    \"s2-前往s3\": 0.7,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.1,\n    \"s4-概率前往\": 0.9,\n}\n\n\n# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\ndef join(str1, str2):\n    return str1 + '-' + str2\n\n接下来我们想要计算该MDP下，一个策略\\pi的状态价值函数。我们现有的工具是MRP的解析解方法，一个自然的想法是给定一个MDP和一个策略\\pi,我们是否可以将其转化为一个MRP？答案是肯定的，我们可以将策略的动作选择进行边缘化（Marginalization），就可以得到没有动作的MRP了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个MRP在该状态下的奖励，即： \nr'(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)r(s,a)\n\n同理，我们计算采取动作的概率与使s转移到s'的概率的乘积，再将这些乘积相加，其和就是一个MRP的状态从s转移至s'的概率：\n\nP'(s'|s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\n\n于是，我们构建得到了一个MRP:\\langle \\mathcal{S},P',r',\\gamma\\rangle.根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。\n接下来，我们用代码实现该方法，计算用随即策略Pi_1时的状态价值函数，为了简单起见，我们将直接给出转化后的MRP的状态转移矩阵和奖励函数。\n\ngamma = 0.5\n# 转化后的MRP的状态转移矩阵\nP_from_mdp_to_mrp = [\n    [0.5, 0.5, 0.0, 0.0, 0.0],\n    [0.5, 0.0, 0.5, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.5, 0.5],\n    [0.0, 0.1, 0.2, 0.2, 0.5],\n    [0.0, 0.0, 0.0, 0.0, 1.0],\n]\nP_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\nR_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n\nV = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\nprint(\"MDP中每个状态价值分别为\\n\", V)\n\nMDP中每个状态价值分别为\n [[-1.22555411]\n [-1.67666232]\n [ 0.51890482]\n [ 6.0756193 ]\n [ 0.        ]]\n\n\n知道了状态价值函数V^{\\pi}(s)后，我们可以计算动作价值函数Q^{\\pi}(s,a).例如(s_4,\\text{概率前往})的动作价值为2.152，根据以下公式可以计算得到：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n这个 MRP 解析解的方法在状态动作集合比较大的时候不是很适用，那有没有其他的方法呢？ 章节 2 将介绍用动态规划算法来计算得到价值函数。 章节 1.3 将介绍用蒙特卡洛方法来近似估计这个价值函数，用蒙特卡洛方法的好处在于我们不需要知道 MDP 的状态转移函数和奖励函数，它可以得到一个近似值，并且采样数越多越准确。"
  },
  {
    "objectID": "index.html#sec-mc",
    "href": "index.html#sec-mc",
    "title": "强化学习基础",
    "section": "",
    "text": "蒙特卡洛方法（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用MC方法时我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。一个简单的例子是用蒙特卡洛方法来计算圆的面积。例如，在正方形内部随机产生若干个点，细数落在圆中点的个数，圆的面积与正方形面积之比就等于圆中点的个数与正方形中点的个数之比。如果我们随机产生的点的个数越多，计算得到圆的面积就越接近于真实的圆的面积。\n我们现在介绍如何用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数。回忆一下，一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望就可以了，公式如下：\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[ G_t|S_t=s \\right]\\approx \\frac{1}{N}\\sum_{i=1}^N G_t^{(i)}\n\n在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。我们介绍的蒙特卡洛价值估计方法会在该状态每一次出现时计算它的回报。还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。假设我们现在用策略\\pi从s开始采样序列，据此来计算状态价值。我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示。\n\n使用策略\\pi 采样若干条序列： \ns_0^{(i)}\\overset{a_0^{(i)}}{\\rightarrow} r_0^{(i)},s_1^{(i)}\\overset{a_1^{(i)}}{\\rightarrow} r_1^{(i)},\ns_2^{(i)}\\overset{a_2^{(i)}}{\\rightarrow},...,\\overset{a_{T-1}^{(i)}}{\\rightarrow} r_{T-1}^{(i)},s_T^{(i)}\n\n对每一条序列中的每一时间步t的状态s进行以下操作：\n\n更新状态s的计数器N(s)\\leftarrow N(s)+1;\n更新状态s的总回报M(s)\\leftarrow M(s)+G_t;\n\n每一个状态的价值被估计为回报的平均值V(s)=\\=M(s)/N(s).\n\n根据大数定律，当N(s)\\rightarrow \\infty,有V(s)\\rightarrow V^{\\pi}(s). 计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态s和对应回报G,进行如下计算：\n\nN(s)\\leftarrow N(s)+1\nV(s)\\leftarrow V(s)+\\frac{G-V(s)}{N(s)}\n\n接下来我们用代码定义一个采样函数，采样函数需要遵守状态转移矩阵和相应策略，每次将(s,a,r,s_next)元组放入序列中，直到到达终止序列。然后我们通过该函数，用随即策略在 图 1 的MDP中随机采样几条序列。\n\ndef sample(MDP, Pi, timestep_max, number):\n    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n    S, A, P, R, gamma = MDP\n    episodes = []\n    for _ in range(number):\n        episode = []\n        timestep = 0\n        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n        # 当前状态为终止状态或者时间步太长时,一次采样结束\n        while s != \"s5\" and timestep &lt;= timestep_max:\n            timestep += 1\n            rand, temp = np.random.rand(), 0\n            # 在状态s下根据策略选择动作\n            for a_opt in A:\n                temp += Pi.get(join(s, a_opt), 0)\n                if temp &gt; rand:\n                    a = a_opt\n                    r = R.get(join(s, a), 0)\n                    break\n            rand, temp = np.random.rand(), 0\n            # 根据状态转移概率得到下一个状态s_next\n            for s_opt in S:\n                temp += P.get(join(join(s, a), s_opt), 0)\n                if temp &gt; rand:\n                    s_next = s_opt\n                    break\n            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n            s = s_next  # s_next变成当前状态,开始接下来的循环\n        episodes.append(episode)\n    return episodes\n\n\n# 采样5次,每个序列最长不超过20步\nepisodes = sample(MDP, Pi_1, 20, 5)\nprint('第一条序列\\n', episodes[0])\nprint('第二条序列\\n', episodes[1])\nprint('第五条序列\\n', episodes[4])\n\n第一条序列\n [('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n第二条序列\n [('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's3'), ('s3', '前往s5', 0, 's5')]\n第五条序列\n [('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '前往s5', 10, 's5')]\n\n\n\n# 对所有采样序列计算所有状态的价值\ndef MC(episodes, V, N, gamma):\n    for episode in episodes:\n        G = 0\n        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算\n            (s, a, r, s_next) = episode[i]\n            G = r + gamma * G\n            N[s] = N[s] + 1\n            V[s] = V[s] + (G - V[s]) / N[s]\n\n\ntimestep_max = 20\n# 采样1000次,可以自行修改\nepisodes = sample(MDP, Pi_1, timestep_max, 1000)\ngamma = 0.5\nV = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nN = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nMC(episodes, V, N, gamma)\nprint(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)\n\n使用蒙特卡洛方法计算MDP的状态价值为\n {'s1': -1.2433556502656373, 's2': -1.6687838281541936, 's3': 0.5172596878892808, 's4': 6.24924512869253, 's5': 0}\n\n\n可以看到用蒙特卡洛方法估计得到的状态价值和我们用 MRP 解析解得到的状态价值是很接近的。这得益于我们采样了比较多的序列，感兴趣的读者可以尝试修改采样次数，然后观察蒙特卡洛方法的结果。"
  },
  {
    "objectID": "index.html#最优策略",
    "href": "index.html#最优策略",
    "title": "强化学习基础",
    "section": "",
    "text": "强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。我们首先定义策略之间的偏序关系：当且仅当对于任意状态s都有V^{\\pi}(s)&gt; V^{\\pi'}(s), 记\\pi&gt;\\pi'. 于是在有限状态和动作集合的MDP中至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是最优策略（Optimal policy）。最优策略可能有很多个，我们都将其表示为\\pi^*(s).\n最优策略都有相同的状态价值函数，我们称之为最有状态价值函数，表示为： \nV^*(s)=\\max_{\\pi} V^{\\pi}(s),\\forall s\\in\\mathcal{S}\n\n同理我们定义最优动作价值函数： \nQ^*(s,a)=\\max_{\\pi}Q^{\\pi}(s,a),\\forall s\\in\\mathcal{S},a\\in\\mathcal{A}\n\n为了使Q^*(s,a)最大，我们需要在当前状态动作对(s,a)之后都执行最优策略，于是我们得到了最有状态价值函数和最优动作价值函数之间的关系：\n\nQ^\\pi{s,a}=r(s,a)+\\gamma\\sum_{s\\in\\mathcal{S}}P(s'|s,a)V^*(s')\n\n这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值： \nV^*(s)=\\max_{a\\in\\mathcal{A}}Q^*(s,a)\n\n\n\n根据V^*(s)和Q^*(s,a)的关系，我们可以得到贝尔曼最优方程（Bellman optimality equation）：\n\n\\begin{aligned}\nV^*(s)&=\\max_{a\\in\\mathcal{A}}\\left\\{ r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^*(s')\\right\\}\\\\\nQ^*(s,a)=r(s,a)+\\gamma\\sum_{s'\\in \\mathcal{S}}p(s'|s,a)\\max_{a'\\in\\mathcal{A}}Q^*(s',a')\n\\end{aligned}\n\n章节 2 将介绍如何用动态规划算法得到最优策略。"
  },
  {
    "objectID": "index.html#总结",
    "href": "index.html#总结",
    "title": "强化学习基础",
    "section": "",
    "text": "马尔可夫决策过程是强化学习中的基础概念，强化学习中的环境就是一个马尔可夫决策过程。我们接下来将要介绍的强化学习算法通常都是在求解马尔可夫决策过程中的最优策略。"
  },
  {
    "objectID": "index.html#简介",
    "href": "index.html#简介",
    "title": "强化学习基础",
    "section": "3.1 简介",
    "text": "3.1 简介\n章节 2 介绍的动态规划算法要求马尔可夫决策过程是已知的，即要求与智能体交互的环境是完全已知的（例如迷宫或者给定规则的网格世界）。在此条件下，智能体其实并不需要和环境真正交互来采样数据，直接用动态规划算法就可以解出最优价值或策略。这就好比对于有监督学习任务，如果直接显式给出了数据的分布公式，那么也可以通过在期望层面上直接最小化模型的泛化误差来更新模型参数，并不需要采样任何数据点。\n但这在大部分场景下并不现实，机器学习的主要方法都是在数据分布未知的情况下针对具体的数据点来对模型做出更新的。对于大部分强化学习现实场景（例如电子游戏或者一些复杂物理环境），其马尔可夫决策过程的状态转移概率是无法写出来的，也就无法直接进行动态规划。在这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为无模型的强化学习（model-free reinforcement learning）。\n不同于动态规划算法，无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样到的数据来学习，这使得它可以被应用到一些简单的实际场景中。本章将要讲解无模型的强化学习中的两大经典算法：Sarsa 和 Q-learning，它们都是基于时序差分（temporal difference，TD）的强化学习算法。同时，本章还会引入一组概念：在线策略学习和离线策略学习。通常来说，在线策略学习要求使用在当前策略下采样得到的样本进行学习，一旦策略被更新，当前的样本就被放弃了，就好像在水龙头下用自来水洗手；而离线策略学习使用经验回放池将之前采样得到的样本收集起来再次利用，就好像使用脸盆接水后洗手。因此，离线策略学习往往能够更好地利用历史数据，并具有更小的样本复杂度（算法达到收敛结果需要在环境中采样的样本数量），这使其被更广泛地应用。"
  },
  {
    "objectID": "index.html#时序差分方法",
    "href": "index.html#时序差分方法",
    "title": "强化学习基础",
    "section": "3.2 时序差分方法",
    "text": "3.2 时序差分方法\n时序差分是一种用来估计一个策略的价值函数的方法，它结合了蒙特卡洛和动态规划算法的思想。时序差分方法和蒙特卡洛的相似之处在于可以从样本数据中学习，不需要事先知道环境；和动态规划的相似之处在于根据贝尔曼方程的思想，利用后续状态的价值估计来更新当前状态的价值估计。回顾一下蒙特卡洛方法对价值函数的增量更新方式：\n\nV(s_t)\\leftarrow V(s_t)+\\alpha\\left[ G_t-V(s_t) \\right]\n\n这里我们将@sec-mc 中的\\frac{1}{N(s)}替换成了\\alpha,表示对价值估计更新的步长。可以将\\alpha取为一个常数，此时更新方式不再像MC方法那样严格取期望。MC方法必须要等整个序列结束之后才能计算得到这一次的回报G_t,而时序差分方法只需要当前步结束即可计算。具体来说，时序差分算法用当前获得的奖励加上下一个状态的价值估计来作为在当前状态会获得的回报，即：\n\nV(s_t)\\leftarrow V(s_t)+\\alpha\\left[ R_t+\\gamma V(s_{t+1})-V(s_t) \\right]\n 其中R_t+\\gamma V(s_{t+1})-V(s_t)通常被称为时序差分误差。时序差分算法将其与步长的乘积作为状态价值的更新量。可以用R_t+\\gamma V(s_{t+1})来代替G_t的原因是： \n\\begin{aligned}\nV^{\\pi}(s)&=\\mathbb{E}_{\\pi}\\left[ G_t|S_t=s \\right]\\\\\n&=\\mathbb{E}_\\pi\\left[ \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k}|S_t=s \\right]\\\\\n&=\\mathbb{E}_{\\pi}\\left[ R_t+\\gamma\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}| S_t=s \\right]\\\\\n&=\\mathbb{E}_{\\pi}\\left[ R_t+\\gamma V^{\\pi}(s_{t+1})| S_t=s \\right]\n\\end{aligned}\n\n因此MC方法将上式第一行作为更新的目标而TD算法将上式最后一行作为更新的目标。于是，再用策略和环境交互时，每采样一步，我们就可以用时序差分算法来更新状态价值估计。时序差分算法用到了V(s_{t+1})的估计值，可以证明它最终收敛到策略\\pi的价值函数，我们在这里不对此进行展开说明。"
  },
  {
    "objectID": "index.html#sarsa算法",
    "href": "index.html#sarsa算法",
    "title": "强化学习基础",
    "section": "3.3 Sarsa算法",
    "text": "3.3 Sarsa算法\n既然我们可以用时序差分方法来估计价值函数，那一个很自然的问题是，我们能否用类似策略迭代的方法来进行强化学习。策略评估已经可以通过时序差分算法实现，那么在不知道奖励函数和状态转移函数的情况下该怎么进行策略提升呢？答案是可以直接用时序差分算法来估计动作价值函数Q:\n\nQ(s_t,a_t)\\leftarrow Q(s_t,a_t)+\\alpha\\left[ R_t+\\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t) \\right]\n 然后我们用贪婪算法来选取在某个状态下动作价值最大的那个动作，即\\arg\\max_{a}Q(s,a). 这样似乎已经形成了一个完整的强化学习算法： 用贪婪算法根据动作价值选取动作来和环境交互，再根据得到的数据用时序差分算法更新动作价值估计。\n然而这个简单的算法存在两个需要进一步考虑的问题。第一，如果要用时序差分算法来准确地估计策略的状态价值函数，我们需要用极大量的样本来进行更新。但实际上我们可以忽略这一点，直接用一些样本来评估策略，然后就可以更新策略了。我们可以这么做的原因是策略提升可以在策略评估未完全进行的情况进行；第二，如果在策略提升中一直根据贪婪算法得到一个确定性策略，可能会导致某些状态动作对(s,a)永远没有在序列中出现以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。\n简单常用的解决方案是不再一味使用贪婪算法，而是采用\\epsilon-贪婪策略：有1-\\epsilon的概率采用动作价值最大的那个动作（贪婪），另外有\\epsilon的概率从动作空间中随机选取一个动作（探索），其公式表示为：\n\n\\pi(a|s)=\n\\begin{cases}\n\\frac{\\epsilon}{|\\mathcal{A}|}+1-\\epsilon &\\text{如果}a=\\arg\\max_{a'}Q(s,a')\\\\\n\\frac{\\epsilon}{|\\mathcal{A}|}& 其他动作\n\\end{cases}\n\n现在，我们就可以得到一个实际的基于时序差分方法的强化学习算法。这个算法被称为 Sarsa，因为它的动作价值更新用到了当前状态s、当前动作a、获得的奖励r、下一个状态s'和下一个动作a',将这些符号拼接后就得到了算法名称，这是一种On-Policy(在线策略，a_{t+1}和a_t来自于同一策略)的时序差分强化学习算法。\n现在我们在悬崖漫步环境下尝试Sarsa算法。首先给出悬崖漫步环境的代码，此时环境不需要提供奖励函数和状态转移函数，而是需要提供一个和智能体进行交互的函数step(),该函数将智能体的动作作为输入，将奖励和下一个状态输出给智能体。\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm  # tqdm是显示循环进度条的库\n\n\nclass CliffWalkingEnv:\n    def __init__(self, ncol, nrow):\n        self.nrow = nrow\n        self.ncol = ncol\n        self.x = 0  # 记录当前智能体位置的横坐标\n        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n\n    def step(self, action):  # 外部调用这个函数来改变当前位置\n        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n        # 定义在左上角\n        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n        next_state = self.y * self.ncol + self.x\n        reward = -1\n        done = False\n        if self.y == self.nrow - 1 and self.x &gt; 0:  # 下一个位置在悬崖或者目标\n            done = True\n            if self.x != self.ncol - 1:\n                reward = -100\n        return next_state, reward, done\n\n    def reset(self):  # 回归初始状态,坐标轴原点在左上角\n        self.x = 0\n        self.y = self.nrow - 1\n        return self.y * self.ncol + self.x\n\n然后我们来实现Sarsa算法，主要维护一个表格Q_table(),用来存储当前策略下所有状态动作对的价值，在用Sarsa算法和环境交互时，用\\epsilon-贪婪策略进行采样，在更新Sarsa算法时，使用时序差分的公式。我们默认终止状态时所有动作的价值都是0,这些价值在初始化为0后就不会进行更新。\n\nclass Sarsa:\n    \"\"\" Sarsa算法 \"\"\"\n    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n        self.n_action = n_action  # 动作个数\n        self.alpha = alpha  # 学习率\n        self.gamma = gamma  # 折扣因子\n        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n\n    def take_action(self, state):  # 选取下一步的操作,具体实现为epsilon-贪婪\n        if np.random.random() &lt; self.epsilon:\n            action = np.random.randint(self.n_action)\n        else:\n            action = np.argmax(self.Q_table[state])\n        return action\n\n    def best_action(self, state):  # 用于打印策略\n        Q_max = np.max(self.Q_table[state])\n        a = [0 for _ in range(self.n_action)]\n        for i in range(self.n_action):  # 若两个动作的价值一样,都会记录下来\n            if self.Q_table[state, i] == Q_max:\n                a[i] = 1\n        return a\n\n    def update(self, s0, a0, r, s1, a1):\n        td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]\n        self.Q_table[s0, a0] += self.alpha * td_error\n\n接下来我们就在悬崖漫步环境中运行Sarsa算法，一起来看看结果吧！\n\nncol = 12\nnrow = 4\nenv = CliffWalkingEnv(ncol, nrow)\nnp.random.seed(0)\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.9\nagent = Sarsa(ncol, nrow, epsilon, alpha, gamma)\nnum_episodes = 500  # 智能体在环境中运行的序列的数量\n\nreturn_list = []  # 记录每一条序列的回报\nfor i in range(10):  # 显示10个进度条\n    # tqdm的进度条功能\n    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n        for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n            episode_return = 0\n            state = env.reset()\n            action = agent.take_action(state)\n            done = False\n            while not done:\n                next_state, reward, done = env.step(action)\n                next_action = agent.take_action(next_state)\n                episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n                agent.update(state, action, reward, next_state, next_action)\n                state = next_state\n                action = next_action\n            return_list.append(episode_return)\n            if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n                pbar.set_postfix({\n                    'episode':\n                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n                    'return':\n                    '%.3f' % np.mean(return_list[-10:])\n                })\n            pbar.update(1)\n\nepisodes_list = list(range(len(return_list)))\nplt.plot(episodes_list, return_list)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('Sarsa on {}'.format('Cliff Walking'))\nplt.show()\n\n\n\n\n\n\n\n\n我们发现，随着训练的进行，Sarsa 算法获得的回报越来越高。在进行 500 条序列的学习后，可以获得 −20 左右的回报，此时已经非常接近最优策略了。然后我们看一下 Sarsa 算法得到的策略在各个状态下会使智能体采取什么样的动作。\n\ndef print_agent(agent, env, action_meaning, disaster=[], end=[]):\n    for i in range(env.nrow):\n        for j in range(env.ncol):\n            if (i * env.ncol + j) in disaster:\n                print('****', end=' ')\n            elif (i * env.ncol + j) in end:\n                print('EEEE', end=' ')\n            else:\n                a = agent.best_action(i * env.ncol + j)\n                pi_str = ''\n                for k in range(len(action_meaning)):\n                    pi_str += action_meaning[k] if a[k] &gt; 0 else 'o'\n                print(pi_str, end=' ')\n        print()\n\n\naction_meaning = ['^', 'v', '&lt;', '&gt;']\nprint('Sarsa算法最终收敛得到的策略为：')\nprint_agent(agent, env, action_meaning, list(range(37, 47)), [47])\n\nSarsa算法最终收敛得到的策略为：\nooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ovoo \nooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ovoo \n^ooo ooo&gt; ^ooo ooo&gt; ooo&gt; ooo&gt; ooo&gt; ^ooo ^ooo ooo&gt; ooo&gt; ovoo \n^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n\n\n可以发现Sarsa算法会采取比较远离悬崖的策略来抵达目标，比较保守。"
  },
  {
    "objectID": "index.html#多步sarsa算法",
    "href": "index.html#多步sarsa算法",
    "title": "强化学习基础",
    "section": "3.4 多步Sarsa算法",
    "text": "3.4 多步Sarsa算法\n蒙特卡洛方法利用当前状态之后每一步的奖励而不使用任何价值估计，时序差分算法只利用一步奖励和下一个状态的价值估计。那它们之间的区别是什么呢？总的来说，蒙特卡洛方法是无偏（unbiased）的，但是具有比较大的方差，因为每一步的状态转移都有不确定性，而每一步状态采取的动作所得到的不一样的奖励最终都会加起来，这会极大影响最终的价值估计；时序差分算法具有非常小的方差，因为只关注了一步状态转移，用到了一步的奖励，但是它是有偏的，因为用到了下一个状态的价值估计而不是其真实的价值。那有没有什么方法可以结合二者的优势呢？答案是多步时序差分！多步时序差分的意思是使用n步的奖励，然后使用之后状态的价值估计。用公式表示，将 \nG_t=R_t+\\gamma Q(s_{t+1},a_{t+1})\n 替换成 \nG_t=R_t+\\gamma R_{t+1}+\\dots+\\gamma^{n-1}R_{t+n-1}+\\gamma^n Q(s_{t+n},a_{t+n})\n 于是Sarsa算法中的动作价值函数更新公式变为 \nQ(s_t,a_t)\\leftarrow Q(s_t,a_t)+\\alpha\\left[ R_t+\\gamma R_{t+1}+\\dots+\\gamma^{n-1}R_{t+n-1}+\\gamma^n Q(s_{t+n},a_{t+n})-Q(s_t,a_t) \\right]\n 接下来将用代码实现n步Sarsa算法，在Sarsa代码基础上进行修改，引入多步时序差分计算。\n\nclass nstep_Sarsa:\n    \"\"\" n步Sarsa算法 \"\"\"\n    def __init__(self, n, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n        self.Q_table = np.zeros([nrow * ncol, n_action])\n        self.n_action = n_action\n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.n = n  # 采用n步Sarsa算法\n        self.state_list = []  # 保存之前的状态\n        self.action_list = []  # 保存之前的动作\n        self.reward_list = []  # 保存之前的奖励\n\n    def take_action(self, state):\n        if np.random.random() &lt; self.epsilon:\n            action = np.random.randint(self.n_action)\n        else:\n            action = np.argmax(self.Q_table[state])\n        return action\n\n    def best_action(self, state):  # 用于打印策略\n        Q_max = np.max(self.Q_table[state])\n        a = [0 for _ in range(self.n_action)]\n        for i in range(self.n_action):\n            if self.Q_table[state, i] == Q_max:\n                a[i] = 1\n        return a\n\n    def update(self, s0, a0, r, s1, a1, done):\n        self.state_list.append(s0)\n        self.action_list.append(a0)\n        self.reward_list.append(r)\n        if len(self.state_list) == self.n:  # 若保存的数据可以进行n步更新\n            G = self.Q_table[s1, a1]  # 得到Q(s_{t+n}, a_{t+n})\n            for i in reversed(range(self.n)):\n                G = self.gamma * G + self.reward_list[i]  # 不断向前计算每一步的回报\n                # 如果到达终止状态,最后几步虽然长度不够n步,也将其进行更新\n                if done and i &gt; 0:\n                    s = self.state_list[i]\n                    a = self.action_list[i]\n                    self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])\n            s = self.state_list.pop(0)  # 将需要更新的状态动作从列表中删除,下次不必更新\n            a = self.action_list.pop(0)\n            self.reward_list.pop(0)\n            # n步Sarsa的主要更新步骤\n            self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])\n        if done:  # 如果到达终止状态,即将开始下一条序列,则将列表全清空\n            self.state_list = []\n            self.action_list = []\n            self.reward_list = []\n\n\nnp.random.seed(0)\nn_step = 5  # 5步Sarsa算法\nalpha = 0.1\nepsilon = 0.1\ngamma = 0.9\nagent = nstep_Sarsa(n_step, ncol, nrow, epsilon, alpha, gamma)\nnum_episodes = 500  # 智能体在环境中运行的序列的数量\n\nreturn_list = []  # 记录每一条序列的回报\nfor i in range(10):  # 显示10个进度条\n    #tqdm的进度条功能\n    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n        for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n            episode_return = 0\n            state = env.reset()\n            action = agent.take_action(state)\n            done = False\n            while not done:\n                next_state, reward, done = env.step(action)\n                next_action = agent.take_action(next_state)\n                episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n                agent.update(state, action, reward, next_state, next_action,\n                             done)\n                state = next_state\n                action = next_action\n            return_list.append(episode_return)\n            if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n                pbar.set_postfix({\n                    'episode':\n                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n                    'return':\n                    '%.3f' % np.mean(return_list[-10:])\n                })\n            pbar.update(1)\n\nepisodes_list = list(range(len(return_list)))\nplt.plot(episodes_list, return_list)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('5-step Sarsa on {}'.format('Cliff Walking'))\nplt.show()\n\n\n\n\n\n\n\n\n通过实验结果可以发现，5步Sarsa算法的收敛速度比单步Sarsa算法更快，我们看下此时的策略表现。\n\naction_meaning = ['^', 'v', '&lt;', '&gt;']\nprint('5步Sarsa算法最终收敛得到的策略为：')\nprint_agent(agent, env, action_meaning, list(range(37, 47)), [47])\n\n5步Sarsa算法最终收敛得到的策略为：\nooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ovoo \n^ooo ^ooo ^ooo oo&lt;o ^ooo ^ooo ^ooo ^ooo ooo&gt; ooo&gt; ^ooo ovoo \nooo&gt; ^ooo ^ooo ^ooo ^ooo ^ooo ^ooo ooo&gt; ooo&gt; ^ooo ooo&gt; ovoo \n^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n\n\n我们发现此时多步 Sarsa 算法得到的策略会在最远离悬崖的一边行走，以保证最大的安全性。"
  },
  {
    "objectID": "index.html#q-learning-算法",
    "href": "index.html#q-learning-算法",
    "title": "强化学习基础",
    "section": "3.5 Q-learning 算法",
    "text": "3.5 Q-learning 算法\n除了 Sarsa，还有一种非常著名的基于时序差分算法的强化学习算法——Q-learning。Q-learning 和 Sarsa 的最大区别在于 Q-learning 的时序差分更新方式为\n\nQ(s_t,a_t)\\leftarrow Q(s_t,a_t)+\\alpha\\left[ R_t+\\gamma \\max_{a}Q(s_{t+1},a)-Q(s_t,a_t) \\right]\n\n我们可以用价值迭代的思想来理解Q-learning, 即Q-learning 是直接在估计Q^*,因为动作价值函数的贝尔曼最优方程是 \nQ^*(s,a)=r(s,a)+\\gamma \\sum_{s'\\in\\mathcal{S}}P(s'|s,a)\\max_{a'}Q^*(s',a')\n 而 Sarsa估计当前\\epsilon-贪婪策略的动作价值函数。需要强调的是，Q-learning的更新并非必须使用当前贪心策略\\arg\\max_{a}Q(s,a)采样得到的数据，因为给定任意(s,a,r,s')都可以直接根据更新公式来更新Q,为了探索，我们通常使用一个\\epsilon-贪婪策略采样得到的数据，因为它的更新中用到的Q(s',a')的a'是当前策略在s'下的动作。我们称Sarsa是在线策略（On-policy）算法，称Q-learning是离线策略（Off-policy）算法，这两个概念在强化学习中非常重要。\n\n3.5.1 在线策略与离线策略\n我们称采样数据的策略为行为策略（behavior policy），称用这些数据来更新的策略为目标策略（target policy）。在线策略（on-policy）算法表示行为策略和目标策略是同一个策略；而离线策略（off-policy）算法表示行为策略和目标策略不是同一个策略。Sarsa 是典型的在线策略算法，而 Q-learning 是典型的离线策略算法。判断二者类别的一个重要手段是看计算时序差分的价值目标的数据是否来自当前的策略，如 图 2 所示。具体而言：\n- 对于Sarsa，它的更新公式必须使用来自当前策略采样得到的五元组(s,a,r,s',a'),因此它是在线策略学习方法； - 对于Q-learning，它的更新公式使用的是四元组(s,a,r,s')来更新当前状态动作对的价值Q(s,a),数据中的s,a是给定的条件，r,s'皆由环境采样得到，该四元组并不需要一定是当前策略采样得到的数据，也可以来自行为策略，因此它是离线策略算法。\n\n\n\n\n\n\n图 2： Sarsa和Q-learning的对比\n\n\n\n在之后的讲解中，我们会注明各个算法分别属于这两类中的哪一类。如前文所述，离线策略算法能够重复使用过往训练样本，往往具有更小的样本复杂度，也因此更受欢迎。\n我们接下来仍然在悬崖漫步环境下来实现 Q-learning 算法。\n\nclass QLearning:\n    \"\"\" Q-learning算法 \"\"\"\n    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n        self.n_action = n_action  # 动作个数\n        self.alpha = alpha  # 学习率\n        self.gamma = gamma  # 折扣因子\n        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n\n    def take_action(self, state):  #选取下一步的操作\n        if np.random.random() &lt; self.epsilon:\n            action = np.random.randint(self.n_action)\n        else:\n            action = np.argmax(self.Q_table[state])\n        return action\n\n    def best_action(self, state):  # 用于打印策略\n        Q_max = np.max(self.Q_table[state])\n        a = [0 for _ in range(self.n_action)]\n        for i in range(self.n_action):\n            if self.Q_table[state, i] == Q_max:\n                a[i] = 1\n        return a\n\n    def update(self, s0, a0, r, s1):\n        td_error = r + self.gamma * self.Q_table[s1].max(\n        ) - self.Q_table[s0, a0]\n        self.Q_table[s0, a0] += self.alpha * td_error\n\n\nnp.random.seed(0)\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.9\nagent = QLearning(ncol, nrow, epsilon, alpha, gamma)\nnum_episodes = 500  # 智能体在环境中运行的序列的数量\n\nreturn_list = []  # 记录每一条序列的回报\nfor i in range(10):  # 显示10个进度条\n    # tqdm的进度条功能\n    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n        for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n            episode_return = 0\n            state = env.reset()\n            done = False\n            while not done:\n                action = agent.take_action(state)\n                next_state, reward, done = env.step(action)\n                episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n                agent.update(state, action, reward, next_state)\n                state = next_state\n            return_list.append(episode_return)\n            if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n                pbar.set_postfix({\n                    'episode':\n                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n                    'return':\n                    '%.3f' % np.mean(return_list[-10:])\n                })\n            pbar.update(1)\n\nepisodes_list = list(range(len(return_list)))\nplt.plot(episodes_list, return_list)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('Q-learning on {}'.format('Cliff Walking'))\nplt.show()\n\naction_meaning = ['^', 'v', '&lt;', '&gt;']\nprint('Q-learning算法最终收敛得到的策略为：')\nprint_agent(agent, env, action_meaning, list(range(37, 47)), [47])\n\n\n\n\n\n\n\n\nQ-learning算法最终收敛得到的策略为：\n^ooo ovoo ovoo ^ooo ^ooo ovoo ooo&gt; ^ooo ^ooo ooo&gt; ooo&gt; ovoo \nooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ^ooo ooo&gt; ooo&gt; ooo&gt; ooo&gt; ovoo \nooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ovoo \n^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n\n\n需要注意的是，打印出来的回报是行为策略在环境中交互得到的，而不是 Q-learning 算法在学习的目标策略的真实回报。我们把目标策略的行为打印出来后，发现其更偏向于走在悬崖边上，这与 Sarsa 算法得到的比较保守的策略相比是更优的。 但是仔细观察 Sarsa 和 Q-learning 在训练过程中的回报曲线图，我们可以发现，在一个序列中 Sarsa 获得的期望回报是高于 Q-learning 的。这是因为在训练过程中智能体采取基于当前Q(s,a)函数的\\epsilon-贪婪策略来平衡探索与利用，Q-learning 算法由于沿着悬崖边走，会以一定概率探索“掉入悬崖”这一动作，而 Sarsa 相对保守的路线使智能体几乎不可能掉入悬崖。"
  },
  {
    "objectID": "index.html#小结",
    "href": "index.html#小结",
    "title": "强化学习基础",
    "section": "3.6 小结",
    "text": "3.6 小结\n本章介绍了无模型的强化学习中的一种非常重要的算法——时序差分算法。时序差分算法的核心思想是用对未来动作选择的价值估计来更新对当前动作选择的价值估计，这是强化学习中的核心思想之一。本章重点讨论了 Sarsa 和 Q-learning 这两个最具有代表性的时序差分算法。当环境是有限状态集合和有限动作集合时，这两个算法非常好用，可以根据任务是否允许在线策略学习来决定使用哪一个算法。 值得注意的是，尽管离线策略学习可以让智能体基于经验回放池中的样本来学习，但需要保证智能体在学习的过程中可以不断和环境进行交互，将采样得到的最新的经验样本加入经验回放池中，从而使经验回放池中有一定数量的样本和当前智能体策略对应的数据分布保持很近的距离。如果不允许智能体在学习过程中和环境进行持续交互，而是完全基于一个给定的样本集来直接训练一个策略，这样的学习范式被称为离线强化学习（offline reinforcement learning），之后的章节将会介绍离线强化学习的相关知识。"
  },
  {
    "objectID": "index.html#简介-1",
    "href": "index.html#简介-1",
    "title": "强化学习基础",
    "section": "4.1 简介",
    "text": "4.1 简介\n在强化学习中，“模型”通常指与智能体交互的环境模型，即对环境的状态转移概率和奖励函数进行建模。根据是否具有环境模型，强化学习算法分为两种：基于模型的强化学习（model-based reinforcement learning）和无模型的强化学习（model-free reinforcement learning）。无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计，章节 3 讨论的两种时序差分算法，即 Sarsa 和 Q-learning 算法，便是两种无模型的强化学习方法，本书在后续章节中将要介绍的方法也大多是无模型的强化学习算法。在基于模型的强化学习中，模型可以是事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后用这个模型帮助策略提升或者价值估计。章节 2 讨论的两种动态规划算法，即策略迭代和价值迭代，则是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的。本章即将介绍的 Dyna-Q 算法也是非常基础的基于模型的强化学习算法，不过它的环境模型是通过采样数据估计得到的。\n强化学习算法有两个重要的评价指标：一个是算法收敛后的策略在初始状态下的期望回报，另一个是样本复杂度，即算法达到收敛结果需要在真实环境中采样的样本数量。基于模型的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，对真实环境中样本的需求量往往就会减少，因此通常会比无模型的强化学习算法具有更低的样本复杂度。但是，环境模型可能并不准确，不能完全代替真实环境，因此基于模型的强化学习算法收敛后其策略的期望回报可能不如无模型的强化学习算法。"
  },
  {
    "objectID": "index.html#dyna-q",
    "href": "index.html#dyna-q",
    "title": "强化学习基础",
    "section": "4.2 Dyna-Q",
    "text": "4.2 Dyna-Q\nDyna-Q算法是一个经典的基于模型的强化学习算法，如 图 3 所示，Dyna-Q使用一种叫做Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态s，采取一个曾经在该状态下执行过的动作a,通过模型得到转移后的状态s'以及奖励r,并根据这个模拟数据(s,a,r,s'),用Q-learning的更新方式来更新动作价值函数。\n\n\n\n\n\n\n图 3： 基于模型的强化学习方法与无模型的强化学习\n\n\n\n\n在每次与环境进行交互执行一次 Q-learning 之后，Dyna-Q 会做n次 Q-planning。其中 Q-planning 的次数N是一个事先可以选择的超参数，当其为 0 时就是普通的 Q-learning。值得注意的是，上述 Dyna-Q 算法是执行在一个离散并且确定的环境中，所以当看到一条经验数据(s,a,r,s')时，可以直接对模型做出更新，即M(s,a)\\leftarrow r,s'."
  },
  {
    "objectID": "index.html#简介-2",
    "href": "index.html#简介-2",
    "title": "强化学习基础",
    "section": "5.1 简介",
    "text": "5.1 简介\n在 章节 3 讲解的Q-learning算法中，我们以矩阵的方式建立了一张存储每个状态下所有动作Q值得表格，表格中的每一个动作价值Q(s,a)表示在状态s下选择动作a然后继续遵循某一策略预期能够得到的期望回报。然而，这种用表格存储动作价值的做法只在环境的状态和动作都是离散的，并且空间都比较小的情况下适用，我们之前进行代码实战的几个环境都是如此（如悬崖漫步）。当状态或者动作数量非常大的时候，这种做法就不适用了，更甚者，当状态或者动作连续的时候，就有无限个状态动作对，我们更加无法使用这种表格形式来记录各个状态动作对的Q值。\n对于这种情况，我们需要用函数拟合的方法来估计Q值，即将这个复杂的Q值表格视作数据，使用一个参数化得函数Q_\\theta来拟合这些数据。很显然，这种函数拟合的方法存在一定的精度损失，因此被称为近似方法。我们今天要介绍的 DQN 算法便可以用来解决连续状态下离散动作的问题。"
  },
  {
    "objectID": "index.html#dqn",
    "href": "index.html#dqn",
    "title": "强化学习基础",
    "section": "5.2 DQN",
    "text": "5.2 DQN\n在类似车杆的环境中得到动作价值函数Q(s,a),由于状态每一维度的值都是连续的，无法使用表格记录，因此一个常见的解决方法便是函数拟合的思想。由于神经网络具有强大的表达能力，因此我们可以用一个神经网络来表示函数Q.若动作是连续（无限）的，神经网络的输入是状态s和动作a,然后输出一个标量，表示在状态s下采取动作a能获得的价值。若动作是离散（有限）的，除了可以采取动作连续情况下的做法，还可以只将状态s输入到神经网络中，使其同时输出每一个动作的Q值。通常DQN（以及Q-learning）只能处理动作离散的情况，因为在函数Q的更新过程中有\\max_a这一操作。假设神经网络用来拟合函数的参数是\\omega，即每一个状态s下所有可能动作a的Q值我们都能表示为Q_\\omega(s,a).我们将用于拟合Q的神经网络称为Q网络，如 图 4 所示。\n\n\n\n\n\n\n图 4： 工作在CartPole环境中的Q网络示意图\n\n\n\n那么Q网络的损失函数是什么样的呢？我们先回顾一下Q-learning的更新规则： \nQ(s,a)\\leftarrow Q(s,a)+\\alpha\\left[ r+\\gamma \\max_{a'\\in\\mathcal{A}}Q(s',a')-Q(s,a) \\right]\n\n上述公式用时序差分学习目标r+\\gamma \\max_{a'\\in\\mathcal{A}}Q(s',a')来增量式更新Q(s,a),也就是说要使Q(s,a)和TD目标r+\\gamma \\max_{a'\\in\\mathcal{A}}Q(s',a')靠近。于是，对于一组数据\\{(s_i,a_i,r_i,s_i')\\}，我们可以很自然地将Q网络的损失函数构造为均方误差的形式：\n\n\\omega^*=\\arg\\max_{\\omega}\\frac{1}{2N}\\sum_{i=1}^n\\left[Q_\\omega(s_i,a_i)-(r_i+\\gamma\\max_{a'}Q_\\omega(s_i',a'))\\right]^2\n\n至此，我们就可以将 Q-learning 扩展到神经网络形式——深度 Q 网络（deep Q network，DQN）算法。由于 DQN 是离线策略算法，因此我们在收集数据的时候可以使用一个\\epsilon-贪婪策略来平衡探索与利用，将收集到的数据存储起来，在后续的训练中使用。DQN 中还有两个非常重要的模块——经验回放和目标网络，它们能够帮助 DQN 取得稳定、出色的性能。\n\n5.2.1 经验回放\n在一般的有监督学习中，假设训练数据是独立同分布的，我们每次训练神经网络的时候从训练数据中随机采样一个或若干个数据进行梯度下降，随着学习的不断进行，每一个训练数据会被使用多次。在原来的Q-learning算法中，每一个数据只会用来更新一次Q值。为了更好地将Q-learning与深度神经网络结合，DQN算法采用了经验回放（Experience reply）方法，具体做法为维护一个回放缓冲区，将每次从环境中采样得到的四元组数据（状态，动作，奖励，下一状态）存储到回放缓冲区中，训练Q网络的时候再从回放缓冲区中随机采样若干数据来进行训练。这么做可以起到以下两个作用\n\n使样本满足独立假设。在 MDP 中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。\n\n提高样本效率。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。\n\n\n\n5.2.2 目标网络\nDQN算法最终更新的目标是让Q_\\omega(s,a)逼近r+\\gamma \\max_{a'}Q_{\\omega}(s',a'),由于TD误差目标本身也在不断改变，因此在更新网络参数的同时目标也在不断改变，这非常容易造成神经网络训练的不稳定性。为了解决这一问题，DQN便使用了目标网络（Target network）的思想：既然训练过程中 Q 网络的不断更新会导致目标不断发生改变，不如暂时先将 TD 目标中的 Q 网络固定住。为了实现这一思想，我们需要利用两套 Q 网络。\n\n原来的训练网络Q_\\omega(s,a),用于计算原来的损失函数\\frac{1}{2}[Q_\\omega(s,a)-(r+\\gamma \\max_{a'}Q_{\\omega^-}(s',a'))]^2中的Q_{\\omega}(s,a)项，并且使用正常梯度下降方法来进行更新。\n目标网络Q_{\\omega^-},用于计算原先损失函数\\frac{1}{2}[Q_\\omega(s,a)-(r+\\gamma \\max_{a'}Q_{\\omega^-}(s',a'))]^2中的(r+\\gamma \\max_{a'}Q_{\\omega^-}(s',a'))项，其中\\omega^-表示目标网络中的参数。\n\n如果两套网络中的参数随时保持一致，则仍为原先不够稳定的算法。为了让更新目标更稳定，目标网络并不会每一步都更新。具体而言，目标网络使用训练网络的一套旧的参数，训练网络Q_{\\omega}(s,a)在训练中的每一步都会更新，而目标网络的参数每隔C步才会与训练网络同步一次，即\\omega^-\\leftarrow\\omega. 这样做使得目标网络相对于训练网络更加稳定。\n综上所述，DQN算法的具体流程如下：\n\n\n\\begin{algorithm} \\caption{DQN算法流程} \\begin{algorithmic} \\State 用随机的网络参数$\\omega$初始化网络$Q_{\\omega}(s,a)$ \\State 复制相同的参数$\\omega_^-\\leftarrow\\omega$初始化目标网络$Q_{\\omega^-}$ \\State 初始化经验放回池$R$ \\For{$e=1\\rightarrow E$} \\State 获取环境初始状态$s_1$ \\For{时间步$t=1\\rightarrow T$} \\State 根据当前网络$Q_{\\omega}(s,a)$以$\\epsilon$-贪婪策略选择动作$a_t$ \\State 执行动作$a_t,$获得奖励$r_t,$环境状态变为$s_{t+1}$ \\State 将$(s_t,a_t,r_t,s_{t+1})$存储进回放池$R$ \\State 若$R$中数据足够，从$R$中采样$N$个数据$\\{(s_i,a_i,r_i,s_{i+1})\\}_{i=1,...,N}$ \\State 对每个元组，用目标网络计算 $y_i=r_i+\\gamma \\max_{a}Q_{\\omega^-}(s_{i+1},a)$ \\State 最小化目标损失 $L=\\frac{1}{N}\\sum_{i=1}^N(y_i-Q_{\\omega}(s_i,a_i))^2$ 以此更新当前网咯$Q_{\\omega}$ \\State 更新目标网络 \\EndFor \\EndFor \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "index.html#简介-3",
    "href": "index.html#简介-3",
    "title": "强化学习基础",
    "section": "6.1 简介",
    "text": "6.1 简介\nDQN 算法敲开了深度强化学习的大门，但是作为先驱性的工作，其本身存在着一些问题以及一些可以改进的地方。于是，在 DQN 之后，学术界涌现出了非常多的改进算法。本章将介绍其中两个非常著名的算法：Double DQN 和 Dueling DQN，这两个算法的实现非常简单，只需要在 DQN 的基础上稍加修改，它们能在一定程度上改善 DQN 的效果。如果读者想要了解更多、更详细的 DQN 改进方法，可以阅读 Rainbow 模型的论文及其引用文献。"
  },
  {
    "objectID": "index.html#策略梯度",
    "href": "index.html#策略梯度",
    "title": "强化学习基础",
    "section": "7.2 策略梯度",
    "text": "7.2 策略梯度\n基于策略的方法首先需要将策略参数化。假设目标策略\\pi_\\theta是一个随机性策略且处处可微，其中\\theta使对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。我们的目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为\n\nJ(\\theta)=\\mathbb{E}_{s_0}[V^{\\pi_\\theta}(s_0)]\n 其中s_0表示初始状态。现在有了目标函数，我们将目标函数对策略\\theta求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数从而得到最优策略。\n\n\\begin{aligned}\n\\nabla_\\theta J(\\theta)&\\propto \\sum_{s\\in\\mathcal{S}}\\nu^{\\pi_\\theta}(s)\\sum_{a\\in\\mathcal{A}}Q^{\\pi_\\theta}(s,a)\\nabla_\\theta\\pi_\\theta(a|s)\\\\\n&=\\sum_{s\\in\\mathcal{S}}\\nu^{\\pi_\\theta}(s)\\sum_{a\\in\\mathcal{A}}\\pi_\\theta(a|s)Q^{\\pi_\\theta}(s,a)\\frac{\\nabla_\\theta\\pi_\\theta(a|s)}{\\pi_\\theta(a|s)}\\\\\n&=\\mathbb{E}_{\\pi_\\theta}\\left[ Q^{\\pi_\\theta}(s,a)\\nabla_\\theta\\log\\pi_\\theta(a|s) \\right]\n\\end{aligned}\n 其中\\nu^\\pi使策略\\pi下的状态访问分布。\n上面的梯度可以用来更新策略。需要注意的是，因为上式中期望\\mathbb{E}的下标是\\pi_\\theta，所以策略梯度算法为在线策略（on-policy）算法，即必须使用当前策略\\pi_\\theta采样得到的数据来计算梯度。直观理解一下策略梯度这个公式，可以发现在每一个状态下，梯度的修改是让策略更多地去采样到带来较高Q值的动作，更少地去采样到带来较低Q值的动作。\n在计算策略梯度的公式中，需要用到Q^{\\pi_\\theta}(s,a),可以用多种方法对它进行估计。接下来要介绍的REINFORCE算法便是采用了MC方法来估计Q^{\\pi_\\theta}(s,a),对于一个有限步数的环境来说，REINFORCE算法中的策略梯度为： \n\\nabla_\\theta J(\\theta)=\\mathbb{E}_{\\pi_\\theta}\\left[ \\sum_{t=0}^T\\left( \\sum_{t'=t}^T\\gamma^{t'-t}r_{t'} \\right)\\nabla_\\theta \\log\\pi_\\theta(a_t|s_t) \\right]\n 其中T是和环境交互的最大步数。"
  },
  {
    "objectID": "index.html#简介-4",
    "href": "index.html#简介-4",
    "title": "强化学习基础",
    "section": "7.1 简介",
    "text": "7.1 简介\n本书之前介绍的 Q-learning、DQN 及 DQN 改进算法都是基于价值（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。策略梯度是基于策略的方法的基础，本章从策略梯度算法说起。"
  },
  {
    "objectID": "index.html#actor-critic",
    "href": "index.html#actor-critic",
    "title": "强化学习基础",
    "section": "8.2 Actor-Critic",
    "text": "8.2 Actor-Critic\n回顾一下，在 REINFORCE 算法中，目标函数的梯度中有一项轨迹回报，用于指导策略的更新。REINFOCE 算法用蒙特卡洛方法来估计Q(s,a),那能不能考虑拟合一个值函数来指导策略进行学习呢？这正是Actor-Critic算法所做的。在策略梯度中，可以把梯度写成下面这个更一般的形式： \ng=\\mathbb{E}\\left[ \\sum_{t=0}^T\\psi_t\\nabla_\\theta\\log\\pi_\\theta(a_t|s_t) \\right]\n 其中\\psi_t可以用很多种形式：\n\n\\sum_{t'=0}^T\\gamma^{t'}r_{t'}:轨迹的总回报；\n\n\\sum_{t'=t}^T\\gamma^{t'-t}r_{t'}:动作a_t之后的总回报；\n\nsum_{t'=t}^T\\gamma^{t'-t}r_{t'}-b(s_t):基准线版本的改进；\n\nQ^{\\pi_\\theta}(s_t,a_t):动作价值函数；\n\nA^{\\pi_\\theta}(s_t,a_t):优势函数；\n\nr_t+\\gamma V^{\\pi_\\theta}(s_{t+1})-V^{\\pi_\\theta}(s_{t}):时序差分残差。\n\nREINFORCE通过MC采样的方法对策略梯度的估计是无偏的但是方差非常大，我们可以用形式3引入基线函数b(s_t)来减小方差。此外，我们也可以采用Actor-Critic算法估计一个动作价值函数Q,代替MC采样得到的回报，这便是形式4.这个时候我们可以把状态价值函数V作为基线，从Q函数减去这个V函数则得到了A函数，称之为优势函数，即形式5.更进一步，我们可以利用Q=r+\\gamma V得到形式6.\n本章主要介绍形式6，即通过时序差分残差\\psi_t=r_t+\\gamma V^{\\pi_\\theta}(s_{t+1})-V^{\\pi_\\theta}(s_{t})来知道策略梯度进行学习。事实上，用Q值或者V值本质上也是用奖励来进行指导，但是用神经网络进行估计的方法可以减小方差、提高鲁棒性。除此之外，REINFORCE 算法基于蒙特卡洛采样，只能在序列结束后进行更新，这同时也要求任务具有有限的步数，而 Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。\n我们将Actor-Critic分为两部分：Actor（策略网络）和Critic（价值网络），如 图 6 所示。\n\nActor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略。\nCritic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。\n\n\n\n\n\n\n\n图 6： Actor和Critic的关系\n\n\n\nActor的更新采用策略梯度的原则，那Critic如何更新呢？我们将Critic价值网络表示为V_\\omega,参数为\\omega.于是，我们可以采用时序差分残差的学习方式，对于单个数据定义如下价值函数的损失函数： \n\\mathcal{L}(\\omega)=\\frac{1}{2}\\left(r+\\gamma V_\\omega(s_{t+1})-V_\\omega(s_t)\\right)^2\n 与DQN中一样，我们采取类似与目标网络的方法，将上式中r+\\gamma V_\\omega(s_{t+1})作为时序差分目标，不会产生梯度来更新价值函数，因此，价值函数的梯度为: \n\\nabla_\\omega\\mathcal{L}=-(r+\\gamma V_\\omega(s_{t+1})-V_\\omega(s_t))\\nabla_{\\omega}V_\\omega(s_t)\n 然后使用梯度下降方法来更新Critic价值网络参数即可。\n\n\n\\begin{algorithm} \\caption{Actor-Critic算法流程} \\begin{algorithmic} \\State \\textbf{初始化策略网络参数}$\\theta$, \\textbf{价值网络参数}$\\omega$ \\For{$e=1\\rightarrow E$} \\State \\textbf{用当前策略}$\\pi_{\\theta}$\\textbf{采样轨迹}$\\{s_1,a_1,r_1,s_2,a_2,r_2,...\\}$ \\State \\textbf{为每一步数据计算}$\\delta_t=r_t+\\gamma V_{\\omega}(s_{t+1})-V_\\omega(s_t)$ \\State \\textbf{更新价值参数}$\\omega=\\omega+\\alpha_\\omega\\sum_t\\delta_t\\nabla_{\\omega}V_\\omega(s_t)$ \\State \\textbf{更新策略参数}$\\theta=\\theta+\\alpha_\\theta\\sum_t\\delta_t\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n以上就是Actor-Critic算法的流程，下面我们用代码实现它！"
  },
  {
    "objectID": "index.html#actor-critic-代码实践",
    "href": "index.html#actor-critic-代码实践",
    "title": "强化学习基础",
    "section": "8.4 Actor-Critic 代码实践",
    "text": "8.4 Actor-Critic 代码实践\n我们想在车杆环境下进行A-C算法的实验，引入一些必要的包\n\nimport gym\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport rl_utils\nfrom tqdm import tqdm\n\n其中rl_utils文件如下所示：\n\n\nCode\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport collections\nimport random\n\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = collections.deque(maxlen=capacity) \n\n    def add(self, state, action, reward, next_state, done): \n        self.buffer.append((state, action, reward, next_state, done)) \n\n    def sample(self, batch_size): \n        transitions = random.sample(self.buffer, batch_size)\n        state, action, reward, next_state, done = zip(*transitions)\n        return np.array(state), action, reward, np.array(next_state), done \n\n    def size(self): \n        return len(self.buffer)\n\ndef moving_average(a, window_size):\n    cumulative_sum = np.cumsum(np.insert(a, 0, 0)) \n    middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size\n    r = np.arange(1, window_size-1, 2)\n    begin = np.cumsum(a[:window_size-1])[::2] / r\n    end = (np.cumsum(a[:-window_size:-1])[::2] / r)[::-1]\n    return np.concatenate((begin, middle, end))\n\ndef train_on_policy_agent(env, agent, num_episodes):\n    return_list = []\n    for i in range(10):\n        with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar:\n            for i_episode in range(int(num_episodes/10)):\n                episode_return = 0\n                transition_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}\n                state = env.reset(seed=0)[0]\n                done = False\n                while not done:\n                    action = agent.take_action(state)\n                    next_state, reward, done, _,_ = env.step(action)\n                    transition_dict['states'].append(state)\n                    transition_dict['actions'].append(action)\n                    transition_dict['next_states'].append(next_state)\n                    transition_dict['rewards'].append(reward)\n                    transition_dict['dones'].append(done)\n                    state = next_state\n                    episode_return += reward\n                return_list.append(episode_return)\n                agent.update(transition_dict)\n                if (i_episode+1) % 10 == 0:\n                    pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})\n                pbar.update(1)\n    return return_list\n\ndef train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size):\n    return_list = []\n    for i in range(10):\n        with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar:\n            for i_episode in range(int(num_episodes/10)):\n                episode_return = 0\n                state = env.reset(seed=0)[0]\n                done = False\n                steps = 0\n                while not done and steps &lt;200:\n                    steps+=1\n                    action = agent.take_action(state)\n                    next_state, reward, done, _,_ = env.step(action)\n                    replay_buffer.add(state, action, reward, next_state, done)\n                    state = next_state\n                    episode_return += reward\n                    if replay_buffer.size() &gt; minimal_size:\n                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n                        transition_dict = {'states': b_s, 'actions': b_a, 'next_states': b_ns, 'rewards': b_r, 'dones': b_d}\n                        agent.update(transition_dict)\n                return_list.append(episode_return)\n                if (i_episode+1) % 10 == 0:\n                    pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})\n                pbar.update(1)\n    return return_list\n\n\ndef compute_advantage(gamma, lmbda, td_delta):\n    td_delta = td_delta.detach().numpy()\n    advantage_list = []\n    advantage = 0.0\n    for delta in td_delta[::-1]:\n        advantage = gamma * lmbda * advantage + delta\n        advantage_list.append(advantage)\n    advantage_list.reverse()\n    return torch.tensor(advantage_list, dtype=torch.float)\n\n\n首先定义策略网络PolicyNet,其输入是某个状态，输出是该状态下的动作概率分布，这里采用在离散动作空间上的softmax()函数来实现一个可学习的多项分布。\n\nclass PolicyNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(PolicyNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return F.softmax(self.fc2(x), dim=1)\n\nA-C算法额外引入一个价值网络，接下来的代码定义价值网络ValueNet,其输入是某个状态，输出是状态的价值。\n\nclass ValueNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim):\n        super(ValueNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n现在定义ActorCritic算法，主要包含采取动作take_action()和更新网络参数update()两个函数。\n\n\nCode\nclass ActorCritic:\n    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n                 gamma, device):\n        # 策略网络\n        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n        self.critic = ValueNet(state_dim, hidden_dim).to(device)  # 价值网络\n        # 策略网络优化器\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n                                                lr=actor_lr)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=critic_lr)  # 价值网络优化器\n        self.gamma = gamma\n        self.device = device\n\n    def take_action(self, state):\n        # state = torch.tensor([state], dtype=torch.float).to(self.device)\n        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n        probs = self.actor(state)\n        action_dist = torch.distributions.Categorical(probs)\n        action = action_dist.sample()\n        return action.item()\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n            self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n\n        # 时序差分目标\n        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n                                                                       dones)\n        td_delta = td_target - self.critic(states)  # 时序差分误差\n        log_probs = torch.log(self.actor(states).gather(1, actions))\n        actor_loss = torch.mean(-log_probs * td_delta.detach())\n        # 均方误差损失函数\n        critic_loss = torch.mean(\n            F.mse_loss(self.critic(states), td_target.detach()))\n        self.actor_optimizer.zero_grad()\n        self.critic_optimizer.zero_grad()\n        actor_loss.backward()  # 计算策略网络的梯度\n        critic_loss.backward()  # 计算价值网络的梯度\n        self.actor_optimizer.step()  # 更新策略网络的参数\n        self.critic_optimizer.step()  # 更新价值网络的参数\n\n\n定义好Actor和Critic,我们就可以开始实验了，看看Actor-Critic在车杆环境下表现如何吧（下面的代码块执行时间会非常长，我暂时先不运行，只给出结果，代码是正确的）！\n\n\nCode\nactor_lr = 1e-3\ncritic_lr = 1e-2\nnum_episodes = 1000\nhidden_dim = 128\ngamma = 0.98\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n    \"cpu\")\n\nenv_name = 'CartPole-v1'\nenv = gym.make(env_name, render_mode='human')\n\ntorch.manual_seed(0)\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nagent = ActorCritic(state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n                    gamma, device)\n\nreturn_list = rl_utils.train_on_policy_agent(env, agent, num_episodes)\n\nepisodes_list = list(range(len(return_list)))\nplt.plot(episodes_list, return_list)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('Actor-Critic on {}'.format(env_name))\nplt.show()\n\nmv_return = rl_utils.moving_average(return_list, 9)\nplt.plot(episodes_list, mv_return)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('Actor-Critic on {}'.format(env_name))\nplt.show()\n\n\n\n\n\n用Actor-Critic算法寻找CartPole最优策略的结果\n\n\n根据实验结果我们可以发现，Actor-Critic 算法很快便能收敛到最优策略，并且训练过程非常稳定，价值函数的引入减小了方差，抖动情况也不严重。"
  },
  {
    "objectID": "index.html#cartpole环境",
    "href": "index.html#cartpole环境",
    "title": "强化学习基础",
    "section": "8.3 CartPole环境",
    "text": "8.3 CartPole环境\n在介绍A-C算法的代码之前，我们先介绍一下如 图 7 所示的CartPole环境，它的状态值是连续的，动作值是离散的。\n\n\n\n\n\n\n图 7： CartPole环境示意图\n\n\n\n在车杆环境中，有一辆小车，智能体的任务是通过左右移动保持车上的杆竖直，若杆的倾斜度数过大，或者车子离初始位置左右的偏离程度过大，或者坚持时间到达 200 帧，则游戏结束。智能体的状态是一个维数为 4 的向量，每一维都是连续的，其动作是离散的，动作空间大小为 2，详见 表 1 , 表 2 .\n\n\n\n表 1： CartPole环境的状态空间\n\n\n\n\n\n维度\n意义\n最小值\n最大值\n\n\n\n\n0\n车的位置\n-2.4\n2.4\n\n\n1\n车的速度\n-Inf\nInf\n\n\n2\n杆的角度\n~-41.8°\n~41.8°\n\n\n3\n杆尖端的速度\n-Inf\nInf\n\n\n\n\n\n\n\n\n\n表 2： CartPole环境的动作空间\n\n\n\n\n\n标号\n动作\n\n\n\n\n0\n向左移动小车\n\n\n1\n向右移动小车"
  },
  {
    "objectID": "index.html#小结-1",
    "href": "index.html#小结-1",
    "title": "强化学习基础",
    "section": "5.5 小结",
    "text": "5.5 小结\n本章讲解了DQN算法，其主要思想是用一个神经网络来表示最优策略的函数Q,然后利用Q-learning的思想进行参数更新。为了保证训练的稳定性和高效性，DQN 算法引入了经验回放和目标网络两大模块，使得算法在实际应用时能够取得更好的效果。"
  },
  {
    "objectID": "index.html#简介-5",
    "href": "index.html#简介-5",
    "title": "强化学习基础",
    "section": "8.1 简介",
    "text": "8.1 简介\n本书之前的章节讲解了基于值函数的方法（DQN）和基于策略的方法（REINFORCE），其中基于值函数的方法只学习一个价值函数，而基于策略的方法只学习一个策略函数。那么，一个很自然的问题是，有没有什么方法既学习价值函数，又学习策略函数呢？答案就是 Actor-Critic。Actor-Critic 是囊括一系列算法的整体架构，目前很多高效的前沿算法都属于 Actor-Critic 算法，本章接下来将会介绍一种最简单的 Actor-Critic 算法。需要明确的是，Actor-Critic 算法本质上是基于策略的算法，因为这一系列算法的目标都是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好地学习。"
  },
  {
    "objectID": "index.html#策略目标",
    "href": "index.html#策略目标",
    "title": "强化学习基础",
    "section": "9.2 策略目标",
    "text": "9.2 策略目标\n假设当前策略为\\pi_{\\theta},参数为\\theta.我们考虑如何借助当前的\\theta找到一个更优秀的\\theta',是的J(\\theta')&gt;J(\\theta). 具体来说，由于初始状态s_0的分布与策略无关，因此行数策略\\pi_{\\theta}下的优化目标J(\\theta)可以写成在新策略\\pi_{\\theta'}的期望形式:\n\nV^{\\pi_{\\theta}}(s_t)=\\mathbb{E}_{a_t\\sim\\pi_{\\theta}}[r(s_t,a_t)+\\gamma V^{\\pi_{\\theta}}(s_{t+1})]\n\n\n\\begin{aligned}\nJ(\\theta)&=\\mathbb{E}_{s_0}\\left[V^{\\pi_{\\theta}}(s_0)\\right]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta'}}\\left[\\sum_{t=0}^\\infty \\gamma^t V^{\\pi_{\\theta}}(s_t)-\\sum_{t=1}^\\infty \\gamma^t V^{\\pi_{\\theta}}(s_t)\\right]\\\\\n&=-\\mathbb{E}_{\\pi_{\\theta'}}\\left[\\sum_{t=0}^\\infty \\gamma^t (\\gamma V^{\\pi_{\\theta}}(s_{t+1})- V^{\\pi_{\\theta}}(s_t))\\right]\n\\end{aligned}\n\n基于上述等式，我们可以推导新旧策略的目标函数之间的差距：\n\n\\begin{aligned}\nJ(\\theta')-J(\\theta)&=\\mathbb{E}_{s_0}[V^{\\pi_{\\theta'}}(s_0)]-\\mathbb{E}_{s_0}[V^{\\pi_{\\theta}}(s_0)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta'}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t) \\right]+\\mathbb{E}_{\\pi_{\\theta'}}\\left[\\sum_{t=0}^\\infty \\gamma^t (\\gamma V^{\\pi_{\\theta}}(s_{t+1})- V^{\\pi_{\\theta}}(s_t))\\right]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta'}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t[r(s_t,a_t)+\\gamma V^{\\pi_{\\theta}}(s_{t+1}-V^{\\pi_{\\theta}}(s_t))] \\right]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta'}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t A^{\\pi_{\\theta}}(s_t,a_t)\\right]\\\\\n&=\\sum_{t=0}^{\\infty}\\gamma^t\\mathbb{E}_{s_t\\sim P_t^{\\pi_{\\theta'}}}\\mathbb{E}_{a_t\\sim \\pi_{\\theta'}(\\cdot|s_t)}[A^{\\pi_{\\theta}}(s_t,a_t)]\\\\\n&=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta'}}}\\mathbb{E}_{a\\sim \\pi_{\\theta'}(\\cdot|s)}[A^{\\pi_\\theta}(s,a)]\n\\end{aligned}\n 其中时许差分残差定义为优势函数A,最后一个等号的成立用到了状态访问分布的定义：\\nu^{\\pi}(s)=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^t P_t^{\\pi}(s),所以只要我们找到一个新策略，使得\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta'}}}\\mathbb{E}_{a\\sim \\pi_{\\theta'}(\\cdot|s)}[A^{\\pi_\\theta}(s,a)]\\geq0,就能保证策略性能单调递增，即J(\\theta')\\geq J(\\theta).\n但直接求解该式是非常困难的，因为\\pi_{\\theta'}是我们需要求解的策略，但我们又要用其来收集样本。把所有可能的新策略都拿来收集数据，然后判断哪个策略满足上述条件的做法显然是不现实的。于是 TRPO 做了一步近似操作，对状态访问分布进行了相应处理。具体而言，忽略两个策略之间的状态访问分布变化，直接采用旧的策略\\pi_{\\theta}的状态分布，定义如下替代优化目标：\n\nL_\\theta(\\theta')=J(\\theta)+\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta}}}\\mathbb{E}_{a\\sim \\pi_{\\theta'}(\\cdot|s)}[A^{\\pi_{\\theta}}(s,a)]\n\n当新旧策略非常接近时，状态访问分布变化很小，这么近似是合理的。其中，动作仍然用新策略\\pi_{\\theta'}采样得到，我们可以用重要性采样对动作分布进行处理：\n\nL_\\theta(\\theta')=J(\\theta)+\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta}}}\\mathbb{E}_{a\\sim \\pi_{\\theta}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta}(a|s)}A^{\\pi_{\\theta}}(s,a)\\right]\n\n这样，我们就可以基于旧策略\\pi_{\\theta}已经采样出的数据来估计并优化新策略\\pi_{\\theta'}了。为了保证新旧策略足够接近，TRPO使用了库尔贝克-莱布勒（Kullback-Leibler，KL）散度来衡量策略之间的距离，并给出了整体的优化公式：\n\n\\begin{aligned}\n\\max_{\\theta'}\\quad & L_{\\theta}(\\theta')\\\\\n\\text{s.t.} \\quad & \\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\left[ D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta'}(\\cdot|s)) \\right]\\leq \\delta\n\\end{aligned}\n\\tag{1}\n这里的不等式约束定义了策略空间中的一个 KL 球，被称为信任区域。在这个区域中，可以认为当前学习策略和环境交互的状态分布与上一轮策略最后采样的状态分布一致，进而可以基于一步行动的重要性采样方法使当前学习策略稳定提升。TRPO 背后的原理如@fig-TRPOshiyitu 所示。\n\n\n\n\n\n\n图 8： TRPO原理示意图\n\n\n\n左图表示当完全不设置信任区域时，策略的梯度更新可能导致策略的性能骤降；右图表示当设置了信任区域时，可以保证每次策略的梯度更新都能来带性能的提升。"
  },
  {
    "objectID": "index.html#近似求解",
    "href": "index.html#近似求解",
    "title": "强化学习基础",
    "section": "9.3 近似求解",
    "text": "9.3 近似求解\n直接求解 Equation 1 带约束的优化问题比较麻烦，TRPO在其具体视线中做了一步近似操作来快速求解。为方便起见，我们在接下来的式子中用\\theta_k来代替之前的\\theta,表示这是第k次迭代之后的策略。首先对目标函数和约束在\\theta_k进行泰勒展开，分别用1阶、2阶进行近似：\n\n\\begin{aligned}\n&\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)\\right]\\approx g^\\top(\\theta'-\\theta_k)\\\\\n&\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\left[ D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta'}(\\cdot|s)) \\right]\\approx\\frac{1}{2}(\\theta'-\\theta_k)^\\top H (\\theta'-\\theta_k)\n\\end{aligned}\n 其中g=\\nabla_{\\theta'}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)\\right]表示目标函数的梯度，H=\\mathbf{H}[\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\left[ D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta'}(\\cdot|s)) \\right]]表示策略之间平均KL距离的Hessian矩阵。\n于是我们的优化目标变成了:\n\n\\theta_{k+1}=\\underset{\\theta'}{\\arg\\max} g^\\top(\\theta'-\\theta_k)\\quad \\text{s.t. }\\frac{1}{2}(\\theta'-\\theta_k)^\\top H (\\theta'-\\theta_k)\\leq \\delta\n\n此时，我们可以用Karush-Kuhn-Tucker (KKT) 条件构造拉格朗日函数后直接导出上述问题的解：\n\n\\theta_{k+1}=\\theta_k+\\sqrt{\\frac{2\\delta}{g^\\top H^{-1}g}}H^{-1}g"
  },
  {
    "objectID": "index.html#共轭梯度",
    "href": "index.html#共轭梯度",
    "title": "强化学习基础",
    "section": "9.4 共轭梯度",
    "text": "9.4 共轭梯度\n一般来说，用神经网络表示的策略函数的参数数量都是成千上万的，计算和存储黑塞矩阵的逆矩阵会耗费大量的内存资源和时间。TRPO 通过共轭梯度法（conjugate gradient method）回避了这个问题，它的核心思想是直接计算x=H^{-1}g,x即参数更新方向，假设满足KL距离约束的参数更新时的最大步长为\\beta,于是根据KL距离约束条件，有\\frac{1}{2}(\\beta x)^\\top H(\\beta x)=\\delta.求解\\beta=\\sqrt{\\frac{2\\delta}{x^\\top Hx}}.此时参数更新方式为 \n\\theta_{k+1}=\\theta_k+\\sqrt{\\frac{2\\delta}{x^\\top Hx}}x\n\n因此，只要可以直接计算x=H^{-1}g,就可以根据该式更新参数，问题转化为解Hx=g.实际上H为对称正定矩阵，所以我们可以使用共轭梯度法来求解。共轭梯度法的具体流程如下：\n\n\n\\begin{algorithm} \\caption{共轭梯度法流程} \\begin{algorithmic} \\State \\textbf{初始化}$r_0=g-Hx_0,p_0=r_0,x_0=0$ \\For{$k=0\\rightarrow N$} \\State $\\alpha_k=\\frac{r_k^\\top r_k}{p_k^\\top Hp_k}$ \\State $x_{k+1}=x_k+\\alpha_kp_k$ \\State $r_{k+1}=r_k-\\alpha_kHp_k$ \\If{$r_{k+1}^\\top r_{k+1}$\\textbf{非常小}} \\State \\textbf{退出循环} \\EndIf \\State $\\beta_k=\\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$ \\State $p_{k+1}=r_{k+1}+\\beta_kp_k$ \\EndFor \\State \\textbf{输出}$x_{N+1}$ \\end{algorithmic} \\end{algorithm}\n\n\n在共轭梯度运算过程中，直接计算\\alpha_k,r_{k+1}需要计算和存储Hessian矩阵H,为了避免这种大矩阵的出现，我们只计算Hx向量，而不直接计算和存储H矩阵。这样做比较容易，因为对于任意的列向量v,容易验证： \nHv=\\nabla_{\\theta} \\left(\\left(\\nabla_{\\theta}(D^{\\nu^{\\pi_{\\theta_k}}}_{KL}(\\pi_{\\theta_k},\\pi_{\\theta'}) )\\right)^\\top \\right)v=\\nabla_{\\theta} \\left(\\left(\\nabla_{\\theta}(D^{\\nu^{\\pi_{\\theta_k}}}_{KL}(\\pi_{\\theta_k},\\pi_{\\theta'}) )\\right)^\\top v \\right)\n\n即先用梯度和向量v点乘后再计算梯度。"
  },
  {
    "objectID": "index.html#线性搜索",
    "href": "index.html#线性搜索",
    "title": "强化学习基础",
    "section": "9.5 线性搜索",
    "text": "9.5 线性搜索\n由于TRPO算法用到了泰勒展开的一阶和二阶近似，并非精确求解，\\theta'可能未必比\\theta_k好，或未必能满足KL散度的限制。TRPO在每次迭代的最后进行一次线性搜索，以确保找到满足条件。具体来说，就是找到一个最小的非负整数i,使得按照\n\n\\theta_{k+1}=\\theta_k+\\alpha^i \\sqrt{\\frac{2\\delta}{x^\\top Hx}}x\n 求出的\\theta_{k+1}依然满足最初的KL散度限制，并且确实能够提升目标函数L_{\\theta_k},这其中\\alpha\\in(0,1)是一个决定线性搜索长度的超参数。\n至此，我们已经基本清楚了TRPO算法的大致过程，具体算法流程如下：\n\n\n\\begin{algorithm} \\caption{TRPO算法流程} \\begin{algorithmic} \\State \\textbf{初始化策略网络参数}$\\theta,$\\textbf{价值网络参数}$\\omega$ \\For{$e=1\\rightarrow E$} \\State \\textbf{用当前策略}$\\pi_{\\theta}$\\textbf{采样轨迹}$\\{s_1,a_1,r_1,s_2,a_2,r_2,\\dots\\}$ \\State \\textbf{根据收集到的数据和价值网络估计每个状态动作对的优势}$A(s_t,a_t)$ \\State \\textbf{计算策略目标函数的梯度}$g$ \\State \\textbf{用共轭梯度法计算}$x=H^{-1}g$ \\State \\textbf{用线性搜索找到一个}$i$\\textbf{值，并更新策略网络参数} \\State $\\theta_{k+1}=\\theta_k+\\alpha^i \\sqrt{\\frac{2\\delta}{x^\\top Hx}}x$,其中$i\\in\\{1,2,...,K\\}$\\textbf{为能提升策略并满足KL距离限制的最小整数} \\State \\textbf{更新价值网络参数（与A-C中的更新方法相同）} \\EndFor \\State \\textbf{输出}$x_{N+1}$ \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "index.html#广义优势估计",
    "href": "index.html#广义优势估计",
    "title": "强化学习基础",
    "section": "9.6 广义优势估计",
    "text": "9.6 广义优势估计\n从上一节内容中发现，我们尚未得知如何估计优势函数A(s_t,a_t).目前比较常用的一种方法是广义优势估计（Generalized Advantage Estimation，GAE），接下来我们简单介绍一下GAE的做法。首先，用\\delta_t=r_t+\\gamma V(s_{t+1})-V(s_t)表示时序差分误差，其中V是一个已经学习过的状态价值函数，于是根据多步时序差分的思想，有：\n\n\\begin{aligned}\nA_t^{(1)}=\\delta_t & =-V(s_t)+r_t+\\gamma V(s_{t+1})\\\\\nA_t^{(2)}=\\delta_t+\\gamma \\delta_{t+1} & =-V(s_t)+r_t+\\gamma r_{t+1}+\\gamma^2 V(s_{t+2})\\\\\n\\cdots\\\\\nA^{(k)}_t=\\sum_{l=0}^{k-1}\\gamma^l\\delta_{t+l}&= -V(s_t)+r_t+\\gamma r_{t+1}+...+\\gamma^{k-1}r_{t+k-1}+\\gamma^k V(s_{t+k})\n\\end{aligned}\n\n然后，GAE将这些不同步数的优势估计进行指数加权平均：\n\n\\begin{aligned}\nA_t^{GAE}&=(1-\\lambda)(A_t^{(1)}+\\lambda A_t^{(2)}+\\lambda^2 A_t^{(3)}+...)\\\\\n&=(1-\\lambda)(\\delta_t+\\lambda(\\delta_t+\\gamma \\delta_{t+1})+...)\\\\\n&=(1-\\lambda)[\\delta_t(1+\\lambda+\\lambda^2+...)+\\gamma\\delta_{t+1}(\\lambda+\\lambda^2+\\lambda^3+...)]\\\\\n&=(1-\\lambda)\\left( \\delta_t\\frac{1}{1-\\lambda}+\\gamma\\delta_{t+1}\\frac{\\lambda}{1-\\lambda}+... \\right)\\\\\n&=\\sum_{l=0}^{\\infty}(\\gamma\\lambda)^l\\delta_{t+l}\n\\end{aligned}\n 其中\\lambda\\in[0,1]是在GAE中额外引入的一个超参数，当\\lambda=0时，A_t^{GAE}=\\delta_t即是仅仅只看一步差分得到的优势；当\\lambda=1时，A_t^{GAE}=\\sum_{l=0}^{\\infty}\\gamma^l\\delta_{t+l}=\\sum_{l=0}^{\\infty}\\gamma^lr_{t+l}-V(s_t),则是看每一步差分得到优势的完全平均值。\n下面是一段GAE代码，给定\\gamma,\\lambda以及每个时间步的\\delta_t之后，可以根据公式直接进行优势估计（下面的代码在rl.utils.py中已给出）。\n\n\nCode\ndef compute_advantage(gamma, lmbda, td_delta):\n    td_delta = td_delta.detach().numpy()\n    advantage_list = []\n    advantage = 0.0\n    for delta in td_delta[::-1]:\n        advantage = gamma * lmbda * advantage + delta\n        advantage_list.append(advantage)\n    advantage_list.reverse()\n    return torch.tensor(advantage_list, dtype=torch.float)"
  },
  {
    "objectID": "index.html#trpo代码实践",
    "href": "index.html#trpo代码实践",
    "title": "强化学习基础",
    "section": "9.7 TRPO代码实践",
    "text": "9.7 TRPO代码实践\n首先导入一些必要的库\n\n\nCode\nimport torch\nimport numpy as np\nimport gym\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport rl_utils\nimport copy\n\n\n然后定义策略网络和价值网络以及TRPO算法，在动作时离散的情况下如下\n\n\nCode\nclass PolicyNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(PolicyNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return F.softmax(self.fc2(x), dim=1)\n\n\nclass ValueNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim):\n        super(ValueNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nclass TRPO:\n    \"\"\" TRPO算法 \"\"\"\n    def __init__(self, hidden_dim, state_space, action_space, lmbda,\n                 kl_constraint, alpha, critic_lr, gamma, device):\n        state_dim = state_space.shape[0]\n        action_dim = action_space.n\n        # 策略网络参数不需要优化器更新\n        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=critic_lr)\n        self.gamma = gamma\n        self.lmbda = lmbda  # GAE参数\n        self.kl_constraint = kl_constraint  # KL距离最大限制\n        self.alpha = alpha  # 线性搜索参数\n        self.device = device\n\n    def take_action(self, state):\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        probs = self.actor(state)\n        action_dist = torch.distributions.Categorical(probs)\n        action = action_dist.sample()\n        return action.item()\n\n    def hessian_matrix_vector_product(self, states, old_action_dists, vector):\n        # 计算黑塞矩阵和一个向量的乘积\n        new_action_dists = torch.distributions.Categorical(self.actor(states))\n        kl = torch.mean(\n            torch.distributions.kl.kl_divergence(old_action_dists,\n                                                 new_action_dists))  # 计算平均KL距离\n        kl_grad = torch.autograd.grad(kl,\n                                      self.actor.parameters(),\n                                      create_graph=True)\n        kl_grad_vector = torch.cat([grad.view(-1) for grad in kl_grad])\n        # KL距离的梯度先和向量进行点积运算\n        kl_grad_vector_product = torch.dot(kl_grad_vector, vector)\n        grad2 = torch.autograd.grad(kl_grad_vector_product,\n                                    self.actor.parameters())\n        grad2_vector = torch.cat([grad.view(-1) for grad in grad2])\n        return grad2_vector\n\n    def conjugate_gradient(self, grad, states, old_action_dists):  # 共轭梯度法求解方程\n        x = torch.zeros_like(grad)\n        r = grad.clone()\n        p = grad.clone()\n        rdotr = torch.dot(r, r)\n        for i in range(10):  # 共轭梯度主循环\n            Hp = self.hessian_matrix_vector_product(states, old_action_dists,\n                                                    p)\n            alpha = rdotr / torch.dot(p, Hp)\n            x += alpha * p\n            r -= alpha * Hp\n            new_rdotr = torch.dot(r, r)\n            if new_rdotr &lt; 1e-10:\n                break\n            beta = new_rdotr / rdotr\n            p = r + beta * p\n            rdotr = new_rdotr\n        return x\n\n    def compute_surrogate_obj(self, states, actions, advantage, old_log_probs,\n                              actor):  # 计算策略目标\n        log_probs = torch.log(actor(states).gather(1, actions))\n        ratio = torch.exp(log_probs - old_log_probs)\n        return torch.mean(ratio * advantage)\n\n    def line_search(self, states, actions, advantage, old_log_probs,\n                    old_action_dists, max_vec):  # 线性搜索\n        old_para = torch.nn.utils.convert_parameters.parameters_to_vector(\n            self.actor.parameters())\n        old_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                             old_log_probs, self.actor)\n        for i in range(15):  # 线性搜索主循环\n            coef = self.alpha**i\n            new_para = old_para + coef * max_vec\n            new_actor = copy.deepcopy(self.actor)\n            torch.nn.utils.convert_parameters.vector_to_parameters(\n                new_para, new_actor.parameters())\n            new_action_dists = torch.distributions.Categorical(\n                new_actor(states))\n            kl_div = torch.mean(\n                torch.distributions.kl.kl_divergence(old_action_dists,\n                                                     new_action_dists))\n            new_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                                 old_log_probs, new_actor)\n            if new_obj &gt; old_obj and kl_div &lt; self.kl_constraint:\n                return new_para\n        return old_para\n\n    def policy_learn(self, states, actions, old_action_dists, old_log_probs,\n                     advantage):  # 更新策略函数\n        surrogate_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                                   old_log_probs, self.actor)\n        grads = torch.autograd.grad(surrogate_obj, self.actor.parameters())\n        obj_grad = torch.cat([grad.view(-1) for grad in grads]).detach()\n        # 用共轭梯度法计算x = H^(-1)g\n        descent_direction = self.conjugate_gradient(obj_grad, states,\n                                                    old_action_dists)\n\n        Hd = self.hessian_matrix_vector_product(states, old_action_dists,\n                                                descent_direction)\n        max_coef = torch.sqrt(2 * self.kl_constraint /\n                              (torch.dot(descent_direction, Hd) + 1e-8))\n        new_para = self.line_search(states, actions, advantage, old_log_probs,\n                                    old_action_dists,\n                                    descent_direction * max_coef)  # 线性搜索\n        torch.nn.utils.convert_parameters.vector_to_parameters(\n            new_para, self.actor.parameters())  # 用线性搜索后的参数更新策略\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n            self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n                                                                       dones)\n        td_delta = td_target - self.critic(states)\n        advantage = compute_advantage(self.gamma, self.lmbda,\n                                      td_delta.cpu()).to(self.device)\n        old_log_probs = torch.log(self.actor(states).gather(1,\n                                                            actions)).detach()\n        old_action_dists = torch.distributions.Categorical(\n            self.actor(states).detach())\n        critic_loss = torch.mean(\n            F.mse_loss(self.critic(states), td_target.detach()))\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()  # 更新价值函数\n        # 更新策略函数\n        self.policy_learn(states, actions, old_action_dists, old_log_probs,\n                          advantage)\n\n\n而如果是与连续动作交互的环境，需要对上面的代码做一些修改。对于策略网络，因为环境是连续动作的，所以策略网络分别输出表示动作分布的高斯分布的均值和标准差。\n\n\nCode\nclass PolicyNetContinuous(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(PolicyNetContinuous, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        mu = 2.0 * torch.tanh(self.fc_mu(x))\n        std = F.softplus(self.fc_std(x))\n        return mu, std  # 高斯分布的均值和标准差\n\n\nclass TRPOContinuous:\n    \"\"\" 处理连续动作的TRPO算法 \"\"\"\n    def __init__(self, hidden_dim, state_space, action_space, lmbda,\n                 kl_constraint, alpha, critic_lr, gamma, device):\n        state_dim = state_space.shape[0]\n        action_dim = action_space.shape[0]\n        self.actor = PolicyNetContinuous(state_dim, hidden_dim,\n                                         action_dim).to(device)\n        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=critic_lr)\n        self.gamma = gamma\n        self.lmbda = lmbda\n        self.kl_constraint = kl_constraint\n        self.alpha = alpha\n        self.device = device\n\n    def take_action(self, state):\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        mu, std = self.actor(state)\n        action_dist = torch.distributions.Normal(mu, std)\n        action = action_dist.sample()\n        return [action.item()]\n\n    def hessian_matrix_vector_product(self,\n                                      states,\n                                      old_action_dists,\n                                      vector,\n                                      damping=0.1):\n        mu, std = self.actor(states)\n        new_action_dists = torch.distributions.Normal(mu, std)\n        kl = torch.mean(\n            torch.distributions.kl.kl_divergence(old_action_dists,\n                                                 new_action_dists))\n        kl_grad = torch.autograd.grad(kl,\n                                      self.actor.parameters(),\n                                      create_graph=True)\n        kl_grad_vector = torch.cat([grad.view(-1) for grad in kl_grad])\n        kl_grad_vector_product = torch.dot(kl_grad_vector, vector)\n        grad2 = torch.autograd.grad(kl_grad_vector_product,\n                                    self.actor.parameters())\n        grad2_vector = torch.cat(\n            [grad.contiguous().view(-1) for grad in grad2])\n        return grad2_vector + damping * vector\n\n    def conjugate_gradient(self, grad, states, old_action_dists):\n        x = torch.zeros_like(grad)\n        r = grad.clone()\n        p = grad.clone()\n        rdotr = torch.dot(r, r)\n        for i in range(10):\n            Hp = self.hessian_matrix_vector_product(states, old_action_dists,\n                                                    p)\n            alpha = rdotr / torch.dot(p, Hp)\n            x += alpha * p\n            r -= alpha * Hp\n            new_rdotr = torch.dot(r, r)\n            if new_rdotr &lt; 1e-10:\n                break\n            beta = new_rdotr / rdotr\n            p = r + beta * p\n            rdotr = new_rdotr\n        return x\n\n    def compute_surrogate_obj(self, states, actions, advantage, old_log_probs,\n                              actor):\n        mu, std = actor(states)\n        action_dists = torch.distributions.Normal(mu, std)\n        log_probs = action_dists.log_prob(actions)\n        ratio = torch.exp(log_probs - old_log_probs)\n        return torch.mean(ratio * advantage)\n\n    def line_search(self, states, actions, advantage, old_log_probs,\n                    old_action_dists, max_vec):\n        old_para = torch.nn.utils.convert_parameters.parameters_to_vector(\n            self.actor.parameters())\n        old_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                             old_log_probs, self.actor)\n        for i in range(15):\n            coef = self.alpha**i\n            new_para = old_para + coef * max_vec\n            new_actor = copy.deepcopy(self.actor)\n            torch.nn.utils.convert_parameters.vector_to_parameters(\n                new_para, new_actor.parameters())\n            mu, std = new_actor(states)\n            new_action_dists = torch.distributions.Normal(mu, std)\n            kl_div = torch.mean(\n                torch.distributions.kl.kl_divergence(old_action_dists,\n                                                     new_action_dists))\n            new_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                                 old_log_probs, new_actor)\n            if new_obj &gt; old_obj and kl_div &lt; self.kl_constraint:\n                return new_para\n        return old_para\n\n    def policy_learn(self, states, actions, old_action_dists, old_log_probs,\n                     advantage):\n        surrogate_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                                   old_log_probs, self.actor)\n        grads = torch.autograd.grad(surrogate_obj, self.actor.parameters())\n        obj_grad = torch.cat([grad.view(-1) for grad in grads]).detach()\n        descent_direction = self.conjugate_gradient(obj_grad, states,\n                                                    old_action_dists)\n        Hd = self.hessian_matrix_vector_product(states, old_action_dists,\n                                                descent_direction)\n        max_coef = torch.sqrt(2 * self.kl_constraint /\n                              (torch.dot(descent_direction, Hd) + 1e-8))\n        new_para = self.line_search(states, actions, advantage, old_log_probs,\n                                    old_action_dists,\n                                    descent_direction * max_coef)\n        torch.nn.utils.convert_parameters.vector_to_parameters(\n            new_para, self.actor.parameters())\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n        rewards = (rewards + 8.0) / 8.0  # 对奖励进行修改,方便训练\n        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n                                                                       dones)\n        td_delta = td_target - self.critic(states)\n        advantage = compute_advantage(self.gamma, self.lmbda,\n                                      td_delta.cpu()).to(self.device)\n        mu, std = self.actor(states)\n        old_action_dists = torch.distributions.Normal(mu.detach(),\n                                                      std.detach())\n        old_log_probs = old_action_dists.log_prob(actions)\n        critic_loss = torch.mean(\n            F.mse_loss(self.critic(states), td_target.detach()))\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n        self.policy_learn(states, actions, old_action_dists, old_log_probs,\n                          advantage)"
  },
  {
    "objectID": "index.html#简介-6",
    "href": "index.html#简介-6",
    "title": "强化学习基础",
    "section": "9.1 简介",
    "text": "9.1 简介\n本书之前介绍的基于策略的方法包括策略梯度算法和Actor-Critic算法。这两种方法虽然简单直观，但在实际应用过程中会遇到训练不稳定的情况。回顾一下基于策略的方法：参数化智能体的策略，并设计衡量策略好坏的函数，通过梯度上升的方法来最大化这个目标函数使得策略最优。具体来说，假设\\theta表示策略\\pi_{\\theta}的参数。定义J(\\theta)=\\mathbb{E}_{\\s_0}\\left[V^{\\pi_\\theta}(s_0)\\right]=\\mathbb{E}_{\\pi_{\\theta}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^tr(s_t,a_t) \\right],基于策略的方法的目标是找到\\theta^*=\\arg\\max_\\theta J(\\theta),策略梯度算法主要沿着\\nabla_{\\theta}J(\\theta)方向迭代更新策略参数\\theta.但是这种算法有一个明显的缺点：当策略网络是深度模型时，沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果。\n针对以上问题，我们考虑在更新时找到一块信任区域（trust region），在这个区域上更新策略时能够得到某种策略性能的安全性保证，这就是信任区域策略优化（trust region policy optimization，TRPO）算法的主要思想。TRPO 算法在 2015 年被提出，它在理论上能够保证策略学习的性能单调性，并在实际应用中取得了比策略梯度算法更好的效果。"
  },
  {
    "objectID": "index.html#ppo-惩罚",
    "href": "index.html#ppo-惩罚",
    "title": "强化学习基础",
    "section": "10.2 PPO-惩罚",
    "text": "10.2 PPO-惩罚\nPPO-惩罚（PPO-Penalty）用拉格朗日乘数法直接将 KL 散度的限制放进了目标函数中，这就变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数。即： \n\\underset{\\theta}{\\arg\\max}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim\\pi_{\\theta_k}(\\cdot|s)}\\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)-\\beta D_{KL}[\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta}(\\cdot|s)] \\right]\n\n令d_k=D_{KL}^{\\nu^{\\pi_{\\theta_k}}}(\\pi_{\\theta_k},\\pi_{\\theta}),\\beta的更新规则如下\n1. 如果d_k&lt;\\delta/1.5,那么\\beta_{k+1}=\\beta/2\n2. 如果d_k&lt;\\delta\\times 1.5,那么\\beta_{k+1}=\\beta\\times 2\n3. 否则\\beta_{k+1}=\\beta_k\n其中\\delta是事先设定的一个超参数，用于限制学习策略和之前一轮策略的差距。"
  },
  {
    "objectID": "index.html#ppo-截断",
    "href": "index.html#ppo-截断",
    "title": "强化学习基础",
    "section": "10.3 PPO-截断",
    "text": "10.3 PPO-截断\nPPO 的另一种形式 PPO-截断（PPO-Clip）更加直接，它在目标函数中进行限制，以保证新的参数和旧的参数的差距不会太大，即：\n\n\\underset{\\theta}{\\arg\\max}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim\\pi_{\\theta_k}(\\cdot|s)}\\left[\n    \\min \\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a),\n    \\text{clip}\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)},1-\\epsilon,1+\\epsilon \\right)A^{\\pi_{\\theta_k}}(s,a)\\right) \\right]\n 其中\\text{clip}(x,l,r):=\\max(\\min(x,r),l),即把x限制在[l,r]内。上式中\\epsilon是一个超参数，表示进行截断的范围。\n如果A^{\\pi_{\\theta_k}}(s,a)&gt;0说明这个动作的价值高于平均，最大化这个式子会增大\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)},但不会让其超过1+\\epsilo;反之，如果A^{\\pi_{\\theta_k}}(s,a)&lt;0说明这个动作的价值低于平均，最大化这个式子会减小\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)},但不会让其超过1-\\epsilon.如图 图 9 所示。\n\n\n\n\n\n\n图 9： PPO-截断示意图"
  },
  {
    "objectID": "index.html#倒立摆环境",
    "href": "index.html#倒立摆环境",
    "title": "强化学习基础",
    "section": "10.4 倒立摆环境",
    "text": "10.4 倒立摆环境\n下面我们介绍一下倒立摆（Inverted Pendulum）环境，该环境下有一个处于随机位置的倒立摆，如图 图 10 所示。环境的状态包括倒立摆角度的正弦值\\sin \\theta,余弦值\\cos \\theta,角速度\\dot{\\theta};动作为对倒立摆施加的力矩，详见 表 3 和 表 4 。每一步都会根据当前倒立摆的状态的好坏给予智能体不同的奖励，该环境的奖励函数为-(\\theta^2+0.1\\dot{\\theta}^2+0.001a^2),倒立摆向上保持直立不动时奖励为0，倒立摆在其他位置时奖励为负数，环境本身没有终止状态，运行200步后游戏自动结束。\n\n\n\n\n\n\n图 10： Pendulum环境示意图\n\n\n\n\n\n\n表 3： Pendulum环境的状态空间\n\n\n\n\n\n标号\n名称\n最小值\n最大值\n\n\n\n\n0\n\\cos\\theta\n-1.0\n1.0\n\n\n1\n\\sin\\theta\n-1.0\n1.0\n\n\n2\n\\dot{\\theta}\n-8.0\n8.0\n\n\n\n\n\n\n\n\n\n表 4： Pendulum环境的动作空间\n\n\n\n\n\n标号\n动作\n最小值\n最大值\n\n\n\n\n0\n力矩\n-2.0\n2.0\n\n\n\n\n\n\n力矩大小是在[-2,2]范围内的连续值。"
  },
  {
    "objectID": "index.html#ppo-代码实践",
    "href": "index.html#ppo-代码实践",
    "title": "强化学习基础",
    "section": "10.5 PPO 代码实践",
    "text": "10.5 PPO 代码实践\n下面我们在倒立摆环境中测试PPO算法，大量实验表明PPO-截断的表现总好于PPO-惩罚，因此我们下面专注于PPO-截断的代码实现。由于倒立摆是与连续动作交互的环境，同TRPO算法一样，我们让策略网络输出连续动作的高斯分布的均值和标准差，后续的连续动作则在该高斯分布中采样得到。\n\n\nCode\nimport gym\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport rl_utils\n\n\nclass PolicyNetContinuous(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(PolicyNetContinuous, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        mu = 2.0 * torch.tanh(self.fc_mu(x))\n        std = F.softplus(self.fc_std(x))\n        return mu, std\n\n\nclass PPOContinuous:\n    ''' 处理连续动作的PPO算法 '''\n    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n                 lmbda, epochs, eps, gamma, device):\n        self.actor = PolicyNetContinuous(state_dim, hidden_dim,\n                                         action_dim).to(device)\n        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n                                                lr=actor_lr)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=critic_lr)\n        self.gamma = gamma\n        self.lmbda = lmbda\n        self.epochs = epochs\n        self.eps = eps\n        self.device = device\n\n    def take_action(self, state):\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        mu, sigma = self.actor(state)\n        action_dist = torch.distributions.Normal(mu, sigma)\n        action = action_dist.sample()\n        return [action.item()]\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n        rewards = (rewards + 8.0) / 8.0  # 和TRPO一样,对奖励进行修改,方便训练\n        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n                                                                       dones)\n        td_delta = td_target - self.critic(states)\n        advantage = rl_utils.compute_advantage(self.gamma, self.lmbda,\n                                               td_delta.cpu()).to(self.device)\n        mu, std = self.actor(states)\n        action_dists = torch.distributions.Normal(mu.detach(), std.detach())\n        # 动作是正态分布\n        old_log_probs = action_dists.log_prob(actions)\n\n        for _ in range(self.epochs):\n            mu, std = self.actor(states)\n            action_dists = torch.distributions.Normal(mu, std)\n            log_probs = action_dists.log_prob(actions)\n            ratio = torch.exp(log_probs - old_log_probs)\n            surr1 = ratio * advantage\n            surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage\n            actor_loss = torch.mean(-torch.min(surr1, surr2))\n            critic_loss = torch.mean(\n                F.mse_loss(self.critic(states), td_target.detach()))\n            self.actor_optimizer.zero_grad()\n            self.critic_optimizer.zero_grad()\n            actor_loss.backward()\n            critic_loss.backward()\n            self.actor_optimizer.step()\n            self.critic_optimizer.step()\n\n\n\n\n\n\n\n\nTip\n\n\n\n下面将会减少一些代码实践，因为研究方向更偏向强化学习在金融方面的应用，也许会写一些强化学习解决金融问题的方法和代码。"
  },
  {
    "objectID": "index.html#简介-7",
    "href": "index.html#简介-7",
    "title": "强化学习基础",
    "section": "10.1 简介",
    "text": "10.1 简介\n上一章介绍的TRPO算法在很多场景上的应用都很成功，但是我们也发现它的计算过程非常复杂，每一步更新的运算量非常大。于是，TRPO 算法的改进版——PPO 算法在 2017 年被提出，PPO 基于 TRPO 的思想，但是其算法实现更加简单。并且大量的实验结果表明，与 TRPO 相比，PPO 能学习得一样好（甚至更快），这使得 PPO 成为非常流行的强化学习算法。如果我们想要尝试在一个新的环境中使用强化学习算法，那么 PPO 就属于可以首先尝试的算法。\n回忆一下TRPO的优化目标可以发现 TRPO 使用泰勒展开近似、共轭梯度、线性搜索等方法直接求解。PPO 的优化目标与 TRPO 相同，但 PPO 用了一些相对简单的方法来求解。具体来说，PPO 有两种形式，一是 PPO-惩罚，二是 PPO-截断，我们接下来对这两种形式进行介绍。"
  },
  {
    "objectID": "index.html#ddpg-算法-1",
    "href": "index.html#ddpg-算法-1",
    "title": "强化学习基础",
    "section": "11.2 DDPG 算法",
    "text": "11.2 DDPG 算法\n之前我们学习的测录是随机性的，可以表示为a\\sim\\pi_{\\theta}(\\cdot|s);而如果策略是确定性的，则可以记为a=\\mu_{\\theta}(s).与策略梯度定理类似，我们可以推导出确定性策略梯度定理（Deterministic policy gradient theorem）:\n\n\\nabla_{\\theta}J(\\pi_{\\theta})=\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\beta}}}\\left[ \\nabla_{\\theta}\\mu_{\\theta}(s)\\nabla_a Q^\\mu_\\omega (s,a)|_{a=\\mu_{\\theta}(s)} \\right]\n\\tag{2}\n其中\\pi_{\\beta}是用来收集数据的行为策略。我们可以这样理解这个定理：假设现在已经有函数Q,给定一个状态s,但由于现在动作空间是无限的，无法通过遍历所有动作得到Q值最大的动作，因此我们想用策略\\mu找到使Q(s,a)值最大的动作a,即\\mu(s)=\\arg\\max_{a}Q(s,a).此时，Q就是Critic，\\mu就是Actor，这是一个Actor-Critic的框架，如 图 11 所示。\n\n\n\n\n\n\n图 11： DDPG中的Actor网络和Critic网络，以倒立摆环境为例\n\n\n\n那如何得到这个\\mu呢？首先用Q对\\mu_{\\theta}求导\\nabla_{\\theta}Q(s,\\mu_{\\theta}(s)),其中会用到梯度的链式法则，先对a求导，再对\\theta求导。然后通过梯度上升的方法来最大化函数Q,得到Q值最大的动作。– Equation 2 的推导也许会在后面给出–\n下面我们来看一下DDPG算法的细节。DDPG要用到四个神经网络，其中Actor和Critic各用一个网络，此外它们都各自有一个目标网络。 DDPG 中 Actor 也需要目标网络因为目标网络也会被用来计算目标Q值。 DDPG中目标网络的更新和DQN中略有不同：在DQN中，每隔一段时间将Q网络直接复制给目标Q网络；而在DDPG中，目标Q网络的更新采取的是一种软更新的方式，即让目标Q网络缓慢更新，逐渐接近Q网络，其公式为： \n\\omega^-\\leftarrow \\tau \\omega+(1-\\tau)\\omega^-\n 通常\\tau是一个比较小的数，当\\tau=1时，就和DQN的更新方式一致。而目标\\mu网络也使用这种软更新的方式。\n另外，由于函数Q存在Q值过高估计的问题，DDPG采用了Double DQN中的技术来更新Q网络。但是，由于DDPG采用的是确定性策略，它本身的探索仍然十分有限。回忆一下DQN算法，它的探索主要由\\epsilon-贪婪策略的行为策略产生。同样作为一种离线策略的算法，DDPG在行为策略上引入一个随机噪声\\mathcal{N}来进行探索，下面看一下DDPG的具体算法流程。\n\n\n\\begin{algorithm} \\caption{DDPG算法流程} \\begin{algorithmic} \\State 随机噪声可以用$\\mathcal{N}$表示，用随机的网络参数$\\omega$和$\\theta$分别初始化Critic网络$Q_\\omega(s,a)$和Actor网络$\\mu_{\\theta}(s)$ \\State 复制相同的参数$\\omega^-\\leftarrow\\omega,\\theta^-\\leftarrow\\theta$,分别初始化目标网络$Q_{\\omega^-},\\mu_{\\theta^-}$ \\State 初始化经验放回斥$R$ \\For{$e=1\\rightarrow E$} \\State 初始化随机过程$\\mathcal{N}$用于动作探索 \\State 获取环境初始状态$s_1$ \\For{时间步$t=1\\rightarrow T$} \\State 根据当前策略和噪声选择动作$a_t=\\mu_\\theta(s_t)+\\mathcal{N}$ \\State 执行动作$a_t,$获得奖励$r_t,$环境状态变为$s_{t+1}$ \\State 将$(s_t,a_t,r_t,s_{t+1})$存储进回放池$R$ \\State 从$R$中采样$N$个元组$\\{(s_i,a_i,r_i,s_{i+1})\\}_{i=1,...,N}$ \\State 对每个元组，用目标网络计算$y_i=r_i+\\gamma Q_{\\omega^-}(s_{i+1},\\mu_{\\theta^-}(s_{i+1}))$ \\State 最小化目标损失函数$L=\\frac{1}{N}\\sum_{i=1}^N(y_i-Q_\\omega(s_i,a_i))^2,$以此更新当前Critic网络 \\State 计算采样的策略梯度，以此来更新当前Actor网络: $\\nabla_\\theta J\\approx \\frac{1}{N}\\sum_{i=1}^N\\nabla_\\theta \\mu_\\theta(s_i)\\nabla_a Q_\\omega (s_i,a)|_{a=\\mu_\\theta(s_i)}$ \\State 更新目标网络: $\\omega^-\\leftarrow \\tau\\omega+(1-\\tau)\\omega^-,\\theta^-\\leftarrow \\tau\\theta+(1-\\tau)\\theta^-$ \\EndFor \\EndFor \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "index.html#ddpg-代码实践",
    "href": "index.html#ddpg-代码实践",
    "title": "强化学习基础",
    "section": "11.3 DDPG 代码实践",
    "text": "11.3 DDPG 代码实践\n对于策略网络和价值网络，我们都采用只有一层隐藏层的神经网络。策略网络的输出层用正切函数y=\\tanh x作为激活函数可以方便地按比例调整环境可以接受的动作范围。在DDPG中处理的是与连续动作交互的环境，Q网络的输入是状态和动作拼接后的向量，Q网络的输出是一个值，表示该状态动作对的价值。\n\n\nCode\nclass PolicyNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):\n        super(PolicyNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n        self.action_bound = action_bound  # action_bound是环境可以接受的动作最大值\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return torch.tanh(self.fc2(x)) * self.action_bound\n\n\nclass QValueNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(QValueNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n        self.fc_out = torch.nn.Linear(hidden_dim, 1)\n\n    def forward(self, x, a):\n        cat = torch.cat([x, a], dim=1) # 拼接状态和动作\n        x = F.relu(self.fc1(cat))\n        x = F.relu(self.fc2(x))\n        return self.fc_out(x)\n\n\n接下来是DDPG算法的主体部分。在用策略网络采取动作的时候，为了更好地探索，我们向动作中加入高斯噪声。在DDPG的原始论文中加入的噪声服从OU（Ornstein-Uhlenbeck，OU）随机过程：\n\n\\delta x_t=\\theta(\\mu-x_{t-1})+\\sigma W,\n 其中\\mu是均值，W是服从布朗运动的随机噪声，\\theta,\\sigma是比例参数。可以看出，当x_{t-1}偏离均值时，x_t会向均值靠拢。OU过程是与时间相关的，适用于有惯性的系统。在DDPG的实践中，不少地方仅使用正态分布的噪声，这里为了简单起见也使用正态分布的噪声，感兴趣的读者可以试试改为OU过程观察效果。\n\n\nCode\nclass DDPG:\n    ''' DDPG算法 '''\n    def __init__(self, state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device):\n        self.actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)\n        self.critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n        self.target_actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)\n        self.target_critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n        # 初始化目标价值网络并设置和价值网络相同的参数\n        self.target_critic.load_state_dict(self.critic.state_dict())\n        # 初始化目标策略网络并设置和策略相同的参数\n        self.target_actor.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n        self.gamma = gamma\n        self.sigma = sigma  # 高斯噪声的标准差,均值直接设为0\n        self.tau = tau  # 目标网络软更新参数\n        self.action_dim = action_dim\n        self.device = device\n\n    def take_action(self, state):\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        action = self.actor(state).item()\n        # 给动作添加噪声，增加探索\n        action = action + self.sigma * np.random.randn(self.action_dim)\n        return action\n\n    def soft_update(self, net, target_net):\n        for param_target, param in zip(target_net.parameters(), net.parameters()):\n            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions'], dtype=torch.float).view(-1, 1).to(self.device)\n        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n\n        next_q_values = self.target_critic(next_states, self.target_actor(next_states))\n        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n        critic_loss = torch.mean(F.mse_loss(self.critic(states, actions), q_targets))\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        actor_loss = -torch.mean(self.critic(states, self.actor(states)))\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        self.soft_update(self.actor, self.target_actor)  # 软更新策略网络\n        self.soft_update(self.critic, self.target_critic)  # 软更新价值网络"
  },
  {
    "objectID": "index.html#小结-2",
    "href": "index.html#小结-2",
    "title": "强化学习基础",
    "section": "7.5 小结",
    "text": "7.5 小结\nREINFORCE 算法是策略梯度乃至强化学习的典型代表，智能体根据当前策略直接和环境交互，通过采样得到的轨迹数据直接计算出策略参数的梯度，进而更新当前策略，使其向最大化策略期望回报的目标靠近。这种学习方式是典型的从交互中学习，并且其优化的目标（即策略期望回报）正是最终所使用策略的性能，这比基于价值的强化学习算法的优化目标（一般是时序差分误差的最小化）要更加直接。 REINFORCE 算法理论上是能保证局部最优的，它实际上是借助蒙特卡洛方法采样轨迹来估计动作价值，这种做法的一大优点是可以得到无偏的梯度。但是，正是因为使用了蒙特卡洛方法，REINFORCE 算法的梯度估计的方差很大，可能会造成一定程度上的不稳定，这也是下一章将介绍的 Actor-Critic 算法要解决的问题。"
  },
  {
    "objectID": "index.html#简介-8",
    "href": "index.html#简介-8",
    "title": "强化学习基础",
    "section": "11.1 简介",
    "text": "11.1 简介\n之前的章节介绍了基于策略梯度的算法 REINFORCE、Actor-Critic 以及两个改进算法——TRPO 和 PPO。这类算法有一个共同的特点：它们都是在线策略算法，这意味着它们的样本效率（sample efficiency）比较低。我们回忆一下 DQN 算法，DQN 算法直接估计最优函数Q，可以做到离线策略学习，但是它只能处理动作空间有限的环境，这是因为它需要从所有动作中挑选一个Q值最大的动作。如果动作个数是无限的，虽然我们可以将动作空间离散化，但这比较粗糙，无法精细控制。那有没有办法可以用类似的思想来处理动作空间无限的环境并且使用的是离线策略算法呢？本章要讲解的深度确定性策略梯度（deep deterministic policy gradient，DDPG）算法就是如此，它构造一个确定性策略，用梯度上升的方法来最大化Q值。DDPG 也属于一种 Actor-Critic 算法。我们之前学习的 REINFORCE、TRPO 和 PPO 学习随机性策略，而本章的 DDPG 则学习一个确定性策略。"
  },
  {
    "objectID": "index.html#最大熵强化学习",
    "href": "index.html#最大熵强化学习",
    "title": "强化学习基础",
    "section": "12.2 最大熵强化学习",
    "text": "12.2 最大熵强化学习\n熵（entropy）表示对一个随机变量的随即程度的度量。具体而言，如果X是一个随机变量，且它的概率密度函数为p,那么它的熵H就被定义为 \nH(X)=\\mathbb{E}_{x\\sim p}[-\\log p(x)]\n\n在强化学习中，我们可以使用H(\\pi(\\cdot|s))来表示策略\\pi在状态s下的随机程度。\n最大熵强化学习（maximum entropy RL）的思想就是除了要最大化累积奖励，还要使得策略更加随机。如此，强化学习的目标中就加入了一项熵的正则项，定义为\n\n\\pi^*=\\underset{\\pi}{\\arg\\max}\\mathbb{E}_\\pi\\left[ \\sum_tr(s_t,a_t)+\\alpha H(\\pi(\\cdot|s_t)) \\right]\n 其中\\alpha是一个正则化系数，用来控制熵的重要程度。\n熵正则化增加了强化学习算法的探索程度，\\alpha越大，探索性就越强，有助于加速后续的策略学习，并减少策略陷入较差的局部最优的可能性。传统强化学习和最大熵强化学习的区别如 图 12 所示。\n\n\n\n\n\n\n图 12： 传统强化学习和最大熵强化学习的区别"
  },
  {
    "objectID": "index.html#soft策略迭代",
    "href": "index.html#soft策略迭代",
    "title": "强化学习基础",
    "section": "12.3 Soft策略迭代",
    "text": "12.3 Soft策略迭代\n在最大熵强化学习框架中，由于目标函数发生了变化，其他的一些定义也有相应的变化。首先，我们看一下 Soft 贝尔曼方程：\n\nQ(s_t,a_t)=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}}[V(s_{t+1})]\n 其中，状态价值函数被改写为 \nV(s_t)=\\mathbb{E}_{a_t\\sim\\pi}[Q(s_t,a_t)-\\alpha \\log\\pi(a_t|s_t)]=\\mathbb{E}_{a_t\\sim\\pi}[Q(s_t,a_t)]+H(\\pi(\\cdot|s_t))\n\n于是，根据该Soft贝尔曼方程，在有限的状态和动作，Soft策略评估可以收敛到策略\\pi的SoftQ函数。然后，根据如下Soft策略提升公式可以改进策略： \n\\pi_{\\text{new}}=\\arg\\min_{\\pi'}D_{KL}\\left( \\pi'(\\cdot|s),\\frac{\\exp\\left( \\frac{1}{\\alpha}Q^{\\pi_{\\text{old}}(s,\\cdot)} \\right)}{Z^{\\pi_{\\text{old}}}(s,\\cdot)} \\right)\n\n重复交替使用 Soft 策略评估和 Soft 策略提升，最终策略可以收敛到最大熵强化学习目标中的最优策略。但该 Soft 策略迭代方法只适用于表格型（tabular）设置的情况，即状态空间和动作空间是有限的情况。在连续空间下，我们需要通过参数化函数Q和策略\\pi来近似这样的迭代。"
  },
  {
    "objectID": "index.html#sac",
    "href": "index.html#sac",
    "title": "强化学习基础",
    "section": "12.4 SAC",
    "text": "12.4 SAC\n在SAC算法中，我们为两个动作价值函数Q（参数分别为\\omega_1,\\omega_2）和一个策略函数\\pi（参数为\\theta）建模。基于Double DQN的思想，SAC使用两个Q网络，但每次用Q网络时会挑选一个Q值小的网络，从而缓解Q值过高估计的问题。任意一个函数Q的损失函数为：\n\n\\begin{aligned}\nL_Q(\\omega)&=\\mathbb{E}_{(s_t,a_t,r_t,s_{t+1})\\sim R}\\left[ \\frac{1}{2}\\left( Q_\\omega(s_t,a_t)-(r_t+\\gamma V_{\\omega^-}(s_{t+1})) \\right)^2 \\right]\\\\\n&=\\mathbb{E}_{(s_t,a_t,r_t,s_{t+1})\\sim R,a_{t+1}\\sim \\pi_\\theta(\\cdot|s_{t+1})}\\left[ \\frac{1}{2}\\left( Q_\\omega(s_t,a_t)-\\left(r_t+\\gamma\\left(\\min_{j=1,2}Q_{\\omega_j^-}(s_{t+1},a_{t+1})-\\alpha \\log\\pi(a_{t+1}|s_{t+1})\\right)\\right) \\right)^2 \\right]\n\\end{aligned}\n 其中，R是策略过去收集的数据，因为SAC是一种离线策略算法。为了让训练更加稳定，这里使用了目标Q网络Q_{\\omega^-},同样是两个目标Q网络，与两个Q网络一一对应。SAC中目标Q网络的更新方式与DDPG中的更新方式一样。\n策略\\pi的损失函数由KL散度得到，化简后为：\n\nL_\\pi(\\theta)=\\mathbb{E}_{s_t\\sim R,a_{t}\\sim \\pi_\\theta}\\left[ \\alpha\\log(\\pi_\\theta(a_t|s_t))-Q_\\omega(s_t,a_t) \\right]\n\n可以理解为最大化函数V,因为有 \nV(s_t)=\\mathbb{E}_{a_t\\sim\\pi}[Q(s_t,a_t)-\\alpha\\log\\pi(a_t|s_t)].\n\n对连续动作空间的环境，SAC 算法的策略输出高斯分布的均值和标准差，但是根据高斯分布来采样动作的过程是不可导的。因此，我们需要用到重参数化技巧（reparameterization trick）。 重参数化的做法是先从一个单位高斯分布\\mathcal{N}采样，再把采样值乘以标准差后加上均值。这样就可以认为是从策略高斯分布采样，并且这样对于策略函数是可导的。我们将其表示为a_t=f_\\theta(\\epsilon_t;s_t),其中\\epsilon_t是一个噪声随机变量。同时考虑到两个函数Q,重写策略的损失函数：\n\nL_\\pi(\\theta)=\\mathbb{E}_{s_t\\sim R,\\epsilon_t\\sim\\mathcal{N}}\\left[ \\alpha\\log(\\pi_{\\theta}(f_\\theta(\\epsilon_t;s_t)|s_t))-\\min_{j=1,2}Q_{\\omega_j}(s_t,f_\\theta(\\epsilon_t;s_t)) \\right]\n\n\n12.4.1 自动调整熵正则化\n在SAC算法中，如何正确选择熵正则项的系数非常重要。在不同的状态下需要不同大小的熵：在最优动作不确定的某个状态下，熵的取值应该大一点；而在某个最优动作比较确定的状态下，熵的取值可以小一点。为了自动调整熵正则项，SAC将强化学习的目标改写为一个带约束的优化问题：\n\n\\max_{\\pi}\\mathbb{E}_\\pi\\left[ \\sum_tr(s_t,a_t) \\right] \\text{ s.t. }\\mathbb{E}_{(s_t,a_t)\\sim \\rho_\\pi}[-\\log(\\pi_t(a_t|s_t))]\\geq\\mathcal{H}_0\n 也就是最大化期望回报同时约束熵的均值大于\\mathcal{H}_0.通过一些数学技巧化简后，得到\\alpha的损失函数： \nL(\\alpha)=\\mathbb{E}_{s_t\\sim R,a_t\\sim\\pi(\\cdot|s_t)}[-\\alpha\\log\\pi(a_t|s_t)-\\alpha\\mathcal{H}_0]\n 即当策略的熵低于目标值\\mathcal{H}_0 时，训练目标L(\\alpha)会使\\alpha的值增大，进而在上述最小化损失函数L_\\pi(\\theta)的过程中增加了策略熵对应项的重要性；而当策略的熵高于目标值\\mathcal{H}_0时，训练目标L(\\alpha)会使\\alpha的值减小，进而使得策略训练时更专注于价值提升。\n\n至此，我们介绍完了 SAC 算法的整体思想，它的具体算法流程如下：\n\n\n\\begin{algorithm} \\caption{SAC算法流程} \\begin{algorithmic} \\State 用随机的网络参数$\\omega_1,\\omega_2$和$\\theta$分别初始化Critic网络$Q_{\\omega_1}(s,a),Q_{\\omega_2}(s,a)$和Actor网络$\\pi_\\theta(s)$ \\State 复制相同的参数$\\omega_1^-\\leftarrow\\omega_1,\\omega_2^-\\leftarrow\\omega_2$,分别初始化目标网络$Q_{\\omega_1^-},Q_{\\omega_2}$ \\State 初始化经验放回池$R$ \\For{$e=1\\rightarrow E$} \\State 获取环境初始状态$s_1$ \\For{时间步$t=1\\rightarrow T$} \\State 根据当前策略选择动作$a_t=\\pi_\\theta(s_t)$ \\State 执行动作$a_t,$获得奖励$r_t,$环境状态变为$s_{t+1}$ \\State 将$(s_t,a_t,r_t,s_{t+1})$存储进回放池$R$ \\For{训练轮数$k=1\\rightarrow K$} \\State 从$R$中采样$N$个元组$\\{(s_i,a_i,r_i,s_{i+1})\\}_{i=1,...,N}$ \\State 对每个元组，用目标网络计算 $y_i=r_i+\\gamma \\min_{j=1,2}Q_{\\omega_j^-}(s_{i+1},a_{i+1})-\\alpha \\log\\pi_\\theta(a_{i+1}|s_{i+1})$,其中$a_{i+1}\\sim \\pi_\\theta(\\cdot|s_{i+1})$ \\State 对两个Critic网络都进行如下更新:对$j=1,2$,最小化损失函数$L=\\frac{1}{N}\\sum_{i=1}^N(y_i-Q_{\\omega_j}(s_i,a_i))^2$ \\State 用重参数化技巧采样动作$\\tilde{a}_i$,然后用以下损失函数更新当前Actor网络： $L_\\pi(\\theta)=\\frac{1}{N}\\left(\\alpha\\log\\pi_\\theta(\\tilde{a}_i|s_i)-\\min_{j=1,2}Q_{\\omega_j}(s_i,\\tilde{a}_i)\\right)$ \\State 更新正则项的系数$\\alpha$ \\State 更新目标网络 $\\omega_1^-\\leftarrow \\tau\\omega_1+(1-\\tau)\\omega_1^-,\\omega_2^-\\leftarrow \\tau\\omega_2+(1-\\tau)\\omega_2^-$ \\EndFor \\EndFor \\EndFor \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "index.html#sac-代码实践",
    "href": "index.html#sac-代码实践",
    "title": "强化学习基础",
    "section": "12.5 SAC 代码实践",
    "text": "12.5 SAC 代码实践\n引入必要的库之后，定义一下策略网络和价值网络。由于处理的是与连续动作交互的环境，策略网络输出一个高斯分布的均值和标准差来表示动作分布；而价值网络的输入是状态和动作的拼接向量，输出一个实数来表示动作价值。\n\n\nCode\nimport random\nimport gym\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nimport torch.nn.functional as F\nfrom torch.distributions import Normal\nimport matplotlib.pyplot as plt\nimport rl_utils\n\nclass PolicyNetContinuous(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):\n        super(PolicyNetContinuous, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n        self.action_bound = action_bound\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        mu = self.fc_mu(x)\n        std = F.softplus(self.fc_std(x))\n        dist = Normal(mu, std)\n        normal_sample = dist.rsample()  # rsample()是重参数化采样\n        log_prob = dist.log_prob(normal_sample)\n        action = torch.tanh(normal_sample)\n        # 计算tanh_normal分布的对数概率密度\n        log_prob = log_prob - torch.log(1 - torch.tanh(action).pow(2) + 1e-7)\n        action = action * self.action_bound\n        return action, log_prob\n\n\nclass QValueNetContinuous(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(QValueNetContinuous, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n        self.fc_out = torch.nn.Linear(hidden_dim, 1)\n\n    def forward(self, x, a):\n        cat = torch.cat([x, a], dim=1)\n        x = F.relu(self.fc1(cat))\n        x = F.relu(self.fc2(x))\n        return self.fc_out(x)\n\n\n接下来看一下SAC算法的主要代码，如前面所述，SAC使用两个Critic网络Q_{\\omega_1},Q_{\\omega_2}来使Actor的训练更加稳定，而这两个Critic网络在训练时各自需要一个目标价值网络，因此SAC算法一共用到5 个网络，分别是一个策略网络、两个价值网络和两个目标价值网络。\n\n\nCode\nclass SACContinuous:\n    ''' 处理连续动作的SAC算法 '''\n    def __init__(self, state_dim, hidden_dim, action_dim, action_bound,\n                 actor_lr, critic_lr, alpha_lr, target_entropy, tau, gamma,\n                 device):\n        self.actor = PolicyNetContinuous(state_dim, hidden_dim, action_dim,\n                                         action_bound).to(device)  # 策略网络\n        self.critic_1 = QValueNetContinuous(state_dim, hidden_dim,\n                                            action_dim).to(device)  # 第一个Q网络\n        self.critic_2 = QValueNetContinuous(state_dim, hidden_dim,\n                                            action_dim).to(device)  # 第二个Q网络\n        self.target_critic_1 = QValueNetContinuous(state_dim,\n                                                   hidden_dim, action_dim).to(\n                                                       device)  # 第一个目标Q网络\n        self.target_critic_2 = QValueNetContinuous(state_dim,\n                                                   hidden_dim, action_dim).to(\n                                                       device)  # 第二个目标Q网络\n        # 令目标Q网络的初始参数和Q网络一样\n        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n                                                lr=actor_lr)\n        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n                                                   lr=critic_lr)\n        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n                                                   lr=critic_lr)\n        # 使用alpha的log值,可以使训练结果比较稳定\n        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n        self.log_alpha.requires_grad = True  # 可以对alpha求梯度\n        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n                                                    lr=alpha_lr)\n        self.target_entropy = target_entropy  # 目标熵的大小\n        self.gamma = gamma\n        self.tau = tau\n        self.device = device\n\n    def take_action(self, state):\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        action = self.actor(state)[0]\n        return [action.item()]\n\n    def calc_target(self, rewards, next_states, dones):  # 计算目标Q值\n        next_actions, log_prob = self.actor(next_states)\n        entropy = -log_prob\n        q1_value = self.target_critic_1(next_states, next_actions)\n        q2_value = self.target_critic_2(next_states, next_actions)\n        next_value = torch.min(q1_value,\n                               q2_value) + self.log_alpha.exp() * entropy\n        td_target = rewards + self.gamma * next_value * (1 - dones)\n        return td_target\n\n    def soft_update(self, net, target_net):\n        for param_target, param in zip(target_net.parameters(),\n                                       net.parameters()):\n            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n                                    param.data * self.tau)\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n        # 和之前章节一样,对倒立摆环境的奖励进行重塑以便训练\n        rewards = (rewards + 8.0) / 8.0\n\n        # 更新两个Q网络\n        td_target = self.calc_target(rewards, next_states, dones)\n        critic_1_loss = torch.mean(\n            F.mse_loss(self.critic_1(states, actions), td_target.detach()))\n        critic_2_loss = torch.mean(\n            F.mse_loss(self.critic_2(states, actions), td_target.detach()))\n        self.critic_1_optimizer.zero_grad()\n        critic_1_loss.backward()\n        self.critic_1_optimizer.step()\n        self.critic_2_optimizer.zero_grad()\n        critic_2_loss.backward()\n        self.critic_2_optimizer.step()\n\n        # 更新策略网络\n        new_actions, log_prob = self.actor(states)\n        entropy = -log_prob\n        q1_value = self.critic_1(states, new_actions)\n        q2_value = self.critic_2(states, new_actions)\n        actor_loss = torch.mean(-self.log_alpha.exp() * entropy -\n                                torch.min(q1_value, q2_value))\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # 更新alpha值\n        alpha_loss = torch.mean(\n            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n        self.log_alpha_optimizer.zero_grad()\n        alpha_loss.backward()\n        self.log_alpha_optimizer.step()\n\n        self.soft_update(self.critic_1, self.target_critic_1)\n        self.soft_update(self.critic_2, self.target_critic_2)"
  },
  {
    "objectID": "index.html#dqn代码实践",
    "href": "index.html#dqn代码实践",
    "title": "强化学习基础",
    "section": "5.3 DQN代码实践",
    "text": "5.3 DQN代码实践\n引入相应的包后定义经验回放池的类，主要包括加入数据、采样数据两大函数。\n\n\nCode\nimport random\nimport gym\nimport numpy as np\nimport collections\nfrom tqdm import tqdm\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport rl_utils\nclass ReplayBuffer:\n    ''' 经验回放池 '''\n    def __init__(self, capacity):\n        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n\n    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n        transitions = random.sample(self.buffer, batch_size)\n        state, action, reward, next_state, done = zip(*transitions)\n        return np.array(state), action, reward, np.array(next_state), done\n\n    def size(self):  # 目前buffer中数据的数量\n        return len(self.buffer)\n\n\n然后定义一个只有一层隐藏层的Q网络\n\n\nCode\nclass Qnet(torch.nn.Module):\n    ''' 只有一层隐藏层的Q网络 '''\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(Qnet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))  # 隐藏层使用ReLU激活函数\n        return self.fc2(x)\n\n\n有了基本组件之后，开始实现DQN算法\n\n\nCode\nclass DQN:\n    ''' DQN算法 '''\n    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma,\n                 epsilon, target_update, device):\n        self.action_dim = action_dim\n        self.q_net = Qnet(state_dim, hidden_dim,\n                          self.action_dim).to(device)  # Q网络\n        # 目标网络\n        self.target_q_net = Qnet(state_dim, hidden_dim,\n                                 self.action_dim).to(device)\n        # 使用Adam优化器\n        self.optimizer = torch.optim.Adam(self.q_net.parameters(),\n                                          lr=learning_rate)\n        self.gamma = gamma  # 折扣因子\n        self.epsilon = epsilon  # epsilon-贪婪策略\n        self.target_update = target_update  # 目标网络更新频率\n        self.count = 0  # 计数器,记录更新次数\n        self.device = device\n\n    def take_action(self, state):  # epsilon-贪婪策略采取动作\n        if np.random.random() &lt; self.epsilon:\n            action = np.random.randint(self.action_dim)\n        else:\n            state = torch.tensor([state], dtype=torch.float).to(self.device)\n            action = self.q_net(state).argmax().item()\n        return action\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n            self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n\n        q_values = self.q_net(states).gather(1, actions)  # Q值\n        # 下个状态的最大Q值\n        max_next_q_values = self.target_q_net(next_states).max(1)[0].view(\n            -1, 1)\n        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones\n                                                                )  # TD误差目标\n        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  # 均方误差损失函数\n        self.optimizer.zero_grad()  # PyTorch中默认梯度会累积,这里需要显式将梯度置为0\n        dqn_loss.backward()  # 反向传播更新参数\n        self.optimizer.step()\n\n        if self.count % self.target_update == 0:\n            self.target_q_net.load_state_dict(\n                self.q_net.state_dict())  # 更新目标网络\n        self.count += 1"
  },
  {
    "objectID": "index.html#加入卷积层的dqn算法",
    "href": "index.html#加入卷积层的dqn算法",
    "title": "强化学习基础",
    "section": "5.4 加入卷积层的DQN算法",
    "text": "5.4 加入卷积层的DQN算法\n让智能体和人一样玩游戏，我们需要让智能体学会以图像作为状态时的决策。我们可以利用 DQN 算法，将卷积层加入其网络结构以提取图像特征，最终实现以图像为输入的强化学习。以图像为输入的 DQN 算法的代码与 7.4 节的代码的不同之处主要在于 Q 网络的结构和数据输入。DQN 网络通常会将最近的几帧图像一起作为输入，从而感知环境的动态性。接下来我们实现以图像为输入的 DQN 算法，但由于代码需要运行较长的时间，我们在此便不展示训练结果。\n\n\nCode\nclass ConvolutionalQnet(torch.nn.Module):\n    ''' 加入卷积层的Q网络 '''\n    def __init__(self, action_dim, in_channels=4):\n        super(ConvolutionalQnet, self).__init__()\n        self.conv1 = torch.nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self.conv3 = torch.nn.Conv2d(64, 64, kernel_size=3, stride=1)\n        self.fc4 = torch.nn.Linear(7 * 7 * 64, 512)\n        self.head = torch.nn.Linear(512, action_dim)\n\n    def forward(self, x):\n        x = x / 255\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.fc4(x))\n        return self.head(x)"
  },
  {
    "objectID": "index.html#double-dqn",
    "href": "index.html#double-dqn",
    "title": "强化学习基础",
    "section": "6.2 Double DQN",
    "text": "6.2 Double DQN\n普通的DQN算法通常会导致对Q值的过高估计。传统DQN优化的TD误差目标为\n\nr+\\gamma\\max_{a'}Q_{\\omega^-}(s',a')\n 其中\\max_{a'}Q_{\\omega^-}(s',a')是由参数为\\omgea^-的目标网络计算得出，我们还可以将其改写为如下形式\n\nQ_{\\omega^-}\\left(s,\\arg\\max_{a'}Q_{\\omega^-}(s',a')\\right)\n\n换句话说，\\max操作实际上可以被拆解为两部分：首先选取状态s'下的最优动作a^*=\\arg\\max_{a'}Q_{\\omega^-}(s',a'),接着计算该动作对应的价值Q_{\\omega^-}(s',a^*).当这两部分采用同一套Q网络进行计算时，每次得到的都是神经网络当前估算的所有动作价值中的最大值。考虑到通过神经网络估算的Q值本身在某些时候会产生正向或负向的误差，在 DQN 的更新方式下神经网络会将正向误差累积。\n例如，我们考虑一个特殊情形：在状态s'下所有动作Q值均为0，即Q_{\\omega^-}(s',a_i)=0,\\forall i,此时正确的更新目标应为r+0=r,但是由于神经网络拟合的误差通常会出现某些动作的估算有正误差的情况，即存在某个动作a'有Q(s',a')&gt;0,此时我们的更新目标出现了过高估计，r+\\gamma\\max Q&gt;r+0. 因此，当我们用DQN更新公式进行更新时，Q(s,a)也就会被过高估计了。同理，我们拿这个Q(s,a)来作为更新目标来更新上一步的Q值时，同样会过高估计，这样的误差将会逐步累积。对于动作空间较大的任务，DQN 中的过高估计问题会非常严重，造成 DQN 无法有效工作的后果。\n为了解决这一问题，Double DQN 算法提出利用两个独立训练的神经网络估算\\max_{a'}Q_*(s',a').具体做法是将原有的\\max_{a'}Q_{\\omega^-}(s',a')更改为Q_{\\omega^-}\\left(s,\\arg\\max_{a'}Q_{\\omega}(s',a')\\right), 即利用一套神经网络Q_{\\omega}的输出选取价值最大的动作，但在使用该动作的价值时，用另一套神经网络Q_{\\omega^-}计算该动作的价值。这样，即使其中一套神经网络的某个动作存在比较严重的过高估计问题，由于另一套神经网络的存在，这个动作最终使用的Q值不会存在很大的过高估计问题。\n在传统的DQN算法中，本来就存在两套Q函数的神经网络——目标网络和训练网络，只不过\\max_{a'}Q_{\\omega^-}(s',a')的计算只用到了其中的目标网络，那么我们恰好可以直接将训练网络作为 Double DQN 算法中的第一套神经网络来选取动作，将目标网络作为第二套神经网络计算Q值，这便是 Double DQN 的主要思想。由于在 DQN 算法中将训练网络的参数记为\\omega,将目标网络的参数记为\\omega^-,这与本节中Double DQN的两套神经网络的参数是统一的，因此，我们可以直接写出如下Double DQN的优化目标：\n\nr+\\gamma Q_{\\omega^-}\\left(s,\\arg\\max_{a'}Q_{\\omega}(s',a')\\right)"
  },
  {
    "objectID": "index.html#double-dqn-代码实践",
    "href": "index.html#double-dqn-代码实践",
    "title": "强化学习基础",
    "section": "6.3 Double DQN 代码实践",
    "text": "6.3 Double DQN 代码实践\n显然，DQN与Double DQN 的差别只是在于计算状态s'下Q值时如何选取动作：\n\nDQN的优化目标可以写为 \n  r+\\gamma Q_{\\omega^-}\\left(s,\\arg\\max_{a'}Q{\\omega^-}(s',a')\\right),\n 动作的选取依旧依靠目标网络Q_{\\omega^-};\nDouble DQN的优化目标为 \n  r+\\gamma Q_{\\omega^-}\\left(s,\\arg\\max_{a'}Q_{\\omega}(s',a')\\right),\n 动作的选取依靠训练网络Q_{\\omega}.\n\n所以 Double DQN 的代码实现可以直接在 DQN 的基础上进行，无须做过多修改。\n\n\nCode\nclass DQN:\n    ''' DQN算法,包括Double DQN '''\n    def __init__(self,\n                 state_dim,\n                 hidden_dim,\n                 action_dim,\n                 learning_rate,\n                 gamma,\n                 epsilon,\n                 target_update,\n                 device,\n                 dqn_type='VanillaDQN'):\n        self.action_dim = action_dim\n        self.q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device)\n        self.target_q_net = Qnet(state_dim, hidden_dim,\n                                 self.action_dim).to(device)\n        self.optimizer = torch.optim.Adam(self.q_net.parameters(),\n                                          lr=learning_rate)\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.target_update = target_update\n        self.count = 0\n        self.dqn_type = dqn_type\n        self.device = device\n\n    def take_action(self, state):\n        if np.random.random() &lt; self.epsilon:\n            action = np.random.randint(self.action_dim)\n        else:\n            state = torch.tensor([state], dtype=torch.float).to(self.device)\n            action = self.q_net(state).argmax().item()\n        return action\n    def max_q_value(self, state):\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        return self.q_net(state).max().item()\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n            self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n\n        q_values = self.q_net(states).gather(1, actions)  # Q值\n        # 下个状态的最大Q值\n        if self.dqn_type == 'DoubleDQN': # DQN与Double DQN的区别\n            max_action = self.q_net(next_states).max(1)[1].view(-1, 1)\n            max_next_q_values = self.target_q_net(next_states).gather(1, max_action)\n        else: # DQN的情况\n            max_next_q_values = self.target_q_net(next_states).max(1)[0].view(-1, 1)\n        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)  # TD误差目标\n        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  # 均方误差损失函数\n        self.optimizer.zero_grad()  # PyTorch中默认梯度会累积,这里需要显式将梯度置为0\n        dqn_loss.backward()  # 反向传播更新参数\n        self.optimizer.step()\n\n        if self.count % self.target_update == 0:\n            self.target_q_net.load_state_dict(\n                self.q_net.state_dict())  # 更新目标网络\n        self.count += 1"
  },
  {
    "objectID": "index.html#dueling-dqn",
    "href": "index.html#dueling-dqn",
    "title": "强化学习基础",
    "section": "6.4 Dueling DQN",
    "text": "6.4 Dueling DQN\nDueling DQN 是 DQN 另一种的改进算法，它在传统 DQN 的基础上只进行了微小的改动，但却能大幅提升 DQN 的表现。在强化学习中，我们将状态动作价值函数Q减去状态价值函数V的结果定义为优势函数A,即A(s,a)=Q(s,a)-V(s).在同一个状态下，所有动作的优势值之和为0,因为所有动作价值的期望就是这个状态的状态价值。据此，在Deuling DQN中，Q网络被建模为：\n\nQ_{\\eta,\\alpha,\\beta}(s,a)=V_{\\eta,\\alpha}(s)+A_{\\eta,\\beta}(s,a)\n 其中V_{\\eta,\\alpha}(s)为状态价值函数，而A_{\\eta,\\beta}(s,a)则为该状态下采取不同动作的优势函数，表示采取不同动作的差异性；\\eta是状态价值函数和优势函数共享的网络参数，一般用在神经网络中，用来提取特征的前几层；\\alpha,\\beta分别为状态价值函数和优势函数的参数。在这样的模型下，我们不再让神经网络直接输出Q值，而是训练神经网络的最后几层的两个分支，分别输出状态价值函数和优势函数，再求和得到Q值。Dueling DQN 的网络结构如 图 5 所示。\n\n\n\n\n\n\n图 5： Dueling DQN的网络结构图\n\n\n\n将状态价值函数和优势函数分别建模的好处在于：某些情境下智能体只会关注状态的价值，而并不关心不同动作导致的差异，此时将二者分开建模能够使智能体更好地处理与动作关联较小的状态。\n对于 Dueling DQN 中的公式Q_{\\eta,\\alpha,\\beta}(s,a)=V_{\\eta,\\alpha}(s)+A_{\\eta,\\beta}(s,a),它存在对于V值和A值建模不唯一性的问题。例如，对于同样的Q值，如果将V值加上任意大小的常数C,再将所有Q值减去C,则得到的Q值仍然不变，这就导致了训练的不稳定性。为了解决这一问题，Dueling DQN强制最优动作的优势函数的实际输出为0，即：\n\nQ_{\\eta,\\alpha,\\beta}(s,a)=V_{\\eta,\\alpha}(s)+A_{\\eta,\\beta}(s,a)-\\max_{a'}A_{\\eta,\\beta}(s,a')\n 此时V(s)=\\max_aQ(s,a),可以确保V值建模的唯一性。在实践过程中，我们还可以用平均代替最大化操作，即： \nQ_{\\eta,\\alpha,\\beta}(s,a)=V_{\\eta,\\alpha}(s)+A_{\\eta,\\beta}(s,a)-\\frac{1}{|\\mathcal{A}|}\\sum_{a'}A_{\\eta,\\beta}(s,a')\n 此时V(s)=\\frac{1}{|\\mathcal{A}|}\\sum_{a'}Q(s,a').在下面的代码中，我们采用此种方式，虽然它不再满足贝尔曼最优方程，但实际应用时更加稳定。\n有的读者可能会问：“为什么 Dueling DQN 会比 DQN 好？”部分原因在于 Dueling DQN 能更高效学习状态价值函数。每一次更新时，函数V都会被更新，这也会影响到其他动作的Q值。而传统的 DQN 只会更新某个动作的Q值，其他动作的Q值就不会更新。因此，Dueling DQN 能够更加频繁、准确地学习状态价值函数。"
  },
  {
    "objectID": "index.html#dueling-dqn-代码实践",
    "href": "index.html#dueling-dqn-代码实践",
    "title": "强化学习基础",
    "section": "6.5 Dueling DQN 代码实践",
    "text": "6.5 Dueling DQN 代码实践\nDueling DQN 与 DQN 相比的差异只是在网络结构上，大部分代码依然可以继续沿用。我们定义状态价值函数和优势函数的复合神经网络VAnet。\n\n\nCode\nclass VAnet(torch.nn.Module):\n    ''' 只有一层隐藏层的A网络和V网络 '''\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(VAnet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)  # 共享网络部分\n        self.fc_A = torch.nn.Linear(hidden_dim, action_dim)\n        self.fc_V = torch.nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        A = self.fc_A(F.relu(self.fc1(x)))\n        V = self.fc_V(F.relu(self.fc1(x)))\n        Q = V + A - A.mean(1).view(-1, 1)  # Q值由V值和A值计算得到\n        return Q\n\n\nclass DQN:\n    ''' DQN算法,包括Double DQN和Dueling DQN '''\n    def __init__(self,\n                 state_dim,\n                 hidden_dim,\n                 action_dim,\n                 learning_rate,\n                 gamma,\n                 epsilon,\n                 target_update,\n                 device,\n                 dqn_type='VanillaDQN'):\n        self.action_dim = action_dim\n        if dqn_type == 'DuelingDQN':  # Dueling DQN采取不一样的网络框架\n            self.q_net = VAnet(state_dim, hidden_dim,\n                               self.action_dim).to(device)\n            self.target_q_net = VAnet(state_dim, hidden_dim,\n                                      self.action_dim).to(device)\n        else:\n            self.q_net = Qnet(state_dim, hidden_dim,\n                              self.action_dim).to(device)\n            self.target_q_net = Qnet(state_dim, hidden_dim,\n                                     self.action_dim).to(device)\n        self.optimizer = torch.optim.Adam(self.q_net.parameters(),\n                                          lr=learning_rate)\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.target_update = target_update\n        self.count = 0\n        self.dqn_type = dqn_type\n        self.device = device\n\n    def take_action(self, state):\n        if np.random.random() &lt; self.epsilon:\n            action = np.random.randint(self.action_dim)\n        else:\n            state = torch.tensor([state], dtype=torch.float).to(self.device)\n            action = self.q_net(state).argmax().item()\n        return action\n\n    def max_q_value(self, state):\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        return self.q_net(state).max().item()\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n            self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n\n        q_values = self.q_net(states).gather(1, actions)\n        if self.dqn_type == 'DoubleDQN':\n            max_action = self.q_net(next_states).max(1)[1].view(-1, 1)\n            max_next_q_values = self.target_q_net(next_states).gather(\n                1, max_action)\n        else:\n            max_next_q_values = self.target_q_net(next_states).max(1)[0].view(\n                -1, 1)\n        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)\n        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))\n        self.optimizer.zero_grad()\n        dqn_loss.backward()\n        self.optimizer.step()\n\n        if self.count % self.target_update == 0:\n            self.target_q_net.load_state_dict(self.q_net.state_dict())\n        self.count += 1\n\n\nrandom.seed(0)\nnp.random.seed(0)\nenv.seed(0)\ntorch.manual_seed(0)\nreplay_buffer = rl_utils.ReplayBuffer(buffer_size)\nagent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon,\n            target_update, device, 'DuelingDQN')\nreturn_list, max_q_value_list = train_DQN(agent, env, num_episodes,\n                                          replay_buffer, minimal_size,\n                                          batch_size)\n\nepisodes_list = list(range(len(return_list)))\nmv_return = rl_utils.moving_average(return_list, 5)\nplt.plot(episodes_list, mv_return)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('Dueling DQN on {}'.format(env_name))\nplt.show()\n\nframes_list = list(range(len(max_q_value_list)))\nplt.plot(frames_list, max_q_value_list)\nplt.axhline(0, c='orange', ls='--')\nplt.axhline(10, c='red', ls='--')\nplt.xlabel('Frames')\nplt.ylabel('Q value')\nplt.title('Dueling DQN on {}'.format(env_name))\nplt.show()"
  },
  {
    "objectID": "index.html#总结-1",
    "href": "index.html#总结-1",
    "title": "强化学习基础",
    "section": "6.6 总结",
    "text": "6.6 总结\n在传统的 DQN 基础上，有两种非常容易实现的变式——Double DQN 和 Dueling DQN，Double DQN 解决了 DQN 中对Q值的过高估计，而 Dueling DQN 能够很好地学习到不同动作的差异性，在动作空间较大的环境下非常有效。从 Double DQN 和 Dueling DQN 的方法原理中，我们也能感受到深度强化学习的研究是在关注深度学习和强化学习有效结合：一是在深度学习的模块的基础上，强化学习方法如何更加有效地工作，并避免深度模型学习行为带来的一些问题，例如使用 Double DQN 解决Q值过高估计的问题；二是在强化学习的场景下，深度学习模型如何有效学习到有用的模式，例如设计 Dueling DQN 网络架构来高效地学习状态价值函数以及动作优势函数。"
  },
  {
    "objectID": "index.html#reinforce",
    "href": "index.html#reinforce",
    "title": "强化学习基础",
    "section": "7.3 REINFORCE",
    "text": "7.3 REINFORCE\nREINFORCE算法的具体流程如下：\n\n\n\\begin{algorithm} \\caption{REINFORCE算法流程} \\begin{algorithmic} \\State 初始化策略参数$\\theta$ \\For{$e=1\\rightarrow E$} \\State 用当前策略$\\pi_\\theta$采样轨迹$\\{s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T\\}$ \\State 计算当前轨迹每个时刻$t$往后的回报$\\sum_{t'=t}^T\\gamma^{t'-t}r_{t'},$记为$\\psi_t$ \\State 对$\\theta$进行更新$\\theta=\\theta+\\alpha\\sum_{t}^T\\psi_t\\nabla_\\theta\\log\\pi_\\theta(a_t|s_t)$ \\EndFor \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "index.html#reinforce-代码实践",
    "href": "index.html#reinforce-代码实践",
    "title": "强化学习基础",
    "section": "7.4 REINFORCE 代码实践",
    "text": "7.4 REINFORCE 代码实践\n首先定义策略网络PolicyNet,其输入是某个状态，输出则是该状态下的动作概率分布，这里采用在离散动作空间上的 softmax()函数来实现一个可学习的多项分布（multinomial distribution）。\n\n\nCode\nimport gym\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport rl_utils\nclass PolicyNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(PolicyNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return F.softmax(self.fc2(x), dim=1)\n\n\n再定义REINFORCE算法。在take_action()函数中，我们通过动作概率分布对离散的动作进行采样。在更新过程中，我们按照算法将损失函数写为策略回报的负数，即-\\sum_t\\psi_t\\nabla_\\theta\\log\\pi_\\theta(a_t|s_t),对\\theta求导后就可以通过梯度下降来更新策略。\n\n\nCode\nclass REINFORCE:\n    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma,\n                 device):\n        self.policy_net = PolicyNet(state_dim, hidden_dim,\n                                    action_dim).to(device)\n        self.optimizer = torch.optim.Adam(self.policy_net.parameters(),\n                                          lr=learning_rate)  # 使用Adam优化器\n        self.gamma = gamma  # 折扣因子\n        self.device = device\n\n    def take_action(self, state):  # 根据动作概率分布随机采样\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        probs = self.policy_net(state)\n        action_dist = torch.distributions.Categorical(probs)\n        action = action_dist.sample()\n        return action.item()\n\n    def update(self, transition_dict):\n        reward_list = transition_dict['rewards']\n        state_list = transition_dict['states']\n        action_list = transition_dict['actions']\n\n        G = 0\n        self.optimizer.zero_grad()\n        for i in reversed(range(len(reward_list))):  # 从最后一步算起\n            reward = reward_list[i]\n            state = torch.tensor([state_list[i]],\n                                 dtype=torch.float).to(self.device)\n            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device)\n            log_prob = torch.log(self.policy_net(state).gather(1, action))\n            G = self.gamma * G + reward\n            loss = -log_prob * G  # 每一步的损失函数\n            loss.backward()  # 反向传播计算梯度\n        self.optimizer.step()  # 梯度下降\n\n\n不过，相比于前面的 DQN 算法，REINFORCE 算法使用了更多的序列，这是因为 REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。此外，REINFORCE 算法的性能也有一定程度的波动，这主要是因为每条采样轨迹的回报值波动比较大，这也是 REINFORCE 算法主要的不足。"
  },
  {
    "objectID": "index.html#策略梯度的证明",
    "href": "index.html#策略梯度的证明",
    "title": "强化学习基础",
    "section": "7.6 策略梯度的证明",
    "text": "7.6 策略梯度的证明\n（略）"
  },
  {
    "objectID": "index.html#小结-3",
    "href": "index.html#小结-3",
    "title": "强化学习基础",
    "section": "8.5 小结",
    "text": "8.5 小结\n本章讲解了 Actor-Critic 算法，它是基于值函数的方法和基于策略的方法的叠加。价值模块 Critic 在策略模块 Actor 采样的数据中学习分辨什么是好的动作，什么不是好的动作，进而指导 Actor 进行策略更新。随着 Actor 的训练的进行，其与环境交互所产生的数据分布也发生改变，这需要 Critic 尽快适应新的数据分布并给出好的判别。\nActor-Critic 算法非常实用，后续章节中的 TRPO、PPO、DDPG、SAC 等深度强化学习算法都是在 Actor-Critic 框架下进行发展的。深入了解 Actor-Critic 算法对读懂目前深度强化学习的研究热点大有裨益。"
  },
  {
    "objectID": "index.html#小结-4",
    "href": "index.html#小结-4",
    "title": "强化学习基础",
    "section": "11.4 小结",
    "text": "11.4 小结\n本章讲解了深度确定性策略梯度算法（DDPG），它是面向连续动作空间的深度确定性策略训练的典型算法。相比于它的先期工作，即确定性梯度算法（DPG），DDPG 加入了目标网络和软更新的方法，这对深度模型构建的价值网络和策略网络的稳定学习起到了关键的作用。DDPG 算法也被引入了多智能体强化学习领域，催生了 MADDPG 算法，我们会在后续的章节中对此展开讨论。"
  },
  {
    "objectID": "index.html#简介-9",
    "href": "index.html#简介-9",
    "title": "强化学习基础",
    "section": "12.1 简介",
    "text": "12.1 简介\n之前的章节提到过在线策略算法的采样效率比较低，我们通常更倾向于使用离线策略算法。然而，虽然 DDPG 是离线策略算法，但是它的训练非常不稳定，收敛性较差，对超参数比较敏感，也难以适应不同的复杂环境。2018 年，一个更加稳定的离线策略算法 Soft Actor-Critic（SAC）被提出。SAC 的前身是 Soft Q-learning，它们都属于最大熵强化学习的范畴。Soft Q-learning 不存在一个显式的策略函数，而是使用一个函数Q的波尔兹曼分布，在连续空间下求解非常麻烦。于是 SAC 提出使用一个 Actor 表示策略函数，从而解决这个问题。目前，在无模型的强化学习算法中，SAC 是一个非常高效的算法，它学习一个随机性策略，在不少标准环境中取得了领先的成绩。"
  }
]