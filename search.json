[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "强化学习基础",
    "section": "",
    "text": "在马尔可夫过程的基础上加入奖励函数r和折扣因子\\gamma,就可以得到马尔可夫奖励过程（Markov Reward Process, MRP）。一个马尔可夫奖励过程由\\langle S,\\mathcal{P},r,\\gamma\\rangle构成。\n\n\n在一个MRP中，从第t时刻状态S_t开始，直到终止状态时，所有奖励的衰减之和称为回报G_t（Return），公式如下：\n\nG_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\dots=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k},\n 其中，R_t表示在时刻t获得的奖励。\n\n\n\n在MRP中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（Value）。所有状态的价值就组成了价值函数（Value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成\n\n\\begin{aligned}\nV(s)&=\\mathbb{E}\\left[G_t\\mid S_t=s\\right]\\\\\n&= \\mathbb{E}\\left[ R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} +\\dots\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma (R_{t+1}+\\gamma R_{t+2} +\\dots)\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma G_{t+1}\\mid S_t=s \\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma V(S_{t+1})\\mid S_t=s \\right]\n\\end{aligned}\n\n在上式的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即\\mathbb{E}\\left[ R_t\\mid S_t=s \\right]=r(s);另一方面，等式中剩余部分\\mathbb{E}\\left[\\gamma V(S_{t+1})\\mid S_t=s\\right]可以根据从状态s出发的转移概率得到，即\n\nV(s)=r(s)+\\gamma \\sum_{s'\\in \\mathcal{S}}p(s'\\mid s)V(s')\n\n上式就是MRP中非常有名的贝尔曼方程（Bellman equation），对每一个状态都成立。若通过矩阵运算可以得到以下解析解：\n\n\\begin{aligned}\n\\mathcal{V}&=\\mathcal{R}+\\gamma\\mathcal{P}\\mathcal{V}\\\\\n\\mathcal{V}&=(I-\\gamma\\mathcal{P})^{-1}\\mathcal{R}\n\\end{aligned}\n 其中\\mathcal{V,R,P}均为向量或矩阵。以上解析解的计算复杂度是O(n^3),其中n是状态个数，因此这种方法只适用于很小的MRP。求解较大规模的MRP奖励过程中的价值函数时，可以使用动态规划、蒙特卡洛、时序差分，这些方法将在之后的章节介绍。\n接下来编写代码来实现求解价值函数的解析解方法，并据此计算该马尔可夫奖励过程中所有状态的价值。\n\ndef compute(P, rewards, gamma, states_num):\n    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n                   rewards)\n    return value\n\n\n\n\n\n如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（MDP）。我们将这个来自外界的刺激称为智能体（Agent）的动作，在MRP的基础上加入动作，就得到了MDP。MDP有元组\\langle \\mathcal{S,A,P},r,\\gamma\\rangle构成。\n\n\n智能体的策略（Policy）通常用字母\\pi表示。策略\\pi(a|s)=P(A_t=a|S_t=s)是一个函数，表示在输入状态s情况下采取动作a的概率。当一个策略是确定性策略（Deterministic policy）时，它在每个状态只输出一个确定性的动作，即只有该动作的概率为1,其他动作的概率为0; 当一个策略是随机性策略（Stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。\n在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。回顾一下在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。这很好理解，因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。\n\n\n\n我们用V^{\\pi}(s)表示在MDP中基于策略\\pi的状态价值函数（State-value function），定义在从状态s出发遵循策略\\pi能获得的期望回报，数学表达为\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]\n\n\n\n\n不同于MRP，MDP中由于动作的存在我们定义一个动作价值函数（Action-value function）。我们用Q^{\\pi}(s,a)表示在MDP遵循策略\\pi时，对当前状态s执行动作a得到的期望回报：\n\nQ^{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]\n\n状态价值函数和动作价值函数之间的关系：\n在使用策略\\pi时，状态s的价值等于在该状态下基于策略\\pi采取所有动作的概率与相应价值相乘再求和的结果：\n\nV^{\\pi}(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)Q^{\\pi}(s,a)\n\n在使用策略\\pi时，状态s下采取动作a的价值等于即时奖励加上经过衰减后的所有可能的下一个状态转移概率与相应价值的乘积：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n\n\n\n在贝尔曼方程中加上期望二字是为了与接下来的贝尔曼最优方程进行区分。我们通过加单推到就可以分别得到两个价值函数的贝尔曼期望方程（Bellman Expect Equation）：\n\n\\begin{aligned}\nV^{\\pi}(s)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma V^{\\pi}(S_{t+1})|S_t=s\\right]\\\\\n&=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left( r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi}(s')\\right)\\\\\n\\\\\nQ^{\\pi}(s,a)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma Q^{\\pi}(S_{t+1},A_{t+1}|S_t=s, A_t=a)\\right]\\\\\n&=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')Q^{\\pi}(s',a)\n\\end{aligned}\n\n价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的。\n现在我们考虑一个MDP的简单例子如 图 1 ，其中每个绿色圆圈代表一个状态，一共有s_1\\sim s_5这5个状态。黑色实线箭头代表可以采取的动作，黄色小圆圈代表动作。需要注意的是，并非在每个状态都能采取所有动作，例如在s_1,智能体只能采取”保持s_1“和”前往s_2“这两个动作，无法采取其他动作。\n每个黄色小圆圈旁的红色数字代表在某个状态下采取某个动作能获得的奖励。虚线箭头代表采取动作后可能转移到的状态，箭头边上的带方框的数字代表转移概率，如果没有数字则表示转移概率为1.例如，在s_2下，如果采取动作”前往s_3“就能得到奖励-2,并且以概率1转移到s_3;在s_4下，如果采取”概率前往”这个动作，就能得到奖励1,并且会分别以概率0.2,0.4,0.4转移到s_2,s_3或s_4.\n\n\n\n\n\n\n图 1： MDP一个简单例子\n\n\n\n接下来我们编写代码来表示 图 1 中的MDP，并定义两个策略，第一个策略是一个完全随机策略，即在每个状态下，智能体会以同样的概率选取它可能采取的动作。例如，在s_1下智能体会以0.5,0.5的概率选取动作”保持s_1“和”前往s_2“.第二个策略是一个提前设定的策略。\n\nimport numpy as np\nS = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\nA = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n# 状态转移函数\nP = {\n    \"s1-保持s1-s1\": 1.0,\n    \"s1-前往s2-s2\": 1.0,\n    \"s2-前往s1-s1\": 1.0,\n    \"s2-前往s3-s3\": 1.0,\n    \"s3-前往s4-s4\": 1.0,\n    \"s3-前往s5-s5\": 1.0,\n    \"s4-前往s5-s5\": 1.0,\n    \"s4-概率前往-s2\": 0.2,\n    \"s4-概率前往-s3\": 0.4,\n    \"s4-概率前往-s4\": 0.4,\n}\n# 奖励函数\nR = {\n    \"s1-保持s1\": -1,\n    \"s1-前往s2\": 0,\n    \"s2-前往s1\": -1,\n    \"s2-前往s3\": -2,\n    \"s3-前往s4\": -2,\n    \"s3-前往s5\": 0,\n    \"s4-前往s5\": 10,\n    \"s4-概率前往\": 1,\n}\ngamma = 0.5  # 折扣因子\nMDP = (S, A, P, R, gamma)\n\n# 策略1,随机策略\nPi_1 = {\n    \"s1-保持s1\": 0.5,\n    \"s1-前往s2\": 0.5,\n    \"s2-前往s1\": 0.5,\n    \"s2-前往s3\": 0.5,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.5,\n    \"s4-概率前往\": 0.5,\n}\n# 策略2\nPi_2 = {\n    \"s1-保持s1\": 0.6,\n    \"s1-前往s2\": 0.4,\n    \"s2-前往s1\": 0.3,\n    \"s2-前往s3\": 0.7,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.1,\n    \"s4-概率前往\": 0.9,\n}\n\n\n# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\ndef join(str1, str2):\n    return str1 + '-' + str2\n\n接下来我们想要计算该MDP下，一个策略\\pi的状态价值函数。我们现有的工具是MRP的解析解方法，一个自然的想法是给定一个MDP和一个策略\\pi,我们是否可以将其转化为一个MRP？答案是肯定的，我们可以将策略的动作选择进行边缘化（Marginalization），就可以得到没有动作的MRP了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个MRP在该状态下的奖励，即： \nr'(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)r(s,a)\n\n同理，我们计算采取动作的概率与使s转移到s'的概率的乘积，再将这些乘积相加，其和就是一个MRP的状态从s转移至s'的概率：\n\nP'(s'|s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\n\n于是，我们构建得到了一个MRP:\\langle \\mathcal{S},P',r',\\gamma\\rangle.根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。\n接下来，我们用代码实现该方法，计算用随即策略Pi_1时的状态价值函数，为了简单起见，我们将直接给出转化后的MRP的状态转移矩阵和奖励函数。\n\ngamma = 0.5\n# 转化后的MRP的状态转移矩阵\nP_from_mdp_to_mrp = [\n    [0.5, 0.5, 0.0, 0.0, 0.0],\n    [0.5, 0.0, 0.5, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.5, 0.5],\n    [0.0, 0.1, 0.2, 0.2, 0.5],\n    [0.0, 0.0, 0.0, 0.0, 1.0],\n]\nP_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\nR_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n\nV = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\nprint(\"MDP中每个状态价值分别为\\n\", V)\n\nMDP中每个状态价值分别为\n [[-1.22555411]\n [-1.67666232]\n [ 0.51890482]\n [ 6.0756193 ]\n [ 0.        ]]\n\n\n知道了状态价值函数V^{\\pi}(s)后，我们可以计算动作价值函数Q^{\\pi}(s,a).例如(s_4,\\text{概率前往})的动作价值为2.152，根据以下公式可以计算得到：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n这个 MRP 解析解的方法在状态动作集合比较大的时候不是很适用，那有没有其他的方法呢？ 章节 2 将介绍用动态规划算法来计算得到价值函数。 章节 1.3 将介绍用蒙特卡洛方法来近似估计这个价值函数，用蒙特卡洛方法的好处在于我们不需要知道 MDP 的状态转移函数和奖励函数，它可以得到一个近似值，并且采样数越多越准确。\n\n\n\n\n蒙特卡洛方法（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用MC方法时我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。一个简单的例子是用蒙特卡洛方法来计算圆的面积。例如，在正方形内部随机产生若干个点，细数落在圆中点的个数，圆的面积与正方形面积之比就等于圆中点的个数与正方形中点的个数之比。如果我们随机产生的点的个数越多，计算得到圆的面积就越接近于真实的圆的面积。\n我们现在介绍如何用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数。回忆一下，一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望就可以了，公式如下：\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[ G_t|S_t=s \\right]\\approx \\frac{1}{N}\\sum_{i=1}^N G_t^{(i)}\n\n在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。我们介绍的蒙特卡洛价值估计方法会在该状态每一次出现时计算它的回报。还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。假设我们现在用策略\\pi从s开始采样序列，据此来计算状态价值。我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示。\n\n使用策略\\pi 采样若干条序列： \ns_0^{(i)}\\overset{a_0^{(i)}}{\\rightarrow} r_0^{(i)},s_1^{(i)}\\overset{a_1^{(i)}}{\\rightarrow} r_1^{(i)},\ns_2^{(i)}\\overset{a_2^{(i)}}{\\rightarrow},...,\\overset{a_{T-1}^{(i)}}{\\rightarrow} r_{T-1}^{(i)},s_T^{(i)}\n\n对每一条序列中的每一时间步t的状态s进行以下操作：\n\n更新状态s的计数器N(s)\\leftarrow N(s)+1;\n更新状态s的总回报M(s)\\leftarrow M(s)+G_t;\n\n每一个状态的价值被估计为回报的平均值V(s)=\\=M(s)/N(s).\n\n根据大数定律，当N(s)\\rightarrow \\infty,有V(s)\\rightarrow V^{\\pi}(s). 计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态s和对应回报G,进行如下计算：\n\nN(s)\\leftarrow N(s)+1\nV(s)\\leftarrow V(s)+\\frac{G-V(s)}{N(s)}\n\n接下来我们用代码定义一个采样函数，采样函数需要遵守状态转移矩阵和相应策略，每次将(s,a,r,s_next)元组放入序列中，直到到达终止序列。然后我们通过该函数，用随即策略在 图 1 的MDP中随机采样几条序列。\n\ndef sample(MDP, Pi, timestep_max, number):\n    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n    S, A, P, R, gamma = MDP\n    episodes = []\n    for _ in range(number):\n        episode = []\n        timestep = 0\n        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n        # 当前状态为终止状态或者时间步太长时,一次采样结束\n        while s != \"s5\" and timestep &lt;= timestep_max:\n            timestep += 1\n            rand, temp = np.random.rand(), 0\n            # 在状态s下根据策略选择动作\n            for a_opt in A:\n                temp += Pi.get(join(s, a_opt), 0)\n                if temp &gt; rand:\n                    a = a_opt\n                    r = R.get(join(s, a), 0)\n                    break\n            rand, temp = np.random.rand(), 0\n            # 根据状态转移概率得到下一个状态s_next\n            for s_opt in S:\n                temp += P.get(join(join(s, a), s_opt), 0)\n                if temp &gt; rand:\n                    s_next = s_opt\n                    break\n            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n            s = s_next  # s_next变成当前状态,开始接下来的循环\n        episodes.append(episode)\n    return episodes\n\n\n# 采样5次,每个序列最长不超过20步\nepisodes = sample(MDP, Pi_1, 20, 5)\nprint('第一条序列\\n', episodes[0])\nprint('第二条序列\\n', episodes[1])\nprint('第五条序列\\n', episodes[4])\n\n第一条序列\n [('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n第二条序列\n [('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '前往s5', 10, 's5')]\n第五条序列\n [('s4', '概率前往', 1, 's3'), ('s3', '前往s5', 0, 's5')]\n\n\n\n# 对所有采样序列计算所有状态的价值\ndef MC(episodes, V, N, gamma):\n    for episode in episodes:\n        G = 0\n        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算\n            (s, a, r, s_next) = episode[i]\n            G = r + gamma * G\n            N[s] = N[s] + 1\n            V[s] = V[s] + (G - V[s]) / N[s]\n\n\ntimestep_max = 20\n# 采样1000次,可以自行修改\nepisodes = sample(MDP, Pi_1, timestep_max, 1000)\ngamma = 0.5\nV = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nN = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nMC(episodes, V, N, gamma)\nprint(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)\n\n使用蒙特卡洛方法计算MDP的状态价值为\n {'s1': -1.2063441904043721, 's2': -1.6514014069048446, 's3': 0.5484280819659458, 's4': 6.293954549406411, 's5': 0}\n\n\n可以看到用蒙特卡洛方法估计得到的状态价值和我们用 MRP 解析解得到的状态价值是很接近的。这得益于我们采样了比较多的序列，感兴趣的读者可以尝试修改采样次数，然后观察蒙特卡洛方法的结果。\n\n\n\n强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。我们首先定义策略之间的偏序关系：当且仅当对于任意状态s都有V^{\\pi}(s)&gt; V^{\\pi'}(s), 记\\pi&gt;\\pi'. 于是在有限状态和动作集合的MDP中至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是最优策略（Optimal policy）。最优策略可能有很多个，我们都将其表示为\\pi^*(s).\n最优策略都有相同的状态价值函数，我们称之为最有状态价值函数，表示为： \nV^*(s)=\\max_{\\pi} V^{\\pi}(s),\\forall s\\in\\mathcal{S}\n\n同理我们定义最优动作价值函数： \nQ^*(s,a)=\\max_{\\pi}Q^{\\pi}(s,a),\\forall s\\in\\mathcal{S},a\\in\\mathcal{A}\n\n为了使Q^*(s,a)最大，我们需要在当前状态动作对(s,a)之后都执行最优策略，于是我们得到了最有状态价值函数和最优动作价值函数之间的关系：\n\nQ^\\pi{s,a}=r(s,a)+\\gamma\\sum_{s\\in\\mathcal{S}}P(s'|s,a)V^*(s')\n\n这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值： \nV^*(s)=\\max_{a\\in\\mathcal{A}}Q^*(s,a)\n\n\n\n根据V^*(s)和Q^*(s,a)的关系，我们可以得到贝尔曼最优方程（Bellman optimality equation）：\n\n\\begin{aligned}\nV^*(s)&=\\max_{a\\in\\mathcal{A}}\\left\\{ r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^*(s')\\right\\}\\\\\nQ^*(s,a)=r(s,a)+\\gamma\\sum_{s'\\in \\mathcal{S}}p(s'|s,a)\\max_{a'\\in\\mathcal{A}}Q^*(s',a')\n\\end{aligned}\n\n章节 2 将介绍如何用动态规划算法得到最优策略。\n\n\n\n\n马尔可夫决策过程是强化学习中的基础概念，强化学习中的环境就是一个马尔可夫决策过程。我们接下来将要介绍的强化学习算法通常都是在求解马尔可夫决策过程中的最优策略。"
  },
  {
    "objectID": "index.html#马尔可夫决策过程",
    "href": "index.html#马尔可夫决策过程",
    "title": "强化学习基础",
    "section": "",
    "text": "在马尔可夫过程的基础上加入奖励函数r和折扣因子\\gamma,就可以得到马尔可夫奖励过程（Markov Reward Process, MRP）。一个马尔可夫奖励过程由\\langle S,\\mathcal{P},r,\\gamma\\rangle构成。\n\n\n在一个MRP中，从第t时刻状态S_t开始，直到终止状态时，所有奖励的衰减之和称为回报G_t（Return），公式如下：\n\nG_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\dots=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k},\n 其中，R_t表示在时刻t获得的奖励。\n\n\n\n在MRP中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（Value）。所有状态的价值就组成了价值函数（Value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成\n\n\\begin{aligned}\nV(s)&=\\mathbb{E}\\left[G_t\\mid S_t=s\\right]\\\\\n&= \\mathbb{E}\\left[ R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} +\\dots\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma (R_{t+1}+\\gamma R_{t+2} +\\dots)\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma G_{t+1}\\mid S_t=s \\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma V(S_{t+1})\\mid S_t=s \\right]\n\\end{aligned}\n\n在上式的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即\\mathbb{E}\\left[ R_t\\mid S_t=s \\right]=r(s);另一方面，等式中剩余部分\\mathbb{E}\\left[\\gamma V(S_{t+1})\\mid S_t=s\\right]可以根据从状态s出发的转移概率得到，即\n\nV(s)=r(s)+\\gamma \\sum_{s'\\in \\mathcal{S}}p(s'\\mid s)V(s')\n\n上式就是MRP中非常有名的贝尔曼方程（Bellman equation），对每一个状态都成立。若通过矩阵运算可以得到以下解析解：\n\n\\begin{aligned}\n\\mathcal{V}&=\\mathcal{R}+\\gamma\\mathcal{P}\\mathcal{V}\\\\\n\\mathcal{V}&=(I-\\gamma\\mathcal{P})^{-1}\\mathcal{R}\n\\end{aligned}\n 其中\\mathcal{V,R,P}均为向量或矩阵。以上解析解的计算复杂度是O(n^3),其中n是状态个数，因此这种方法只适用于很小的MRP。求解较大规模的MRP奖励过程中的价值函数时，可以使用动态规划、蒙特卡洛、时序差分，这些方法将在之后的章节介绍。\n接下来编写代码来实现求解价值函数的解析解方法，并据此计算该马尔可夫奖励过程中所有状态的价值。\n\ndef compute(P, rewards, gamma, states_num):\n    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n                   rewards)\n    return value\n\n\n\n\n\n如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（MDP）。我们将这个来自外界的刺激称为智能体（Agent）的动作，在MRP的基础上加入动作，就得到了MDP。MDP有元组\\langle \\mathcal{S,A,P},r,\\gamma\\rangle构成。\n\n\n智能体的策略（Policy）通常用字母\\pi表示。策略\\pi(a|s)=P(A_t=a|S_t=s)是一个函数，表示在输入状态s情况下采取动作a的概率。当一个策略是确定性策略（Deterministic policy）时，它在每个状态只输出一个确定性的动作，即只有该动作的概率为1,其他动作的概率为0; 当一个策略是随机性策略（Stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。\n在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。回顾一下在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。这很好理解，因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。\n\n\n\n我们用V^{\\pi}(s)表示在MDP中基于策略\\pi的状态价值函数（State-value function），定义在从状态s出发遵循策略\\pi能获得的期望回报，数学表达为\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]\n\n\n\n\n不同于MRP，MDP中由于动作的存在我们定义一个动作价值函数（Action-value function）。我们用Q^{\\pi}(s,a)表示在MDP遵循策略\\pi时，对当前状态s执行动作a得到的期望回报：\n\nQ^{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]\n\n状态价值函数和动作价值函数之间的关系：\n在使用策略\\pi时，状态s的价值等于在该状态下基于策略\\pi采取所有动作的概率与相应价值相乘再求和的结果：\n\nV^{\\pi}(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)Q^{\\pi}(s,a)\n\n在使用策略\\pi时，状态s下采取动作a的价值等于即时奖励加上经过衰减后的所有可能的下一个状态转移概率与相应价值的乘积：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n\n\n\n在贝尔曼方程中加上期望二字是为了与接下来的贝尔曼最优方程进行区分。我们通过加单推到就可以分别得到两个价值函数的贝尔曼期望方程（Bellman Expect Equation）：\n\n\\begin{aligned}\nV^{\\pi}(s)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma V^{\\pi}(S_{t+1})|S_t=s\\right]\\\\\n&=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left( r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi}(s')\\right)\\\\\n\\\\\nQ^{\\pi}(s,a)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma Q^{\\pi}(S_{t+1},A_{t+1}|S_t=s, A_t=a)\\right]\\\\\n&=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')Q^{\\pi}(s',a)\n\\end{aligned}\n\n价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的。\n现在我们考虑一个MDP的简单例子如@fig-mdp ，其中每个绿色圆圈代表一个状态，一共有s_1\\sim s_5这5个状态。黑色实线箭头代表可以采取的动作，黄色小圆圈代表动作。需要注意的是，并非在每个状态都能采取所有动作，例如在s_1,智能体只能采取”保持s_1“和”前往s_2“这两个动作，无法采取其他动作。\n每个黄色小圆圈旁的红色数字代表在某个状态下采取某个动作能获得的奖励。虚线箭头代表采取动作后可能转移到的状态，箭头边上的带方框的数字代表转移概率，如果没有数字则表示转移概率为1.例如，在s_2下，如果采取动作”前往s_3“就能得到奖励-2,并且以概率1转移到s_3;在s_4下，如果采取”概率前往”这个动作，就能得到奖励1,并且会分别以概率0.2,0.4,0.4转移到s_2,s_3或s_4.\n\n\n\n\n\n\n图 1： MDP一个简单例子\n\n\n\n接下来我们编写代码来表示@fig-mdp 中的MDP，并定义两个策略，第一个策略是一个完全随机策略，即在每个状态下，智能体会以同样的概率选取它可能采取的动作。例如，在s_1下智能体会以0.5,0.5的概率选取动作”保持s_1“和”前往s_2“.第二个策略是一个提前设定的策略。\n\nimport numpy as np\nS = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\nA = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n# 状态转移函数\nP = {\n    \"s1-保持s1-s1\": 1.0,\n    \"s1-前往s2-s2\": 1.0,\n    \"s2-前往s1-s1\": 1.0,\n    \"s2-前往s3-s3\": 1.0,\n    \"s3-前往s4-s4\": 1.0,\n    \"s3-前往s5-s5\": 1.0,\n    \"s4-前往s5-s5\": 1.0,\n    \"s4-概率前往-s2\": 0.2,\n    \"s4-概率前往-s3\": 0.4,\n    \"s4-概率前往-s4\": 0.4,\n}\n# 奖励函数\nR = {\n    \"s1-保持s1\": -1,\n    \"s1-前往s2\": 0,\n    \"s2-前往s1\": -1,\n    \"s2-前往s3\": -2,\n    \"s3-前往s4\": -2,\n    \"s3-前往s5\": 0,\n    \"s4-前往s5\": 10,\n    \"s4-概率前往\": 1,\n}\ngamma = 0.5  # 折扣因子\nMDP = (S, A, P, R, gamma)\n\n# 策略1,随机策略\nPi_1 = {\n    \"s1-保持s1\": 0.5,\n    \"s1-前往s2\": 0.5,\n    \"s2-前往s1\": 0.5,\n    \"s2-前往s3\": 0.5,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.5,\n    \"s4-概率前往\": 0.5,\n}\n# 策略2\nPi_2 = {\n    \"s1-保持s1\": 0.6,\n    \"s1-前往s2\": 0.4,\n    \"s2-前往s1\": 0.3,\n    \"s2-前往s3\": 0.7,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.1,\n    \"s4-概率前往\": 0.9,\n}\n\n\n# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\ndef join(str1, str2):\n    return str1 + '-' + str2\n\n接下来我们想要计算该MDP下，一个策略\\pi的状态价值函数。我们现有的工具是MRP的解析解方法，一个自然的想法是给定一个MDP和一个策略\\pi,我们是否可以将其转化为一个MRP？答案是肯定的，我们可以将策略的动作选择进行边缘化（Marginalization），就可以得到没有动作的MRP了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个MRP在该状态下的奖励，即： \nr'(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)r(s,a)\n\n同理，我们计算采取动作的概率与使s转移到s'的概率的乘积，再将这些乘积相加，其和就是一个MRP的状态从s转移至s'的概率：\n\nP'(s'|s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\n\n于是，我们构建得到了一个MRP:\\langle \\mathcal{S},P',r',\\gamma\\rangle.根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。\n接下来，我们用代码实现该方法，计算用随即策略Pi_1时的状态价值函数，为了简单起见，我们将直接给出转化后的MRP的状态转移矩阵和奖励函数。\n\ngamma = 0.5\n# 转化后的MRP的状态转移矩阵\nP_from_mdp_to_mrp = [\n    [0.5, 0.5, 0.0, 0.0, 0.0],\n    [0.5, 0.0, 0.5, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.5, 0.5],\n    [0.0, 0.1, 0.2, 0.2, 0.5],\n    [0.0, 0.0, 0.0, 0.0, 1.0],\n]\nP_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\nR_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n\nV = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\nprint(\"MDP中每个状态价值分别为\\n\", V)\n\nMDP中每个状态价值分别为\n [[-1.22555411]\n [-1.67666232]\n [ 0.51890482]\n [ 6.0756193 ]\n [ 0.        ]]\n\n\n知道了状态价值函数V^{\\pi}(s)后，我们可以计算动作价值函数Q^{\\pi}(s,a).例如(s_4,\\text{概率前往})的动作价值为2.152，根据以下公式可以计算得到：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n这个 MRP 解析解的方法在状态动作集合比较大的时候不是很适用，那有没有其他的方法呢？第 章节 1.2 章将介绍用动态规划算法来计算得到价值函数。章节 1.1.3 节将介绍用蒙特卡洛方法来近似估计这个价值函数，用蒙特卡洛方法的好处在于我们不需要知道 MDP 的状态转移函数和奖励函数，它可以得到一个近似值，并且采样数越多越准确。\n\n\n\n\n蒙特卡洛方法（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用MC方法时我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。一个简单的例子是用蒙特卡洛方法来计算圆的面积。例如，在正方形内部随机产生若干个点，细数落在圆中点的个数，圆的面积与正方形面积之比就等于圆中点的个数与正方形中点的个数之比。如果我们随机产生的点的个数越多，计算得到圆的面积就越接近于真实的圆的面积。\n我们现在介绍如何用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数。回忆一下，一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望就可以了，公式如下：\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[ G_t|S_t=s \\right]\\approx \\frac{1}{N}\\sum_{i=1}^N G_t^{(i)}\n\n在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。我们介绍的蒙特卡洛价值估计方法会在该状态每一次出现时计算它的回报。还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。假设我们现在用策略\\pi从s开始采样序列，据此来计算状态价值。我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示。\n\n使用策略\\pi 采样若干条序列： \ns_0^{(i)}\\overset{a_0^{(i)}}{\\rightarrow} r_0^{(i)},s_1^{(i)}\\overset{a_1^{(i)}}{\\rightarrow} r_1^{(i)},\ns_2^{(i)}\\overset{a_2^{(i)}}{\\rightarrow},...,\\overset{a_{T-1}^{(i)}}{\\rightarrow} r_{T-1}^{(i)},s_T^{(i)}\n\n对每一条序列中的每一时间步t的状态s进行以下操作：\n\n更新状态s的计数器N(s)\\leftarrow N(s)+1;\n更新状态s的总回报M(s)\\leftarrow M(s)+G_t;\n\n每一个状态的价值被估计为回报的平均值V(s)=\\=M(s)/N(s).\n\n根据大数定律，当N(s)\\rightarrow \\infty,有V(s)\\rightarrow V^{\\pi}(s). 计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态s和对应回报G,进行如下计算：\n\nN(s)\\leftarrow N(s)+1\nV(s)\\leftarrow V(s)+\\frac{G-V(s)}{N(s)}\n\n接下来我们用代码定义一个采样函数，采样函数需要遵守状态转移矩阵和相应策略，每次将(s,a,r,s_next)元组放入序列中，直到到达终止序列。然后我们通过该函数，用随即策略在@fig-mdp 的MDP中随机采样几条序列。\n\ndef sample(MDP, Pi, timestep_max, number):\n    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n    S, A, P, R, gamma = MDP\n    episodes = []\n    for _ in range(number):\n        episode = []\n        timestep = 0\n        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n        # 当前状态为终止状态或者时间步太长时,一次采样结束\n        while s != \"s5\" and timestep &lt;= timestep_max:\n            timestep += 1\n            rand, temp = np.random.rand(), 0\n            # 在状态s下根据策略选择动作\n            for a_opt in A:\n                temp += Pi.get(join(s, a_opt), 0)\n                if temp &gt; rand:\n                    a = a_opt\n                    r = R.get(join(s, a), 0)\n                    break\n            rand, temp = np.random.rand(), 0\n            # 根据状态转移概率得到下一个状态s_next\n            for s_opt in S:\n                temp += P.get(join(join(s, a), s_opt), 0)\n                if temp &gt; rand:\n                    s_next = s_opt\n                    break\n            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n            s = s_next  # s_next变成当前状态,开始接下来的循环\n        episodes.append(episode)\n    return episodes\n\n\n# 采样5次,每个序列最长不超过20步\nepisodes = sample(MDP, Pi_1, 20, 5)\nprint('第一条序列\\n', episodes[0])\nprint('第二条序列\\n', episodes[1])\nprint('第五条序列\\n', episodes[4])\n\n第一条序列\n [('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n第二条序列\n [('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s5', 0, 's5')]\n第五条序列\n [('s2', '前往s1', -1, 's1'), ('s1', '前往s2', 0, 's2'), ('s2', '前往s3', -2, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n\n\n\n# 对所有采样序列计算所有状态的价值\ndef MC(episodes, V, N, gamma):\n    for episode in episodes:\n        G = 0\n        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算\n            (s, a, r, s_next) = episode[i]\n            G = r + gamma * G\n            N[s] = N[s] + 1\n            V[s] = V[s] + (G - V[s]) / N[s]\n\n\ntimestep_max = 20\n# 采样1000次,可以自行修改\nepisodes = sample(MDP, Pi_1, timestep_max, 1000)\ngamma = 0.5\nV = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nN = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nMC(episodes, V, N, gamma)\nprint(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)\n\n使用蒙特卡洛方法计算MDP的状态价值为\n {'s1': -1.2199909859035727, 's2': -1.6944299913955496, 's3': 0.4373151566032693, 's4': 5.962755391182896, 's5': 0}\n\n\n可以看到用蒙特卡洛方法估计得到的状态价值和我们用 MRP 解析解得到的状态价值是很接近的。这得益于我们采样了比较多的序列，感兴趣的读者可以尝试修改采样次数，然后观察蒙特卡洛方法的结果。\n\n\n\n强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。我们首先定义策略之间的偏序关系：当且仅当对于任意状态s都有V^{\\pi}(s)&gt; V^{\\pi'}(s), 记\\pi&gt;\\pi'. 于是在有限状态和动作集合的MDP中至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是最优策略（Optimal policy）。最优策略可能有很多个，我们都将其表示为\\pi^*(s).\n最优策略都有相同的状态价值函数，我们称之为最有状态价值函数，表示为： \nV^*(s)=\\max_{\\pi} V^{\\pi}(s),\\forall s\\in\\mathcal{S}\n\n同理我们定义最优动作价值函数： \nQ^*(s,a)=\\max_{\\pi}Q^{\\pi}(s,a),\\forall s\\in\\mathcal{S},a\\in\\mathcal{A}\n\n为了使Q^*(s,a)最大，我们需要在当前状态动作对(s,a)之后都执行最优策略，于是我们得到了最有状态价值函数和最优动作价值函数之间的关系：\n\nQ^\\pi{s,a}=r(s,a)+\\gamma\\sum_{s\\in\\mathcal{S}}P(s'|s,a)V^*(s')\n\n这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值： \nV^*(s)=\\max_{a\\in\\mathcal{A}}Q^*(s,a)\n\n\n\n根据V^*(s)和Q^*(s,a)的关系，我们可以得到贝尔曼最优方程（Bellman optimality equation）：\n\n\\begin{aligned}\nV^*(s)&=\\max_{a\\in\\mathcal{A}}\\left\\{ r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^*(s')\\right\\}\\\\\nQ^*(s,a)=r(s,a)+\\gamma\\sum_{s'\\in \\mathcal{S}}p(s'|s,a)\\max_{a'\\in\\mathcal{A}}Q^*(s',a')\n\\end{aligned}\n\n第@sec-dynpa 章将介绍如何用动态规划算法得到最优策略。\n\n\n\n\n马尔可夫决策过程是强化学习中的基础概念，强化学习中的环境就是一个马尔可夫决策过程。我们接下来将要介绍的强化学习算法通常都是在求解马尔可夫决策过程中的最优策略。"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#马尔可夫奖励过程",
    "href": "index.html#马尔可夫奖励过程",
    "title": "强化学习基础",
    "section": "",
    "text": "在马尔可夫过程的基础上加入奖励函数r和折扣因子\\gamma,就可以得到马尔可夫奖励过程（Markov Reward Process, MRP）。一个马尔可夫奖励过程由\\langle S,\\mathcal{P},r,\\gamma\\rangle构成。\n\n\n在一个MRP中，从第t时刻状态S_t开始，直到终止状态时，所有奖励的衰减之和称为回报G_t（Return），公式如下：\n\nG_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\dots=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k},\n 其中，R_t表示在时刻t获得的奖励。\n\n\n\n在MRP中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的价值（Value）。所有状态的价值就组成了价值函数（Value function），价值函数的输入为某个状态，输出为这个状态的价值。我们将价值函数写成\n\n\\begin{aligned}\nV(s)&=\\mathbb{E}\\left[G_t\\mid S_t=s\\right]\\\\\n&= \\mathbb{E}\\left[ R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2} +\\dots\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma (R_{t+1}+\\gamma R_{t+2} +\\dots)\\mid S_t=s\\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma G_{t+1}\\mid S_t=s \\right]\\\\\n&=\\mathbb{E}\\left[ R_t+\\gamma V(S_{t+1})\\mid S_t=s \\right]\n\\end{aligned}\n\n在上式的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即\\mathbb{E}\\left[ R_t\\mid S_t=s \\right]=r(s);另一方面，等式中剩余部分\\mathbb{E}\\left[\\gamma V(S_{t+1})\\mid S_t=s\\right]可以根据从状态s出发的转移概率得到，即\n\nV(s)=r(s)+\\gamma \\sum_{s'\\in \\mathcal{S}}p(s'\\mid s)V(s')\n\n上式就是MRP中非常有名的贝尔曼方程（Bellman equation），对每一个状态都成立。若通过矩阵运算可以得到以下解析解：\n\n\\begin{aligned}\n\\mathcal{V}&=\\mathcal{R}+\\gamma\\mathcal{P}\\mathcal{V}\\\\\n\\mathcal{V}&=(I-\\gamma\\mathcal{P})^{-1}\\mathcal{R}\n\\end{aligned}\n 其中\\mathcal{V,R,P}均为向量或矩阵。以上解析解的计算复杂度是O(n^3),其中n是状态个数，因此这种方法只适用于很小的MRP。求解较大规模的MRP奖励过程中的价值函数时，可以使用动态规划、蒙特卡洛、时序差分，这些方法将在之后的章节介绍。\n接下来编写代码来实现求解价值函数的解析解方法，并据此计算该马尔可夫奖励过程中所有状态的价值。\n\ndef compute(P, rewards, gamma, states_num):\n    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n                   rewards)\n    return value"
  },
  {
    "objectID": "index.html#马尔可夫决策过程-1",
    "href": "index.html#马尔可夫决策过程-1",
    "title": "强化学习基础",
    "section": "",
    "text": "如果有一个外界的“刺激”来共同改变这个随机过程，就有了马尔可夫决策过程（MDP）。我们将这个来自外界的刺激称为智能体（Agent）的动作，在MRP的基础上加入动作，就得到了MDP。MDP有元组\\langle \\mathcal{S,A,P},r,\\gamma\\rangle构成。\n\n\n智能体的策略（Policy）通常用字母\\pi表示。策略\\pi(a|s)=P(A_t=a|S_t=s)是一个函数，表示在输入状态s情况下采取动作a的概率。当一个策略是确定性策略（Deterministic policy）时，它在每个状态只输出一个确定性的动作，即只有该动作的概率为1,其他动作的概率为0; 当一个策略是随机性策略（Stochastic policy）时，它在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。\n在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。回顾一下在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。这很好理解，因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。\n\n\n\n我们用V^{\\pi}(s)表示在MDP中基于策略\\pi的状态价值函数（State-value function），定义在从状态s出发遵循策略\\pi能获得的期望回报，数学表达为\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}[G_t|S_t=s]\n\n\n\n\n不同于MRP，MDP中由于动作的存在我们定义一个动作价值函数（Action-value function）。我们用Q^{\\pi}(s,a)表示在MDP遵循策略\\pi时，对当前状态s执行动作a得到的期望回报：\n\nQ^{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a]\n\n状态价值函数和动作价值函数之间的关系：\n在使用策略\\pi时，状态s的价值等于在该状态下基于策略\\pi采取所有动作的概率与相应价值相乘再求和的结果：\n\nV^{\\pi}(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)Q^{\\pi}(s,a)\n\n在使用策略\\pi时，状态s下采取动作a的价值等于即时奖励加上经过衰减后的所有可能的下一个状态转移概率与相应价值的乘积：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n\n\n\n在贝尔曼方程中加上期望二字是为了与接下来的贝尔曼最优方程进行区分。我们通过加单推到就可以分别得到两个价值函数的贝尔曼期望方程（Bellman Expect Equation）：\n\n\\begin{aligned}\nV^{\\pi}(s)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma V^{\\pi}(S_{t+1})|S_t=s\\right]\\\\\n&=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left( r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^{\\pi}(s')\\right)\\\\\n\\\\\nQ^{\\pi}(s,a)&=\\mathbb{E}_{\\pi}\\left[R_t+\\gamma Q^{\\pi}(S_{t+1},A_{t+1}|S_t=s, A_t=a)\\right]\\\\\n&=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')Q^{\\pi}(s',a)\n\\end{aligned}\n\n价值函数和贝尔曼方程是强化学习非常重要的组成部分，之后的一些强化学习算法都是据此推导出来的。\n现在我们考虑一个MDP的简单例子如 图 1 ，其中每个绿色圆圈代表一个状态，一共有s_1\\sim s_5这5个状态。黑色实线箭头代表可以采取的动作，黄色小圆圈代表动作。需要注意的是，并非在每个状态都能采取所有动作，例如在s_1,智能体只能采取”保持s_1“和”前往s_2“这两个动作，无法采取其他动作。\n每个黄色小圆圈旁的红色数字代表在某个状态下采取某个动作能获得的奖励。虚线箭头代表采取动作后可能转移到的状态，箭头边上的带方框的数字代表转移概率，如果没有数字则表示转移概率为1.例如，在s_2下，如果采取动作”前往s_3“就能得到奖励-2,并且以概率1转移到s_3;在s_4下，如果采取”概率前往”这个动作，就能得到奖励1,并且会分别以概率0.2,0.4,0.4转移到s_2,s_3或s_4.\n\n\n\n\n\n\n图 1： MDP一个简单例子\n\n\n\n接下来我们编写代码来表示 图 1 中的MDP，并定义两个策略，第一个策略是一个完全随机策略，即在每个状态下，智能体会以同样的概率选取它可能采取的动作。例如，在s_1下智能体会以0.5,0.5的概率选取动作”保持s_1“和”前往s_2“.第二个策略是一个提前设定的策略。\n\nimport numpy as np\nS = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\nA = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n# 状态转移函数\nP = {\n    \"s1-保持s1-s1\": 1.0,\n    \"s1-前往s2-s2\": 1.0,\n    \"s2-前往s1-s1\": 1.0,\n    \"s2-前往s3-s3\": 1.0,\n    \"s3-前往s4-s4\": 1.0,\n    \"s3-前往s5-s5\": 1.0,\n    \"s4-前往s5-s5\": 1.0,\n    \"s4-概率前往-s2\": 0.2,\n    \"s4-概率前往-s3\": 0.4,\n    \"s4-概率前往-s4\": 0.4,\n}\n# 奖励函数\nR = {\n    \"s1-保持s1\": -1,\n    \"s1-前往s2\": 0,\n    \"s2-前往s1\": -1,\n    \"s2-前往s3\": -2,\n    \"s3-前往s4\": -2,\n    \"s3-前往s5\": 0,\n    \"s4-前往s5\": 10,\n    \"s4-概率前往\": 1,\n}\ngamma = 0.5  # 折扣因子\nMDP = (S, A, P, R, gamma)\n\n# 策略1,随机策略\nPi_1 = {\n    \"s1-保持s1\": 0.5,\n    \"s1-前往s2\": 0.5,\n    \"s2-前往s1\": 0.5,\n    \"s2-前往s3\": 0.5,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.5,\n    \"s4-概率前往\": 0.5,\n}\n# 策略2\nPi_2 = {\n    \"s1-保持s1\": 0.6,\n    \"s1-前往s2\": 0.4,\n    \"s2-前往s1\": 0.3,\n    \"s2-前往s3\": 0.7,\n    \"s3-前往s4\": 0.5,\n    \"s3-前往s5\": 0.5,\n    \"s4-前往s5\": 0.1,\n    \"s4-概率前往\": 0.9,\n}\n\n\n# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\ndef join(str1, str2):\n    return str1 + '-' + str2\n\n接下来我们想要计算该MDP下，一个策略\\pi的状态价值函数。我们现有的工具是MRP的解析解方法，一个自然的想法是给定一个MDP和一个策略\\pi,我们是否可以将其转化为一个MRP？答案是肯定的，我们可以将策略的动作选择进行边缘化（Marginalization），就可以得到没有动作的MRP了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个MRP在该状态下的奖励，即： \nr'(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)r(s,a)\n\n同理，我们计算采取动作的概率与使s转移到s'的概率的乘积，再将这些乘积相加，其和就是一个MRP的状态从s转移至s'的概率：\n\nP'(s'|s)=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\n\n于是，我们构建得到了一个MRP:\\langle \\mathcal{S},P',r',\\gamma\\rangle.根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。\n接下来，我们用代码实现该方法，计算用随即策略Pi_1时的状态价值函数，为了简单起见，我们将直接给出转化后的MRP的状态转移矩阵和奖励函数。\n\ngamma = 0.5\n# 转化后的MRP的状态转移矩阵\nP_from_mdp_to_mrp = [\n    [0.5, 0.5, 0.0, 0.0, 0.0],\n    [0.5, 0.0, 0.5, 0.0, 0.0],\n    [0.0, 0.0, 0.0, 0.5, 0.5],\n    [0.0, 0.1, 0.2, 0.2, 0.5],\n    [0.0, 0.0, 0.0, 0.0, 1.0],\n]\nP_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\nR_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n\nV = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\nprint(\"MDP中每个状态价值分别为\\n\", V)\n\nMDP中每个状态价值分别为\n [[-1.22555411]\n [-1.67666232]\n [ 0.51890482]\n [ 6.0756193 ]\n [ 0.        ]]\n\n\n知道了状态价值函数V^{\\pi}(s)后，我们可以计算动作价值函数Q^{\\pi}(s,a).例如(s_4,\\text{概率前往})的动作价值为2.152，根据以下公式可以计算得到：\n\nQ^{\\pi}(s,a)=r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n\n这个 MRP 解析解的方法在状态动作集合比较大的时候不是很适用，那有没有其他的方法呢？ 章节 2 将介绍用动态规划算法来计算得到价值函数。 章节 1.3 将介绍用蒙特卡洛方法来近似估计这个价值函数，用蒙特卡洛方法的好处在于我们不需要知道 MDP 的状态转移函数和奖励函数，它可以得到一个近似值，并且采样数越多越准确。"
  },
  {
    "objectID": "index.html#sec-mc",
    "href": "index.html#sec-mc",
    "title": "强化学习基础",
    "section": "",
    "text": "蒙特卡洛方法（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用MC方法时我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。一个简单的例子是用蒙特卡洛方法来计算圆的面积。例如，在正方形内部随机产生若干个点，细数落在圆中点的个数，圆的面积与正方形面积之比就等于圆中点的个数与正方形中点的个数之比。如果我们随机产生的点的个数越多，计算得到圆的面积就越接近于真实的圆的面积。\n我们现在介绍如何用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数。回忆一下，一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望就可以了，公式如下：\n\nV^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[ G_t|S_t=s \\right]\\approx \\frac{1}{N}\\sum_{i=1}^N G_t^{(i)}\n\n在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。我们介绍的蒙特卡洛价值估计方法会在该状态每一次出现时计算它的回报。还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。假设我们现在用策略\\pi从s开始采样序列，据此来计算状态价值。我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示。\n\n使用策略\\pi 采样若干条序列： \ns_0^{(i)}\\overset{a_0^{(i)}}{\\rightarrow} r_0^{(i)},s_1^{(i)}\\overset{a_1^{(i)}}{\\rightarrow} r_1^{(i)},\ns_2^{(i)}\\overset{a_2^{(i)}}{\\rightarrow},...,\\overset{a_{T-1}^{(i)}}{\\rightarrow} r_{T-1}^{(i)},s_T^{(i)}\n\n对每一条序列中的每一时间步t的状态s进行以下操作：\n\n更新状态s的计数器N(s)\\leftarrow N(s)+1;\n更新状态s的总回报M(s)\\leftarrow M(s)+G_t;\n\n每一个状态的价值被估计为回报的平均值V(s)=\\=M(s)/N(s).\n\n根据大数定律，当N(s)\\rightarrow \\infty,有V(s)\\rightarrow V^{\\pi}(s). 计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态s和对应回报G,进行如下计算：\n\nN(s)\\leftarrow N(s)+1\nV(s)\\leftarrow V(s)+\\frac{G-V(s)}{N(s)}\n\n接下来我们用代码定义一个采样函数，采样函数需要遵守状态转移矩阵和相应策略，每次将(s,a,r,s_next)元组放入序列中，直到到达终止序列。然后我们通过该函数，用随即策略在 图 1 的MDP中随机采样几条序列。\n\ndef sample(MDP, Pi, timestep_max, number):\n    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n    S, A, P, R, gamma = MDP\n    episodes = []\n    for _ in range(number):\n        episode = []\n        timestep = 0\n        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n        # 当前状态为终止状态或者时间步太长时,一次采样结束\n        while s != \"s5\" and timestep &lt;= timestep_max:\n            timestep += 1\n            rand, temp = np.random.rand(), 0\n            # 在状态s下根据策略选择动作\n            for a_opt in A:\n                temp += Pi.get(join(s, a_opt), 0)\n                if temp &gt; rand:\n                    a = a_opt\n                    r = R.get(join(s, a), 0)\n                    break\n            rand, temp = np.random.rand(), 0\n            # 根据状态转移概率得到下一个状态s_next\n            for s_opt in S:\n                temp += P.get(join(join(s, a), s_opt), 0)\n                if temp &gt; rand:\n                    s_next = s_opt\n                    break\n            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n            s = s_next  # s_next变成当前状态,开始接下来的循环\n        episodes.append(episode)\n    return episodes\n\n\n# 采样5次,每个序列最长不超过20步\nepisodes = sample(MDP, Pi_1, 20, 5)\nprint('第一条序列\\n', episodes[0])\nprint('第二条序列\\n', episodes[1])\nprint('第五条序列\\n', episodes[4])\n\n第一条序列\n [('s3', '前往s4', -2, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's3'), ('s3', '前往s4', -2, 's4'), ('s4', '前往s5', 10, 's5')]\n第二条序列\n [('s4', '概率前往', 1, 's4'), ('s4', '概率前往', 1, 's4'), ('s4', '前往s5', 10, 's5')]\n第五条序列\n [('s4', '概率前往', 1, 's3'), ('s3', '前往s5', 0, 's5')]\n\n\n\n# 对所有采样序列计算所有状态的价值\ndef MC(episodes, V, N, gamma):\n    for episode in episodes:\n        G = 0\n        for i in range(len(episode) - 1, -1, -1):  #一个序列从后往前计算\n            (s, a, r, s_next) = episode[i]\n            G = r + gamma * G\n            N[s] = N[s] + 1\n            V[s] = V[s] + (G - V[s]) / N[s]\n\n\ntimestep_max = 20\n# 采样1000次,可以自行修改\nepisodes = sample(MDP, Pi_1, timestep_max, 1000)\ngamma = 0.5\nV = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nN = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0}\nMC(episodes, V, N, gamma)\nprint(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V)\n\n使用蒙特卡洛方法计算MDP的状态价值为\n {'s1': -1.2063441904043721, 's2': -1.6514014069048446, 's3': 0.5484280819659458, 's4': 6.293954549406411, 's5': 0}\n\n\n可以看到用蒙特卡洛方法估计得到的状态价值和我们用 MRP 解析解得到的状态价值是很接近的。这得益于我们采样了比较多的序列，感兴趣的读者可以尝试修改采样次数，然后观察蒙特卡洛方法的结果。"
  },
  {
    "objectID": "index.html#最优策略",
    "href": "index.html#最优策略",
    "title": "强化学习基础",
    "section": "",
    "text": "强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。我们首先定义策略之间的偏序关系：当且仅当对于任意状态s都有V^{\\pi}(s)&gt; V^{\\pi'}(s), 记\\pi&gt;\\pi'. 于是在有限状态和动作集合的MDP中至少存在一个策略比其他所有策略都好或者至少存在一个策略不差于其他所有策略，这个策略就是最优策略（Optimal policy）。最优策略可能有很多个，我们都将其表示为\\pi^*(s).\n最优策略都有相同的状态价值函数，我们称之为最有状态价值函数，表示为： \nV^*(s)=\\max_{\\pi} V^{\\pi}(s),\\forall s\\in\\mathcal{S}\n\n同理我们定义最优动作价值函数： \nQ^*(s,a)=\\max_{\\pi}Q^{\\pi}(s,a),\\forall s\\in\\mathcal{S},a\\in\\mathcal{A}\n\n为了使Q^*(s,a)最大，我们需要在当前状态动作对(s,a)之后都执行最优策略，于是我们得到了最有状态价值函数和最优动作价值函数之间的关系：\n\nQ^\\pi{s,a}=r(s,a)+\\gamma\\sum_{s\\in\\mathcal{S}}P(s'|s,a)V^*(s')\n\n这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值： \nV^*(s)=\\max_{a\\in\\mathcal{A}}Q^*(s,a)\n\n\n\n根据V^*(s)和Q^*(s,a)的关系，我们可以得到贝尔曼最优方程（Bellman optimality equation）：\n\n\\begin{aligned}\nV^*(s)&=\\max_{a\\in\\mathcal{A}}\\left\\{ r(s,a)+\\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)V^*(s')\\right\\}\\\\\nQ^*(s,a)=r(s,a)+\\gamma\\sum_{s'\\in \\mathcal{S}}p(s'|s,a)\\max_{a'\\in\\mathcal{A}}Q^*(s',a')\n\\end{aligned}\n\n章节 2 将介绍如何用动态规划算法得到最优策略。"
  },
  {
    "objectID": "index.html#总结",
    "href": "index.html#总结",
    "title": "强化学习基础",
    "section": "",
    "text": "马尔可夫决策过程是强化学习中的基础概念，强化学习中的环境就是一个马尔可夫决策过程。我们接下来将要介绍的强化学习算法通常都是在求解马尔可夫决策过程中的最优策略。"
  },
  {
    "objectID": "index.html#简介",
    "href": "index.html#简介",
    "title": "强化学习基础",
    "section": "3.1 简介",
    "text": "3.1 简介\n章节 2 介绍的动态规划算法要求马尔可夫决策过程是已知的，即要求与智能体交互的环境是完全已知的（例如迷宫或者给定规则的网格世界）。在此条件下，智能体其实并不需要和环境真正交互来采样数据，直接用动态规划算法就可以解出最优价值或策略。这就好比对于有监督学习任务，如果直接显式给出了数据的分布公式，那么也可以通过在期望层面上直接最小化模型的泛化误差来更新模型参数，并不需要采样任何数据点。\n但这在大部分场景下并不现实，机器学习的主要方法都是在数据分布未知的情况下针对具体的数据点来对模型做出更新的。对于大部分强化学习现实场景（例如电子游戏或者一些复杂物理环境），其马尔可夫决策过程的状态转移概率是无法写出来的，也就无法直接进行动态规划。在这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为无模型的强化学习（model-free reinforcement learning）。\n不同于动态规划算法，无模型的强化学习算法不需要事先知道环境的奖励函数和状态转移函数，而是直接使用和环境交互的过程中采样到的数据来学习，这使得它可以被应用到一些简单的实际场景中。本章将要讲解无模型的强化学习中的两大经典算法：Sarsa 和 Q-learning，它们都是基于时序差分（temporal difference，TD）的强化学习算法。同时，本章还会引入一组概念：在线策略学习和离线策略学习。通常来说，在线策略学习要求使用在当前策略下采样得到的样本进行学习，一旦策略被更新，当前的样本就被放弃了，就好像在水龙头下用自来水洗手；而离线策略学习使用经验回放池将之前采样得到的样本收集起来再次利用，就好像使用脸盆接水后洗手。因此，离线策略学习往往能够更好地利用历史数据，并具有更小的样本复杂度（算法达到收敛结果需要在环境中采样的样本数量），这使其被更广泛地应用。"
  },
  {
    "objectID": "index.html#时序差分方法",
    "href": "index.html#时序差分方法",
    "title": "强化学习基础",
    "section": "3.2 时序差分方法",
    "text": "3.2 时序差分方法\n时序差分是一种用来估计一个策略的价值函数的方法，它结合了蒙特卡洛和动态规划算法的思想。时序差分方法和蒙特卡洛的相似之处在于可以从样本数据中学习，不需要事先知道环境；和动态规划的相似之处在于根据贝尔曼方程的思想，利用后续状态的价值估计来更新当前状态的价值估计。回顾一下蒙特卡洛方法对价值函数的增量更新方式：\n\nV(s_t)\\leftarrow V(s_t)+\\alpha\\left[ G_t-V(s_t) \\right]\n\n这里我们将@sec-mc 中的\\frac{1}{N(s)}替换成了\\alpha,表示对价值估计更新的步长。可以将\\alpha取为一个常数，此时更新方式不再像MC方法那样严格取期望。MC方法必须要等整个序列结束之后才能计算得到这一次的回报G_t,而时序差分方法只需要当前步结束即可计算。具体来说，时序差分算法用当前获得的奖励加上下一个状态的价值估计来作为在当前状态会获得的回报，即：\n\nV(s_t)\\leftarrow V(s_t)+\\alpha\\left[ R_t+\\gamma V(s_{t+1})-V(s_t) \\right]\n 其中R_t+\\gamma V(s_{t+1})-V(s_t)通常被称为时序差分误差。时序差分算法将其与步长的乘积作为状态价值的更新量。可以用R_t+\\gamma V(s_{t+1})来代替G_t的原因是： \n\\begin{aligned}\nV^{\\pi}(s)&=\\mathbb{E}_{\\pi}\\left[ G_t|S_t=s \\right]\\\\\n&=\\mathbb{E}_\\pi\\left[ \\sum_{k=0}^{\\infty}\\gamma^k R_{t+k}|S_t=s \\right]\\\\\n&=\\mathbb{E}_{\\pi}\\left[ R_t+\\gamma\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}| S_t=s \\right]\\\\\n&=\\mathbb{E}_{\\pi}\\left[ R_t+\\gamma V^{\\pi}(s_{t+1})| S_t=s \\right]\n\\end{aligned}\n\n因此MC方法将上式第一行作为更新的目标而TD算法将上式最后一行作为更新的目标。于是，再用策略和环境交互时，每采样一步，我们就可以用时序差分算法来更新状态价值估计。时序差分算法用到了V(s_{t+1})的估计值，可以证明它最终收敛到策略\\pi的价值函数，我们在这里不对此进行展开说明。"
  },
  {
    "objectID": "index.html#sarsa算法",
    "href": "index.html#sarsa算法",
    "title": "强化学习基础",
    "section": "3.3 Sarsa算法",
    "text": "3.3 Sarsa算法\n既然我们可以用时序差分方法来估计价值函数，那一个很自然的问题是，我们能否用类似策略迭代的方法来进行强化学习。策略评估已经可以通过时序差分算法实现，那么在不知道奖励函数和状态转移函数的情况下该怎么进行策略提升呢？答案是可以直接用时序差分算法来估计动作价值函数Q:\n\nQ(s_t,a_t)\\leftarrow Q(s_t,a_t)+\\alpha\\left[ R_t+\\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t) \\right]\n 然后我们用贪婪算法来选取在某个状态下动作价值最大的那个动作，即\\arg\\max_{a}Q(s,a). 这样似乎已经形成了一个完整的强化学习算法： 用贪婪算法根据动作价值选取动作来和环境交互，再根据得到的数据用时序差分算法更新动作价值估计。\n然而这个简单的算法存在两个需要进一步考虑的问题。第一，如果要用时序差分算法来准确地估计策略的状态价值函数，我们需要用极大量的样本来进行更新。但实际上我们可以忽略这一点，直接用一些样本来评估策略，然后就可以更新策略了。我们可以这么做的原因是策略提升可以在策略评估未完全进行的情况进行；第二，如果在策略提升中一直根据贪婪算法得到一个确定性策略，可能会导致某些状态动作对(s,a)永远没有在序列中出现以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。\n简单常用的解决方案是不再一味使用贪婪算法，而是采用\\epsilon-贪婪策略：有1-\\epsilon的概率采用动作价值最大的那个动作（贪婪），另外有\\epsilon的概率从动作空间中随机选取一个动作（探索），其公式表示为：\n\n\\pi(a|s)=\n\\begin{cases}\n\\frac{\\epsilon}{|\\mathcal{A}|}+1-\\epsilon &\\text{如果}a=\\arg\\max_{a'}Q(s,a')\\\\\n\\frac{\\epsilon}{|\\mathcal{A}|}& 其他动作\n\\end{cases}\n\n现在，我们就可以得到一个实际的基于时序差分方法的强化学习算法。这个算法被称为 Sarsa，因为它的动作价值更新用到了当前状态s、当前动作a、获得的奖励r、下一个状态s'和下一个动作a',将这些符号拼接后就得到了算法名称，这是一种On-Policy(在线策略，a_{t+1}和a_t来自于同一策略)的时序差分强化学习算法。\n现在我们在悬崖漫步环境下尝试Sarsa算法。首先给出悬崖漫步环境的代码，此时环境不需要提供奖励函数和状态转移函数，而是需要提供一个和智能体进行交互的函数step(),该函数将智能体的动作作为输入，将奖励和下一个状态输出给智能体。\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm  # tqdm是显示循环进度条的库\n\n\nclass CliffWalkingEnv:\n    def __init__(self, ncol, nrow):\n        self.nrow = nrow\n        self.ncol = ncol\n        self.x = 0  # 记录当前智能体位置的横坐标\n        self.y = self.nrow - 1  # 记录当前智能体位置的纵坐标\n\n    def step(self, action):  # 外部调用这个函数来改变当前位置\n        # 4种动作, change[0]:上, change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n        # 定义在左上角\n        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n        self.x = min(self.ncol - 1, max(0, self.x + change[action][0]))\n        self.y = min(self.nrow - 1, max(0, self.y + change[action][1]))\n        next_state = self.y * self.ncol + self.x\n        reward = -1\n        done = False\n        if self.y == self.nrow - 1 and self.x &gt; 0:  # 下一个位置在悬崖或者目标\n            done = True\n            if self.x != self.ncol - 1:\n                reward = -100\n        return next_state, reward, done\n\n    def reset(self):  # 回归初始状态,坐标轴原点在左上角\n        self.x = 0\n        self.y = self.nrow - 1\n        return self.y * self.ncol + self.x\n\n然后我们来实现Sarsa算法，主要维护一个表格Q_table(),用来存储当前策略下所有状态动作对的价值，在用Sarsa算法和环境交互时，用\\epsilon-贪婪策略进行采样，在更新Sarsa算法时，使用时序差分的公式。我们默认终止状态时所有动作的价值都是0,这些价值在初始化为0后就不会进行更新。\n\nclass Sarsa:\n    \"\"\" Sarsa算法 \"\"\"\n    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n        self.n_action = n_action  # 动作个数\n        self.alpha = alpha  # 学习率\n        self.gamma = gamma  # 折扣因子\n        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n\n    def take_action(self, state):  # 选取下一步的操作,具体实现为epsilon-贪婪\n        if np.random.random() &lt; self.epsilon:\n            action = np.random.randint(self.n_action)\n        else:\n            action = np.argmax(self.Q_table[state])\n        return action\n\n    def best_action(self, state):  # 用于打印策略\n        Q_max = np.max(self.Q_table[state])\n        a = [0 for _ in range(self.n_action)]\n        for i in range(self.n_action):  # 若两个动作的价值一样,都会记录下来\n            if self.Q_table[state, i] == Q_max:\n                a[i] = 1\n        return a\n\n    def update(self, s0, a0, r, s1, a1):\n        td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]\n        self.Q_table[s0, a0] += self.alpha * td_error\n\n接下来我们就在悬崖漫步环境中运行Sarsa算法，一起来看看结果吧！\n\nncol = 12\nnrow = 4\nenv = CliffWalkingEnv(ncol, nrow)\nnp.random.seed(0)\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.9\nagent = Sarsa(ncol, nrow, epsilon, alpha, gamma)\nnum_episodes = 500  # 智能体在环境中运行的序列的数量\n\nreturn_list = []  # 记录每一条序列的回报\nfor i in range(10):  # 显示10个进度条\n    # tqdm的进度条功能\n    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n        for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n            episode_return = 0\n            state = env.reset()\n            action = agent.take_action(state)\n            done = False\n            while not done:\n                next_state, reward, done = env.step(action)\n                next_action = agent.take_action(next_state)\n                episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n                agent.update(state, action, reward, next_state, next_action)\n                state = next_state\n                action = next_action\n            return_list.append(episode_return)\n            if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n                pbar.set_postfix({\n                    'episode':\n                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n                    'return':\n                    '%.3f' % np.mean(return_list[-10:])\n                })\n            pbar.update(1)\n\nepisodes_list = list(range(len(return_list)))\nplt.plot(episodes_list, return_list)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('Sarsa on {}'.format('Cliff Walking'))\nplt.show()\n\n\n\n\n\n\n\n\n我们发现，随着训练的进行，Sarsa 算法获得的回报越来越高。在进行 500 条序列的学习后，可以获得 −20 左右的回报，此时已经非常接近最优策略了。然后我们看一下 Sarsa 算法得到的策略在各个状态下会使智能体采取什么样的动作。\n\ndef print_agent(agent, env, action_meaning, disaster=[], end=[]):\n    for i in range(env.nrow):\n        for j in range(env.ncol):\n            if (i * env.ncol + j) in disaster:\n                print('****', end=' ')\n            elif (i * env.ncol + j) in end:\n                print('EEEE', end=' ')\n            else:\n                a = agent.best_action(i * env.ncol + j)\n                pi_str = ''\n                for k in range(len(action_meaning)):\n                    pi_str += action_meaning[k] if a[k] &gt; 0 else 'o'\n                print(pi_str, end=' ')\n        print()\n\n\naction_meaning = ['^', 'v', '&lt;', '&gt;']\nprint('Sarsa算法最终收敛得到的策略为：')\nprint_agent(agent, env, action_meaning, list(range(37, 47)), [47])\n\nSarsa算法最终收敛得到的策略为：\nooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ovoo \nooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ovoo \n^ooo ooo&gt; ^ooo ooo&gt; ooo&gt; ooo&gt; ooo&gt; ^ooo ^ooo ooo&gt; ooo&gt; ovoo \n^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n\n\n可以发现Sarsa算法会采取比较远离悬崖的策略来抵达目标，比较保守。"
  },
  {
    "objectID": "index.html#多步sarsa算法",
    "href": "index.html#多步sarsa算法",
    "title": "强化学习基础",
    "section": "3.4 多步Sarsa算法",
    "text": "3.4 多步Sarsa算法\n蒙特卡洛方法利用当前状态之后每一步的奖励而不使用任何价值估计，时序差分算法只利用一步奖励和下一个状态的价值估计。那它们之间的区别是什么呢？总的来说，蒙特卡洛方法是无偏（unbiased）的，但是具有比较大的方差，因为每一步的状态转移都有不确定性，而每一步状态采取的动作所得到的不一样的奖励最终都会加起来，这会极大影响最终的价值估计；时序差分算法具有非常小的方差，因为只关注了一步状态转移，用到了一步的奖励，但是它是有偏的，因为用到了下一个状态的价值估计而不是其真实的价值。那有没有什么方法可以结合二者的优势呢？答案是多步时序差分！多步时序差分的意思是使用n步的奖励，然后使用之后状态的价值估计。用公式表示，将 \nG_t=R_t+\\gamma Q(s_{t+1},a_{t+1})\n 替换成 \nG_t=R_t+\\gamma R_{t+1}+\\dots+\\gamma^{n-1}R_{t+n-1}+\\gamma^n Q(s_{t+n},a_{t+n})\n 于是Sarsa算法中的动作价值函数更新公式变为 \nQ(s_t,a_t)\\leftarrow Q(s_t,a_t)+\\alpha\\left[ R_t+\\gamma R_{t+1}+\\dots+\\gamma^{n-1}R_{t+n-1}+\\gamma^n Q(s_{t+n},a_{t+n})-Q(s_t,a_t) \\right]\n 接下来将用代码实现n步Sarsa算法，在Sarsa代码基础上进行修改，引入多步时序差分计算。\n\nclass nstep_Sarsa:\n    \"\"\" n步Sarsa算法 \"\"\"\n    def __init__(self, n, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n        self.Q_table = np.zeros([nrow * ncol, n_action])\n        self.n_action = n_action\n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.n = n  # 采用n步Sarsa算法\n        self.state_list = []  # 保存之前的状态\n        self.action_list = []  # 保存之前的动作\n        self.reward_list = []  # 保存之前的奖励\n\n    def take_action(self, state):\n        if np.random.random() &lt; self.epsilon:\n            action = np.random.randint(self.n_action)\n        else:\n            action = np.argmax(self.Q_table[state])\n        return action\n\n    def best_action(self, state):  # 用于打印策略\n        Q_max = np.max(self.Q_table[state])\n        a = [0 for _ in range(self.n_action)]\n        for i in range(self.n_action):\n            if self.Q_table[state, i] == Q_max:\n                a[i] = 1\n        return a\n\n    def update(self, s0, a0, r, s1, a1, done):\n        self.state_list.append(s0)\n        self.action_list.append(a0)\n        self.reward_list.append(r)\n        if len(self.state_list) == self.n:  # 若保存的数据可以进行n步更新\n            G = self.Q_table[s1, a1]  # 得到Q(s_{t+n}, a_{t+n})\n            for i in reversed(range(self.n)):\n                G = self.gamma * G + self.reward_list[i]  # 不断向前计算每一步的回报\n                # 如果到达终止状态,最后几步虽然长度不够n步,也将其进行更新\n                if done and i &gt; 0:\n                    s = self.state_list[i]\n                    a = self.action_list[i]\n                    self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])\n            s = self.state_list.pop(0)  # 将需要更新的状态动作从列表中删除,下次不必更新\n            a = self.action_list.pop(0)\n            self.reward_list.pop(0)\n            # n步Sarsa的主要更新步骤\n            self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])\n        if done:  # 如果到达终止状态,即将开始下一条序列,则将列表全清空\n            self.state_list = []\n            self.action_list = []\n            self.reward_list = []\n\n\nnp.random.seed(0)\nn_step = 5  # 5步Sarsa算法\nalpha = 0.1\nepsilon = 0.1\ngamma = 0.9\nagent = nstep_Sarsa(n_step, ncol, nrow, epsilon, alpha, gamma)\nnum_episodes = 500  # 智能体在环境中运行的序列的数量\n\nreturn_list = []  # 记录每一条序列的回报\nfor i in range(10):  # 显示10个进度条\n    #tqdm的进度条功能\n    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n        for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n            episode_return = 0\n            state = env.reset()\n            action = agent.take_action(state)\n            done = False\n            while not done:\n                next_state, reward, done = env.step(action)\n                next_action = agent.take_action(next_state)\n                episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n                agent.update(state, action, reward, next_state, next_action,\n                             done)\n                state = next_state\n                action = next_action\n            return_list.append(episode_return)\n            if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n                pbar.set_postfix({\n                    'episode':\n                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n                    'return':\n                    '%.3f' % np.mean(return_list[-10:])\n                })\n            pbar.update(1)\n\nepisodes_list = list(range(len(return_list)))\nplt.plot(episodes_list, return_list)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('5-step Sarsa on {}'.format('Cliff Walking'))\nplt.show()\n\n\n\n\n\n\n\n\n通过实验结果可以发现，5步Sarsa算法的收敛速度比单步Sarsa算法更快，我们看下此时的策略表现。\n\naction_meaning = ['^', 'v', '&lt;', '&gt;']\nprint('5步Sarsa算法最终收敛得到的策略为：')\nprint_agent(agent, env, action_meaning, list(range(37, 47)), [47])\n\n5步Sarsa算法最终收敛得到的策略为：\nooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ovoo \n^ooo ^ooo ^ooo oo&lt;o ^ooo ^ooo ^ooo ^ooo ooo&gt; ooo&gt; ^ooo ovoo \nooo&gt; ^ooo ^ooo ^ooo ^ooo ^ooo ^ooo ooo&gt; ooo&gt; ^ooo ooo&gt; ovoo \n^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n\n\n我们发现此时多步 Sarsa 算法得到的策略会在最远离悬崖的一边行走，以保证最大的安全性。"
  },
  {
    "objectID": "index.html#q-learning-算法",
    "href": "index.html#q-learning-算法",
    "title": "强化学习基础",
    "section": "3.5 Q-learning 算法",
    "text": "3.5 Q-learning 算法\n除了 Sarsa，还有一种非常著名的基于时序差分算法的强化学习算法——Q-learning。Q-learning 和 Sarsa 的最大区别在于 Q-learning 的时序差分更新方式为\n\nQ(s_t,a_t)\\leftarrow Q(s_t,a_t)+\\alpha\\left[ R_t+\\gamma \\max_{a}Q(s_{t+1},a)-Q(s_t,a_t) \\right]\n\n我们可以用价值迭代的思想来理解Q-learning, 即Q-learning 是直接在估计Q^*,因为动作价值函数的贝尔曼最优方程是 \nQ^*(s,a)=r(s,a)+\\gamma \\sum_{s'\\in\\mathcal{S}}P(s'|s,a)\\max_{a'}Q^*(s',a')\n 而 Sarsa估计当前\\epsilon-贪婪策略的动作价值函数。需要强调的是，Q-learning的更新并非必须使用当前贪心策略\\arg\\max_{a}Q(s,a)采样得到的数据，因为给定任意(s,a,r,s')都可以直接根据更新公式来更新Q,为了探索，我们通常使用一个\\epsilon-贪婪策略采样得到的数据，因为它的更新中用到的Q(s',a')的a'是当前策略在s'下的动作。我们称Sarsa是在线策略（On-policy）算法，称Q-learning是离线策略（Off-policy）算法，这两个概念在强化学习中非常重要。\n\n3.5.1 在线策略与离线策略\n我们称采样数据的策略为行为策略（behavior policy），称用这些数据来更新的策略为目标策略（target policy）。在线策略（on-policy）算法表示行为策略和目标策略是同一个策略；而离线策略（off-policy）算法表示行为策略和目标策略不是同一个策略。Sarsa 是典型的在线策略算法，而 Q-learning 是典型的离线策略算法。判断二者类别的一个重要手段是看计算时序差分的价值目标的数据是否来自当前的策略，如 图 2 所示。具体而言：\n- 对于Sarsa，它的更新公式必须使用来自当前策略采样得到的五元组(s,a,r,s',a'),因此它是在线策略学习方法； - 对于Q-learning，它的更新公式使用的是四元组(s,a,r,s')来更新当前状态动作对的价值Q(s,a),数据中的s,a是给定的条件，r,s'皆由环境采样得到，该四元组并不需要一定是当前策略采样得到的数据，也可以来自行为策略，因此它是离线策略算法。\n\n\n\n\n\n\n图 2： Sarsa和Q-learning的对比\n\n\n\n在之后的讲解中，我们会注明各个算法分别属于这两类中的哪一类。如前文所述，离线策略算法能够重复使用过往训练样本，往往具有更小的样本复杂度，也因此更受欢迎。\n我们接下来仍然在悬崖漫步环境下来实现 Q-learning 算法。\n\nclass QLearning:\n    \"\"\" Q-learning算法 \"\"\"\n    def __init__(self, ncol, nrow, epsilon, alpha, gamma, n_action=4):\n        self.Q_table = np.zeros([nrow * ncol, n_action])  # 初始化Q(s,a)表格\n        self.n_action = n_action  # 动作个数\n        self.alpha = alpha  # 学习率\n        self.gamma = gamma  # 折扣因子\n        self.epsilon = epsilon  # epsilon-贪婪策略中的参数\n\n    def take_action(self, state):  #选取下一步的操作\n        if np.random.random() &lt; self.epsilon:\n            action = np.random.randint(self.n_action)\n        else:\n            action = np.argmax(self.Q_table[state])\n        return action\n\n    def best_action(self, state):  # 用于打印策略\n        Q_max = np.max(self.Q_table[state])\n        a = [0 for _ in range(self.n_action)]\n        for i in range(self.n_action):\n            if self.Q_table[state, i] == Q_max:\n                a[i] = 1\n        return a\n\n    def update(self, s0, a0, r, s1):\n        td_error = r + self.gamma * self.Q_table[s1].max(\n        ) - self.Q_table[s0, a0]\n        self.Q_table[s0, a0] += self.alpha * td_error\n\n\nnp.random.seed(0)\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.9\nagent = QLearning(ncol, nrow, epsilon, alpha, gamma)\nnum_episodes = 500  # 智能体在环境中运行的序列的数量\n\nreturn_list = []  # 记录每一条序列的回报\nfor i in range(10):  # 显示10个进度条\n    # tqdm的进度条功能\n    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n        for i_episode in range(int(num_episodes / 10)):  # 每个进度条的序列数\n            episode_return = 0\n            state = env.reset()\n            done = False\n            while not done:\n                action = agent.take_action(state)\n                next_state, reward, done = env.step(action)\n                episode_return += reward  # 这里回报的计算不进行折扣因子衰减\n                agent.update(state, action, reward, next_state)\n                state = next_state\n            return_list.append(episode_return)\n            if (i_episode + 1) % 10 == 0:  # 每10条序列打印一下这10条序列的平均回报\n                pbar.set_postfix({\n                    'episode':\n                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n                    'return':\n                    '%.3f' % np.mean(return_list[-10:])\n                })\n            pbar.update(1)\n\nepisodes_list = list(range(len(return_list)))\nplt.plot(episodes_list, return_list)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('Q-learning on {}'.format('Cliff Walking'))\nplt.show()\n\naction_meaning = ['^', 'v', '&lt;', '&gt;']\nprint('Q-learning算法最终收敛得到的策略为：')\nprint_agent(agent, env, action_meaning, list(range(37, 47)), [47])\n\n\n\n\n\n\n\n\nQ-learning算法最终收敛得到的策略为：\n^ooo ovoo ovoo ^ooo ^ooo ovoo ooo&gt; ^ooo ^ooo ooo&gt; ooo&gt; ovoo \nooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ^ooo ooo&gt; ooo&gt; ooo&gt; ooo&gt; ovoo \nooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ooo&gt; ovoo \n^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n\n\n需要注意的是，打印出来的回报是行为策略在环境中交互得到的，而不是 Q-learning 算法在学习的目标策略的真实回报。我们把目标策略的行为打印出来后，发现其更偏向于走在悬崖边上，这与 Sarsa 算法得到的比较保守的策略相比是更优的。 但是仔细观察 Sarsa 和 Q-learning 在训练过程中的回报曲线图，我们可以发现，在一个序列中 Sarsa 获得的期望回报是高于 Q-learning 的。这是因为在训练过程中智能体采取基于当前Q(s,a)函数的\\epsilon-贪婪策略来平衡探索与利用，Q-learning 算法由于沿着悬崖边走，会以一定概率探索“掉入悬崖”这一动作，而 Sarsa 相对保守的路线使智能体几乎不可能掉入悬崖。"
  },
  {
    "objectID": "index.html#小结",
    "href": "index.html#小结",
    "title": "强化学习基础",
    "section": "3.6 小结",
    "text": "3.6 小结\n本章介绍了无模型的强化学习中的一种非常重要的算法——时序差分算法。时序差分算法的核心思想是用对未来动作选择的价值估计来更新对当前动作选择的价值估计，这是强化学习中的核心思想之一。本章重点讨论了 Sarsa 和 Q-learning 这两个最具有代表性的时序差分算法。当环境是有限状态集合和有限动作集合时，这两个算法非常好用，可以根据任务是否允许在线策略学习来决定使用哪一个算法。 值得注意的是，尽管离线策略学习可以让智能体基于经验回放池中的样本来学习，但需要保证智能体在学习的过程中可以不断和环境进行交互，将采样得到的最新的经验样本加入经验回放池中，从而使经验回放池中有一定数量的样本和当前智能体策略对应的数据分布保持很近的距离。如果不允许智能体在学习过程中和环境进行持续交互，而是完全基于一个给定的样本集来直接训练一个策略，这样的学习范式被称为离线强化学习（offline reinforcement learning），之后的章节将会介绍离线强化学习的相关知识。"
  },
  {
    "objectID": "index.html#简介-1",
    "href": "index.html#简介-1",
    "title": "强化学习基础",
    "section": "4.1 简介",
    "text": "4.1 简介\n在强化学习中，“模型”通常指与智能体交互的环境模型，即对环境的状态转移概率和奖励函数进行建模。根据是否具有环境模型，强化学习算法分为两种：基于模型的强化学习（model-based reinforcement learning）和无模型的强化学习（model-free reinforcement learning）。无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计，章节 3 讨论的两种时序差分算法，即 Sarsa 和 Q-learning 算法，便是两种无模型的强化学习方法，本书在后续章节中将要介绍的方法也大多是无模型的强化学习算法。在基于模型的强化学习中，模型可以是事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后用这个模型帮助策略提升或者价值估计。章节 2 讨论的两种动态规划算法，即策略迭代和价值迭代，则是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的。本章即将介绍的 Dyna-Q 算法也是非常基础的基于模型的强化学习算法，不过它的环境模型是通过采样数据估计得到的。\n强化学习算法有两个重要的评价指标：一个是算法收敛后的策略在初始状态下的期望回报，另一个是样本复杂度，即算法达到收敛结果需要在真实环境中采样的样本数量。基于模型的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，对真实环境中样本的需求量往往就会减少，因此通常会比无模型的强化学习算法具有更低的样本复杂度。但是，环境模型可能并不准确，不能完全代替真实环境，因此基于模型的强化学习算法收敛后其策略的期望回报可能不如无模型的强化学习算法。"
  },
  {
    "objectID": "index.html#dyna-q",
    "href": "index.html#dyna-q",
    "title": "强化学习基础",
    "section": "4.2 Dyna-Q",
    "text": "4.2 Dyna-Q\nDyna-Q算法是一个经典的基于模型的强化学习算法，如 图 3 所示，Dyna-Q使用一种叫做Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态s，采取一个曾经在该状态下执行过的动作a,通过模型得到转移后的状态s'以及奖励r,并根据这个模拟数据(s,a,r,s'),用Q-learning的更新方式来更新动作价值函数。\n\n\n\n\n\n\n图 3： 基于模型的强化学习方法与无模型的强化学习\n\n\n\n\n在每次与环境进行交互执行一次 Q-learning 之后，Dyna-Q 会做n次 Q-planning。其中 Q-planning 的次数N是一个事先可以选择的超参数，当其为 0 时就是普通的 Q-learning。值得注意的是，上述 Dyna-Q 算法是执行在一个离散并且确定的环境中，所以当看到一条经验数据(s,a,r,s')时，可以直接对模型做出更新，即M(s,a)\\leftarrow r,s'."
  },
  {
    "objectID": "index.html#简介-2",
    "href": "index.html#简介-2",
    "title": "强化学习基础",
    "section": "5.1 简介",
    "text": "5.1 简介\n在 章节 3 讲解的Q-learning算法中，我们以矩阵的方式建立了一张存储每个状态下所有动作Q值得表格，表格中的每一个动作价值Q(s,a)表示在状态s下选择动作a然后继续遵循某一策略预期能够得到的期望回报。然而，这种用表格存储动作价值的做法只在环境的状态和动作都是离散的，并且空间都比较小的情况下适用，我们之前进行代码实战的几个环境都是如此（如悬崖漫步）。当状态或者动作数量非常大的时候，这种做法就不适用了，更甚者，当状态或者动作连续的时候，就有无限个状态动作对，我们更加无法使用这种表格形式来记录各个状态动作对的Q值。\n对于这种情况，我们需要用函数拟合的方法来估计Q值，即将这个复杂的Q值表格视作数据，使用一个参数化得函数Q_\\theta来拟合这些数据。很显然，这种函数拟合的方法存在一定的精度损失，因此被称为近似方法。我们今天要介绍的 DQN 算法便可以用来解决连续状态下离散动作的问题。"
  },
  {
    "objectID": "index.html#dqn",
    "href": "index.html#dqn",
    "title": "强化学习基础",
    "section": "5.2 DQN",
    "text": "5.2 DQN\n在类似车杆的环境中得到动作价值函数Q(s,a),由于状态每一维度的值都是连续的，无法使用表格记录，因此一个常见的解决方法便是函数拟合的思想。由于神经网络具有强大的表达能力，因此我们可以用一个神经网络来表示函数Q.若动作是连续（无限）的，神经网络的输入是状态s和动作a,然后输出一个标量，表示在状态s下采取动作a能获得的价值。若动作是离散（有限）的，除了可以采取动作连续情况下的做法，还可以只将状态s输入到神经网络中，使其同时输出每一个动作的Q值。通常DQN（以及Q-learning）只能处理动作离散的情况，因为在函数Q的更新过程中有\\max_a这一操作。假设神经网络用来拟合函数的参数是\\omega，即每一个状态s下所有可能动作a的Q值我们都能表示为Q_\\omega(s,a).我们将用于拟合Q的神经网络称为Q网络，如 图 4 所示。\n\n\n\n\n\n\n图 4： 工作在CartPole环境中的Q网络示意图\n\n\n\n那么Q网络的损失函数是什么样的呢？我们先回顾一下Q-learning的更新规则： \nQ(s,a)\\leftarrow Q(s,a)+\\alpha\\left[ r+\\gamma \\max_{a'\\in\\mathcal{A}}Q(s',a')-Q(s,a) \\right]\n\n上述公式用时序差分学习目标r+\\gamma \\max_{a'\\in\\mathcal{A}}Q(s',a')来增量式更新Q(s,a),也就是说要使Q(s,a)和TD目标r+\\gamma \\max_{a'\\in\\mathcal{A}}Q(s',a')靠近。于是，对于一组数据\\{(s_i,a_i,r_i,s_i')\\}，我们可以很自然地将Q网络的损失函数构造为均方误差的形式：\n\n\\omega^*=\\arg\\max_{\\omega}\\frac{1}{2N}\\sum_{i=1}^n\\left[Q_\\omega(s_i,a_i)-(r_i+\\gamma\\max_{a'}Q_\\omega(s_i',a'))\\right]^2\n\n至此，我们就可以将 Q-learning 扩展到神经网络形式——深度 Q 网络（deep Q network，DQN）算法。由于 DQN 是离线策略算法，因此我们在收集数据的时候可以使用一个\\epsilon-贪婪策略来平衡探索与利用，将收集到的数据存储起来，在后续的训练中使用。DQN 中还有两个非常重要的模块——经验回放和目标网络，它们能够帮助 DQN 取得稳定、出色的性能。\nDQN其余的内容留到后面再更新。"
  },
  {
    "objectID": "index.html#简介-3",
    "href": "index.html#简介-3",
    "title": "强化学习基础",
    "section": "6.1 简介",
    "text": "6.1 简介\n本书之前介绍的 Q-learning、DQN 及 DQN 改进算法都是基于价值（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。策略梯度是基于策略的方法的基础，本章从策略梯度算法说起。"
  },
  {
    "objectID": "index.html#策略梯度",
    "href": "index.html#策略梯度",
    "title": "强化学习基础",
    "section": "6.2 策略梯度",
    "text": "6.2 策略梯度\n基于策略的方法首先需要将策略参数化。假设目标策略\\pi_\\theta是一个随机性策略且处处可微，其中\\theta使对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。我们的目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为\n\nJ(\\theta)=\\mathbb{E}_{s_0}[V^{\\pi_\\theta}(s_0)]\n 其中s_0表示初始状态。现在有了目标函数，我们将目标函数对策略\\theta求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数从而得到最优策略。\n\n\\begin{aligned}\n\\nabla_\\theta J(\\theta)&\\propto \\sum_{s\\in\\mathcal{S}}\\nu^{\\pi_\\theta}(s)\\sum_{a\\in\\mathcal{A}}Q^{\\pi_\\theta}(s,a)\\nabla_\\theta\\pi_\\theta(a|s)\\\\\n&=\\sum_{s\\in\\mathcal{S}}\\nu^{\\pi_\\theta}(s)\\sum_{a\\in\\mathcal{A}}\\pi_\\theta(a|s)Q^{\\pi_\\theta}(s,a)\\frac{\\nabla_\\theta\\pi_\\theta(a|s)}{\\pi_\\theta(a|s)}\\\\\n&=\\mathbb{E}_{\\pi_\\theta}\\left[ Q^{\\pi_\\theta}(s,a)\\nabla_\\theta\\log\\pi_\\theta(a|s) \\right]\n\\end{aligned}\n 其中\\nu^\\pi使策略\\pi下的状态访问分布。\n上面的梯度可以用来更新策略。需要注意的是，因为上式中期望\\mathbb{E}的下标是\\pi_\\theta，所以策略梯度算法为在线策略（on-policy）算法，即必须使用当前策略\\pi_\\theta采样得到的数据来计算梯度。直观理解一下策略梯度这个公式，可以发现在每一个状态下，梯度的修改是让策略更多地去采样到带来较高Q值的动作，更少地去采样到带来较低Q值的动作。\n在计算策略梯度的公式中，需要用到Q^{\\pi_\\theta}(s,a),可以用多种方法对它进行估计。接下来要介绍的REINFORCE算法便是采用了MC方法来估计Q^{\\pi_\\theta}(s,a),对于一个有限步数的环境来说，REINFORCE算法中的策略梯度为： \n\\nabla_\\theta J(\\theta)=\\mathbb{E}_{\\pi_\\theta}\\left[ \\sum_{t=0}^T\\left( \\sum_{t'=t}^T\\gamma^{t'-t}r_{t'} \\right)\\nabla_\\theta \\log\\pi_\\theta(a_t|s_t) \\right]\n 其中T是和环境交互的最大步数。"
  },
  {
    "objectID": "index.html#简介-4",
    "href": "index.html#简介-4",
    "title": "强化学习基础",
    "section": "7.1 简介",
    "text": "7.1 简介\n本书之前的章节讲解了基于值函数的方法（DQN）和基于策略的方法（REINFORCE），其中基于值函数的方法只学习一个价值函数，而基于策略的方法只学习一个策略函数。那么，一个很自然的问题是，有没有什么方法既学习价值函数，又学习策略函数呢？答案就是 Actor-Critic。Actor-Critic 是囊括一系列算法的整体架构，目前很多高效的前沿算法都属于 Actor-Critic 算法，本章接下来将会介绍一种最简单的 Actor-Critic 算法。需要明确的是，Actor-Critic 算法本质上是基于策略的算法，因为这一系列算法的目标都是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好地学习。"
  },
  {
    "objectID": "index.html#actor-critic",
    "href": "index.html#actor-critic",
    "title": "强化学习基础",
    "section": "7.2 Actor-Critic",
    "text": "7.2 Actor-Critic\n回顾一下，在 REINFORCE 算法中，目标函数的梯度中有一项轨迹回报，用于指导策略的更新。REINFOCE 算法用蒙特卡洛方法来估计Q(s,a),那能不能考虑拟合一个值函数来指导策略进行学习呢？这正是Actor-Critic算法所做的。在策略梯度中，可以把梯度写成下面这个更一般的形式： \ng=\\mathbb{E}\\left[ \\sum_{t=0}^T\\psi_t\\nabla_\\theta\\log\\pi_\\theta(a_t|s_t) \\right]\n 其中\\psi_t可以用很多种形式：\n\n\\sum_{t'=0}^T\\gamma^{t'}r_{t'}:轨迹的总回报；\n\n\\sum_{t'=t}^T\\gamma^{t'-t}r_{t'}:动作a_t之后的总回报；\n\nsum_{t'=t}^T\\gamma^{t'-t}r_{t'}-b(s_t):基准线版本的改进；\n\nQ^{\\pi_\\theta}(s_t,a_t):动作价值函数；\n\nA^{\\pi_\\theta}(s_t,a_t):优势函数；\n\nr_t+\\gamma V^{\\pi_\\theta}(s_{t+1})-V^{\\pi_\\theta}(s_{t}):时序差分残差。\n\nREINFORCE通过MC采样的方法对策略梯度的估计是无偏的但是方差非常大，我们可以用形式3引入基线函数b(s_t)来减小方差。此外，我们也可以采用Actor-Critic算法估计一个动作价值函数Q,代替MC采样得到的回报，这便是形式4.这个时候我们可以把状态价值函数V作为基线，从Q函数减去这个V函数则得到了A函数，称之为优势函数，即形式5.更进一步，我们可以利用Q=r+\\gamma V得到形式6.\n本章主要介绍形式6，即通过时序差分残差\\psi_t=r_t+\\gamma V^{\\pi_\\theta}(s_{t+1})-V^{\\pi_\\theta}(s_{t})来知道策略梯度进行学习。事实上，用Q值或者V值本质上也是用奖励来进行指导，但是用神经网络进行估计的方法可以减小方差、提高鲁棒性。除此之外，REINFORCE 算法基于蒙特卡洛采样，只能在序列结束后进行更新，这同时也要求任务具有有限的步数，而 Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。\n我们将Actor-Critic分为两部分：Actor（策略网络）和Critic（价值网络），如 图 5 所示。\n\nActor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略。\nCritic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。\n\n\n\n\n\n\n\n图 5： Actor和Critic的关系\n\n\n\nActor的更新采用策略梯度的原则，那Critic如何更新呢？我们将Critic价值网络表示为V_\\omega,参数为\\omega.于是，我们可以采用时序差分残差的学习方式，对于单个数据定义如下价值函数的损失函数： \n\\mathcal{L}(\\omega)=\\frac{1}{2}\\left(r+\\gamma V_\\omega(s_{t+1})-V_\\omega(s_t)\\right)^2\n 与DQN中一样，我们采取类似与目标网络的方法，将上式中r+\\gamma V_\\omega(s_{t+1})作为时序差分目标，不会产生梯度来更新价值函数，因此，价值函数的梯度为: \n\\nabla_\\omega\\mathcal{L}=-(r+\\gamma V_\\omega(s_{t+1})-V_\\omega(s_t))\\nabla_{\\omega}V_\\omega(s_t)\n 然后使用梯度下降方法来更新Critic价值网络参数即可。\n\n\n\\begin{algorithm} \\caption{Actor-Critic算法流程} \\begin{algorithmic} \\State \\textbf{初始化策略网络参数}$\\theta$, \\textbf{价值网络参数}$\\omega$ \\For{$e=1\\rightarrow E$} \\State \\textbf{用当前策略}$\\pi_{\\theta}$\\textbf{采样轨迹}$\\{s_1,a_1,r_1,s_2,a_2,r_2,...\\}$ \\State \\textbf{为每一步数据计算}$\\delta_t=r_t+\\gamma V_{\\omega}(s_{t+1})-V_\\omega(s_t)$ \\State \\textbf{更新价值参数}$\\omega=\\omega+\\alpha_\\omega\\sum_t\\delta_t\\nabla_{\\omega}V_\\omega(s_t)$ \\State \\textbf{更新策略参数}$\\theta=\\theta+\\alpha_\\theta\\sum_t\\delta_t\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n以上就是Actor-Critic算法的流程，下面我们用代码实现它！"
  },
  {
    "objectID": "index.html#actor-critic-代码实践",
    "href": "index.html#actor-critic-代码实践",
    "title": "强化学习基础",
    "section": "7.4 Actor-Critic 代码实践",
    "text": "7.4 Actor-Critic 代码实践\n我们想在车杆环境下进行A-C算法的实验，引入一些必要的包\n\nimport gym\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport rl_utils\nfrom tqdm import tqdm\n\n其中rl_utils文件如下所示：\n\n\nCode\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport collections\nimport random\n\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = collections.deque(maxlen=capacity) \n\n    def add(self, state, action, reward, next_state, done): \n        self.buffer.append((state, action, reward, next_state, done)) \n\n    def sample(self, batch_size): \n        transitions = random.sample(self.buffer, batch_size)\n        state, action, reward, next_state, done = zip(*transitions)\n        return np.array(state), action, reward, np.array(next_state), done \n\n    def size(self): \n        return len(self.buffer)\n\ndef moving_average(a, window_size):\n    cumulative_sum = np.cumsum(np.insert(a, 0, 0)) \n    middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size\n    r = np.arange(1, window_size-1, 2)\n    begin = np.cumsum(a[:window_size-1])[::2] / r\n    end = (np.cumsum(a[:-window_size:-1])[::2] / r)[::-1]\n    return np.concatenate((begin, middle, end))\n\ndef train_on_policy_agent(env, agent, num_episodes):\n    return_list = []\n    for i in range(10):\n        with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar:\n            for i_episode in range(int(num_episodes/10)):\n                episode_return = 0\n                transition_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}\n                state = env.reset(seed=0)[0]\n                done = False\n                while not done:\n                    action = agent.take_action(state)\n                    next_state, reward, done, _,_ = env.step(action)\n                    transition_dict['states'].append(state)\n                    transition_dict['actions'].append(action)\n                    transition_dict['next_states'].append(next_state)\n                    transition_dict['rewards'].append(reward)\n                    transition_dict['dones'].append(done)\n                    state = next_state\n                    episode_return += reward\n                return_list.append(episode_return)\n                agent.update(transition_dict)\n                if (i_episode+1) % 10 == 0:\n                    pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})\n                pbar.update(1)\n    return return_list\n\ndef train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size):\n    return_list = []\n    for i in range(10):\n        with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar:\n            for i_episode in range(int(num_episodes/10)):\n                episode_return = 0\n                state = env.reset(seed=0)[0]\n                done = False\n                steps = 0\n                while not done and steps &lt;200:\n                    steps+=1\n                    action = agent.take_action(state)\n                    next_state, reward, done, _,_ = env.step(action)\n                    replay_buffer.add(state, action, reward, next_state, done)\n                    state = next_state\n                    episode_return += reward\n                    if replay_buffer.size() &gt; minimal_size:\n                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n                        transition_dict = {'states': b_s, 'actions': b_a, 'next_states': b_ns, 'rewards': b_r, 'dones': b_d}\n                        agent.update(transition_dict)\n                return_list.append(episode_return)\n                if (i_episode+1) % 10 == 0:\n                    pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})\n                pbar.update(1)\n    return return_list\n\n\ndef compute_advantage(gamma, lmbda, td_delta):\n    td_delta = td_delta.detach().numpy()\n    advantage_list = []\n    advantage = 0.0\n    for delta in td_delta[::-1]:\n        advantage = gamma * lmbda * advantage + delta\n        advantage_list.append(advantage)\n    advantage_list.reverse()\n    return torch.tensor(advantage_list, dtype=torch.float)\n\n\n首先定义策略网络PolicyNet,其输入是某个状态，输出是该状态下的动作概率分布，这里采用在离散动作空间上的softmax()函数来实现一个可学习的多项分布。\n\nclass PolicyNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(PolicyNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return F.softmax(self.fc2(x), dim=1)\n\nA-C算法额外引入一个价值网络，接下来的代码定义价值网络ValueNet,其输入是某个状态，输出是状态的价值。\n\nclass ValueNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim):\n        super(ValueNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n现在定义ActorCritic算法，主要包含采取动作take_action()和更新网络参数update()两个函数。\n\n\nCode\nclass ActorCritic:\n    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n                 gamma, device):\n        # 策略网络\n        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n        self.critic = ValueNet(state_dim, hidden_dim).to(device)  # 价值网络\n        # 策略网络优化器\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n                                                lr=actor_lr)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=critic_lr)  # 价值网络优化器\n        self.gamma = gamma\n        self.device = device\n\n    def take_action(self, state):\n        # state = torch.tensor([state], dtype=torch.float).to(self.device)\n        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n        probs = self.actor(state)\n        action_dist = torch.distributions.Categorical(probs)\n        action = action_dist.sample()\n        return action.item()\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n            self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n\n        # 时序差分目标\n        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n                                                                       dones)\n        td_delta = td_target - self.critic(states)  # 时序差分误差\n        log_probs = torch.log(self.actor(states).gather(1, actions))\n        actor_loss = torch.mean(-log_probs * td_delta.detach())\n        # 均方误差损失函数\n        critic_loss = torch.mean(\n            F.mse_loss(self.critic(states), td_target.detach()))\n        self.actor_optimizer.zero_grad()\n        self.critic_optimizer.zero_grad()\n        actor_loss.backward()  # 计算策略网络的梯度\n        critic_loss.backward()  # 计算价值网络的梯度\n        self.actor_optimizer.step()  # 更新策略网络的参数\n        self.critic_optimizer.step()  # 更新价值网络的参数\n\n\n定义好Actor和Critic,我们就可以开始实验了，看看Actor-Critic在车杆环境下表现如何吧（下面的代码块执行时间会非常长，我暂时先不运行，只给出结果，代码是正确的）！\n\n\nCode\nactor_lr = 1e-3\ncritic_lr = 1e-2\nnum_episodes = 1000\nhidden_dim = 128\ngamma = 0.98\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n    \"cpu\")\n\nenv_name = 'CartPole-v1'\nenv = gym.make(env_name, render_mode='human')\n\ntorch.manual_seed(0)\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nagent = ActorCritic(state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n                    gamma, device)\n\nreturn_list = rl_utils.train_on_policy_agent(env, agent, num_episodes)\n\nepisodes_list = list(range(len(return_list)))\nplt.plot(episodes_list, return_list)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('Actor-Critic on {}'.format(env_name))\nplt.show()\n\nmv_return = rl_utils.moving_average(return_list, 9)\nplt.plot(episodes_list, mv_return)\nplt.xlabel('Episodes')\nplt.ylabel('Returns')\nplt.title('Actor-Critic on {}'.format(env_name))\nplt.show()\n\n\n\n\n\n用Actor-Critic算法寻找CartPole最优策略的结果\n\n\n根据实验结果我们可以发现，Actor-Critic 算法很快便能收敛到最优策略，并且训练过程非常稳定，价值函数的引入减小了方差，抖动情况也不严重。"
  },
  {
    "objectID": "index.html#cartpole环境",
    "href": "index.html#cartpole环境",
    "title": "强化学习基础",
    "section": "7.3 CartPole环境",
    "text": "7.3 CartPole环境\n在介绍A-C算法的代码之前，我们先介绍一下如 图 6 所示的CartPole环境，它的状态值是连续的，动作值是离散的。\n\n\n\n\n\n\n图 6： CartPole环境示意图\n\n\n\n在车杆环境中，有一辆小车，智能体的任务是通过左右移动保持车上的杆竖直，若杆的倾斜度数过大，或者车子离初始位置左右的偏离程度过大，或者坚持时间到达 200 帧，则游戏结束。智能体的状态是一个维数为 4 的向量，每一维都是连续的，其动作是离散的，动作空间大小为 2，详见 表 1 , 表 2 .\n\n\n\n表 1： CartPole环境的状态空间\n\n\n\n\n\n维度\n意义\n最小值\n最大值\n\n\n\n\n0\n车的位置\n-2.4\n2.4\n\n\n1\n车的速度\n-Inf\nInf\n\n\n2\n杆的角度\n~-41.8°\n~41.8°\n\n\n3\n杆尖端的速度\n-Inf\nInf\n\n\n\n\n\n\n\n\n\n表 2： CartPole环境的动作空间\n\n\n\n\n\n标号\n动作\n\n\n\n\n0\n向左移动小车\n\n\n1\n向右移动小车"
  },
  {
    "objectID": "index.html#小结-1",
    "href": "index.html#小结-1",
    "title": "强化学习基础",
    "section": "7.5 小结",
    "text": "7.5 小结\n本章讲解了 Actor-Critic 算法，它是基于值函数的方法和基于策略的方法的叠加。价值模块 Critic 在策略模块 Actor 采样的数据中学习分辨什么是好的动作，什么不是好的动作，进而指导 Actor 进行策略更新。随着 Actor 的训练的进行，其与环境交互所产生的数据分布也发生改变，这需要 Critic 尽快适应新的数据分布并给出好的判别。\nActor-Critic 算法非常实用，后续章节中的 TRPO、PPO、DDPG、SAC 等深度强化学习算法都是在 Actor-Critic 框架下进行发展的。深入了解 Actor-Critic 算法对读懂目前深度强化学习的研究热点大有裨益。"
  },
  {
    "objectID": "index.html#简介-5",
    "href": "index.html#简介-5",
    "title": "强化学习基础",
    "section": "8.1 简介",
    "text": "8.1 简介\n本书之前介绍的基于策略的方法包括策略梯度算法和Actor-Critic算法。这两种方法虽然简单直观，但在实际应用过程中会遇到训练不稳定的情况。回顾一下基于策略的方法：参数化智能体的策略，并设计衡量策略好坏的函数，通过梯度上升的方法来最大化这个目标函数使得策略最优。具体来说，假设\\theta表示策略\\pi_{\\theta}的参数。定义J(\\theta)=\\mathbb{E}_{\\s_0}\\left[V^{\\pi_\\theta}(s_0)\\right]=\\mathbb{E}_{\\pi_{\\theta}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^tr(s_t,a_t) \\right],基于策略的方法的目标是找到\\theta^*=\\arg\\max_\\theta J(\\theta),策略梯度算法主要沿着\\nabla_{\\theta}J(\\theta)方向迭代更新策略参数\\theta.但是这种算法有一个明显的缺点：当策略网络是深度模型时，沿着策略梯度更新参数，很有可能由于步长太长，策略突然显著变差，进而影响训练效果。\n针对以上问题，我们考虑在更新时找到一块信任区域（trust region），在这个区域上更新策略时能够得到某种策略性能的安全性保证，这就是信任区域策略优化（trust region policy optimization，TRPO）算法的主要思想。TRPO 算法在 2015 年被提出，它在理论上能够保证策略学习的性能单调性，并在实际应用中取得了比策略梯度算法更好的效果。"
  },
  {
    "objectID": "index.html#策略目标",
    "href": "index.html#策略目标",
    "title": "强化学习基础",
    "section": "8.2 策略目标",
    "text": "8.2 策略目标\n假设当前策略为\\pi_{\\theta},参数为\\theta.我们考虑如何借助当前的\\theta找到一个更优秀的\\theta',是的J(\\theta')&gt;J(\\theta). 具体来说，由于初始状态s_0的分布与策略无关，因此行数策略\\pi_{\\theta}下的优化目标J(\\theta)可以写成在新策略\\pi_{\\theta'}的期望形式:\n\nV^{\\pi_{\\theta}}(s_t)=\\mathbb{E}_{a_t\\sim\\pi_{\\theta}}[r(s_t,a_t)+\\gamma V^{\\pi_{\\theta}}(s_{t+1})]\n\n\n\\begin{aligned}\nJ(\\theta)&=\\mathbb{E}_{s_0}\\left[V^{\\pi_{\\theta}}(s_0)\\right]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta'}}\\left[\\sum_{t=0}^\\infty \\gamma^t V^{\\pi_{\\theta}}(s_t)-\\sum_{t=1}^\\infty \\gamma^t V^{\\pi_{\\theta}}(s_t)\\right]\\\\\n&=-\\mathbb{E}_{\\pi_{\\theta'}}\\left[\\sum_{t=0}^\\infty \\gamma^t (\\gamma V^{\\pi_{\\theta}}(s_{t+1})- V^{\\pi_{\\theta}}(s_t))\\right]\n\\end{aligned}\n\n基于上述等式，我们可以推导新旧策略的目标函数之间的差距：\n\n\\begin{aligned}\nJ(\\theta')-J(\\theta)&=\\mathbb{E}_{s_0}[V^{\\pi_{\\theta'}}(s_0)]-\\mathbb{E}_{s_0}[V^{\\pi_{\\theta}}(s_0)]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta'}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t) \\right]+\\mathbb{E}_{\\pi_{\\theta'}}\\left[\\sum_{t=0}^\\infty \\gamma^t (\\gamma V^{\\pi_{\\theta}}(s_{t+1})- V^{\\pi_{\\theta}}(s_t))\\right]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta'}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t[r(s_t,a_t)+\\gamma V^{\\pi_{\\theta}}(s_{t+1}-V^{\\pi_{\\theta}}(s_t))] \\right]\\\\\n&=\\mathbb{E}_{\\pi_{\\theta'}}\\left[ \\sum_{t=0}^{\\infty}\\gamma^t A^{\\pi_{\\theta}}(s_t,a_t)\\right]\\\\\n&=\\sum_{t=0}^{\\infty}\\gamma^t\\mathbb{E}_{s_t\\sim P_t^{\\pi_{\\theta'}}}\\mathbb{E}_{a_t\\sim \\pi_{\\theta'}(\\cdot|s_t)}[A^{\\pi_{\\theta}}(s_t,a_t)]\\\\\n&=\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta'}}}\\mathbb{E}_{a\\sim \\pi_{\\theta'}(\\cdot|s)}[A^{\\pi_\\theta}(s,a)]\n\\end{aligned}\n 其中时许差分残差定义为优势函数A,最后一个等号的成立用到了状态访问分布的定义：\\nu^{\\pi}(s)=(1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^t P_t^{\\pi}(s),所以只要我们找到一个新策略，使得\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta'}}}\\mathbb{E}_{a\\sim \\pi_{\\theta'}(\\cdot|s)}[A^{\\pi_\\theta}(s,a)]\\geq0,就能保证策略性能单调递增，即J(\\theta')\\geq J(\\theta).\n但直接求解该式是非常困难的，因为\\pi_{\\theta'}是我们需要求解的策略，但我们又要用其来收集样本。把所有可能的新策略都拿来收集数据，然后判断哪个策略满足上述条件的做法显然是不现实的。于是 TRPO 做了一步近似操作，对状态访问分布进行了相应处理。具体而言，忽略两个策略之间的状态访问分布变化，直接采用旧的策略\\pi_{\\theta}的状态分布，定义如下替代优化目标：\n\nL_\\theta(\\theta')=J(\\theta)+\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta}}}\\mathbb{E}_{a\\sim \\pi_{\\theta'}(\\cdot|s)}[A^{\\pi_{\\theta}}(s,a)]\n\n当新旧策略非常接近时，状态访问分布变化很小，这么近似是合理的。其中，动作仍然用新策略\\pi_{\\theta'}采样得到，我们可以用重要性采样对动作分布进行处理：\n\nL_\\theta(\\theta')=J(\\theta)+\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta}}}\\mathbb{E}_{a\\sim \\pi_{\\theta}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta}(a|s)}A^{\\pi_{\\theta}}(s,a)\\right]\n\n这样，我们就可以基于旧策略\\pi_{\\theta}已经采样出的数据来估计并优化新策略\\pi_{\\theta'}了。为了保证新旧策略足够接近，TRPO使用了库尔贝克-莱布勒（Kullback-Leibler，KL）散度来衡量策略之间的距离，并给出了整体的优化公式：\n\n\\begin{aligned}\n\\max_{\\theta'}\\quad & L_{\\theta}(\\theta')\\\\\n\\text{s.t.} \\quad & \\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\left[ D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta'}(\\cdot|s)) \\right]\\leq \\delta\n\\end{aligned}\n\\tag{1}\n这里的不等式约束定义了策略空间中的一个 KL 球，被称为信任区域。在这个区域中，可以认为当前学习策略和环境交互的状态分布与上一轮策略最后采样的状态分布一致，进而可以基于一步行动的重要性采样方法使当前学习策略稳定提升。TRPO 背后的原理如@fig-TRPOshiyitu 所示。\n\n\n\n\n\n\n图 7： TRPO原理示意图\n\n\n\n左图表示当完全不设置信任区域时，策略的梯度更新可能导致策略的性能骤降；右图表示当设置了信任区域时，可以保证每次策略的梯度更新都能来带性能的提升。"
  },
  {
    "objectID": "index.html#近似求解",
    "href": "index.html#近似求解",
    "title": "强化学习基础",
    "section": "8.3 近似求解",
    "text": "8.3 近似求解\n直接求解 Equation 1 带约束的优化问题比较麻烦，TRPO在其具体视线中做了一步近似操作来快速求解。为方便起见，我们在接下来的式子中用\\theta_k来代替之前的\\theta,表示这是第k次迭代之后的策略。首先对目标函数和约束在\\theta_k进行泰勒展开，分别用1阶、2阶进行近似：\n\n\\begin{aligned}\n&\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)\\right]\\approx g^\\top(\\theta'-\\theta_k)\\\\\n&\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\left[ D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta'}(\\cdot|s)) \\right]\\approx\\frac{1}{2}(\\theta'-\\theta_k)^\\top H (\\theta'-\\theta_k)\n\\end{aligned}\n 其中g=\\nabla_{\\theta'}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim \\pi_{\\theta_k}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)\\right]表示目标函数的梯度，H=\\mathbf{H}[\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\left[ D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta'}(\\cdot|s)) \\right]]表示策略之间平均KL距离的Hessian矩阵。\n于是我们的优化目标变成了:\n\n\\theta_{k+1}=\\underset{\\theta'}{\\arg\\max} g^\\top(\\theta'-\\theta_k)\\quad \\text{s.t. }\\frac{1}{2}(\\theta'-\\theta_k)^\\top H (\\theta'-\\theta_k)\\leq \\delta\n\n此时，我们可以用Karush-Kuhn-Tucker (KKT) 条件构造拉格朗日函数后直接导出上述问题的解：\n\n\\theta_{k+1}=\\theta_k+\\sqrt{\\frac{2\\delta}{g^\\top H^{-1}g}}H^{-1}g"
  },
  {
    "objectID": "index.html#共轭梯度",
    "href": "index.html#共轭梯度",
    "title": "强化学习基础",
    "section": "8.4 共轭梯度",
    "text": "8.4 共轭梯度\n一般来说，用神经网络表示的策略函数的参数数量都是成千上万的，计算和存储黑塞矩阵的逆矩阵会耗费大量的内存资源和时间。TRPO 通过共轭梯度法（conjugate gradient method）回避了这个问题，它的核心思想是直接计算x=H^{-1}g,x即参数更新方向，假设满足KL距离约束的参数更新时的最大步长为\\beta,于是根据KL距离约束条件，有\\frac{1}{2}(\\beta x)^\\top H(\\beta x)=\\delta.求解\\beta=\\sqrt{\\frac{2\\delta}{x^\\top Hx}}.此时参数更新方式为 \n\\theta_{k+1}=\\theta_k+\\sqrt{\\frac{2\\delta}{x^\\top Hx}}x\n\n因此，只要可以直接计算x=H^{-1}g,就可以根据该式更新参数，问题转化为解Hx=g.实际上H为对称正定矩阵，所以我们可以使用共轭梯度法来求解。共轭梯度法的具体流程如下：\n\n\n\\begin{algorithm} \\caption{共轭梯度法流程} \\begin{algorithmic} \\State \\textbf{初始化}$r_0=g-Hx_0,p_0=r_0,x_0=0$ \\For{$k=0\\rightarrow N$} \\State $\\alpha_k=\\frac{r_k^\\top r_k}{p_k^\\top Hp_k}$ \\State $x_{k+1}=x_k+\\alpha_kp_k$ \\State $r_{k+1}=r_k-\\alpha_kHp_k$ \\If{$r_{k+1}^\\top r_{k+1}$\\textbf{非常小}} \\State \\textbf{退出循环} \\EndIf \\State $\\beta_k=\\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$ \\State $p_{k+1}=r_{k+1}+\\beta_kp_k$ \\EndFor \\State \\textbf{输出}$x_{N+1}$ \\end{algorithmic} \\end{algorithm}\n\n\n在共轭梯度运算过程中，直接计算\\alpha_k,r_{k+1}需要计算和存储Hessian矩阵H,为了避免这种大矩阵的出现，我们只计算Hx向量，而不直接计算和存储H矩阵。这样做比较容易，因为对于任意的列向量v,容易验证： \nHv=\\nabla_{\\theta} \\left(\\left(\\nabla_{\\theta}(D^{\\nu^{\\pi_{\\theta_k}}}_{KL}(\\pi_{\\theta_k},\\pi_{\\theta'}) )\\right)^\\top \\right)v=\\nabla_{\\theta} \\left(\\left(\\nabla_{\\theta}(D^{\\nu^{\\pi_{\\theta_k}}}_{KL}(\\pi_{\\theta_k},\\pi_{\\theta'}) )\\right)^\\top v \\right)\n\n即先用梯度和向量v点乘后再计算梯度。"
  },
  {
    "objectID": "index.html#线性搜索",
    "href": "index.html#线性搜索",
    "title": "强化学习基础",
    "section": "8.5 线性搜索",
    "text": "8.5 线性搜索\n由于TRPO算法用到了泰勒展开的一阶和二阶近似，并非精确求解，\\theta'可能未必比\\theta_k好，或未必能满足KL散度的限制。TRPO在每次迭代的最后进行一次线性搜索，以确保找到满足条件。具体来说，就是找到一个最小的非负整数i,使得按照\n\n\\theta_{k+1}=\\theta_k+\\alpha^i \\sqrt{\\frac{2\\delta}{x^\\top Hx}}x\n 求出的\\theta_{k+1}依然满足最初的KL散度限制，并且确实能够提升目标函数L_{\\theta_k},这其中\\alpha\\in(0,1)是一个决定线性搜索长度的超参数。\n至此，我们已经基本清楚了TRPO算法的大致过程，具体算法流程如下：\n\n\n\\begin{algorithm} \\caption{TRPO算法流程} \\begin{algorithmic} \\State \\textbf{初始化策略网络参数}$\\theta,$\\textbf{价值网络参数}$\\omega$ \\For{$e=1\\rightarrow E$} \\State \\textbf{用当前策略}$\\pi_{\\theta}$\\textbf{采样轨迹}$\\{s_1,a_1,r_1,s_2,a_2,r_2,\\dots\\}$ \\State \\textbf{根据收集到的数据和价值网络估计每个状态动作对的优势}$A(s_t,a_t)$ \\State \\textbf{计算策略目标函数的梯度}$g$ \\State \\textbf{用共轭梯度法计算}$x=H^{-1}g$ \\State \\textbf{用线性搜索找到一个}$i$\\textbf{值，并更新策略网络参数} \\State $\\theta_{k+1}=\\theta_k+\\alpha^i \\sqrt{\\frac{2\\delta}{x^\\top Hx}}x$,其中$i\\in\\{1,2,...,K\\}$\\textbf{为能提升策略并满足KL距离限制的最小整数} \\State \\textbf{更新价值网络参数（与A-C中的更新方法相同）} \\EndFor \\State \\textbf{输出}$x_{N+1}$ \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "index.html#广义优势估计",
    "href": "index.html#广义优势估计",
    "title": "强化学习基础",
    "section": "8.6 广义优势估计",
    "text": "8.6 广义优势估计\n从上一节内容中发现，我们尚未得知如何估计优势函数A(s_t,a_t).目前比较常用的一种方法是广义优势估计（Generalized Advantage Estimation，GAE），接下来我们简单介绍一下GAE的做法。首先，用\\delta_t=r_t+\\gamma V(s_{t+1})-V(s_t)表示时序差分误差，其中V是一个已经学习过的状态价值函数，于是根据多步时序差分的思想，有：\n\n\\begin{aligned}\nA_t^{(1)}=\\delta_t & =-V(s_t)+r_t+\\gamma V(s_{t+1})\\\\\nA_t^{(2)}=\\delta_t+\\gamma \\delta_{t+1} & =-V(s_t)+r_t+\\gamma r_{t+1}+\\gamma^2 V(s_{t+2})\\\\\n\\cdots\\\\\nA^{(k)}_t=\\sum_{l=0}^{k-1}\\gamma^l\\delta_{t+l}&= -V(s_t)+r_t+\\gamma r_{t+1}+...+\\gamma^{k-1}r_{t+k-1}+\\gamma^k V(s_{t+k})\n\\end{aligned}\n\n然后，GAE将这些不同步数的优势估计进行指数加权平均：\n\n\\begin{aligned}\nA_t^{GAE}&=(1-\\lambda)(A_t^{(1)}+\\lambda A_t^{(2)}+\\lambda^2 A_t^{(3)}+...)\\\\\n&=(1-\\lambda)(\\delta_t+\\lambda(\\delta_t+\\gamma \\delta_{t+1})+...)\\\\\n&=(1-\\lambda)[\\delta_t(1+\\lambda+\\lambda^2+...)+\\gamma\\delta_{t+1}(\\lambda+\\lambda^2+\\lambda^3+...)]\\\\\n&=(1-\\lambda)\\left( \\delta_t\\frac{1}{1-\\lambda}+\\gamma\\delta_{t+1}\\frac{\\lambda}{1-\\lambda}+... \\right)\\\\\n&=\\sum_{l=0}^{\\infty}(\\gamma\\lambda)^l\\delta_{t+l}\n\\end{aligned}\n 其中\\lambda\\in[0,1]是在GAE中额外引入的一个超参数，当\\lambda=0时，A_t^{GAE}=\\delta_t即是仅仅只看一步差分得到的优势；当\\lambda=1时，A_t^{GAE}=\\sum_{l=0}^{\\infty}\\gamma^l\\delta_{t+l}=\\sum_{l=0}^{\\infty}\\gamma^lr_{t+l}-V(s_t),则是看每一步差分得到优势的完全平均值。\n下面是一段GAE代码，给定\\gamma,\\lambda以及每个时间步的\\delta_t之后，可以根据公式直接进行优势估计（下面的代码在rl.utils.py中已给出）。\n\n\nCode\ndef compute_advantage(gamma, lmbda, td_delta):\n    td_delta = td_delta.detach().numpy()\n    advantage_list = []\n    advantage = 0.0\n    for delta in td_delta[::-1]:\n        advantage = gamma * lmbda * advantage + delta\n        advantage_list.append(advantage)\n    advantage_list.reverse()\n    return torch.tensor(advantage_list, dtype=torch.float)"
  },
  {
    "objectID": "index.html#trpo代码实践",
    "href": "index.html#trpo代码实践",
    "title": "强化学习基础",
    "section": "8.7 TRPO代码实践",
    "text": "8.7 TRPO代码实践\n首先导入一些必要的库\n\n\nCode\nimport torch\nimport numpy as np\nimport gym\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport rl_utils\nimport copy\n\n\n然后定义策略网络和价值网络以及TRPO算法，在动作时离散的情况下如下\n\n\nCode\nclass PolicyNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(PolicyNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return F.softmax(self.fc2(x), dim=1)\n\n\nclass ValueNet(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim):\n        super(ValueNet, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        return self.fc2(x)\n\n\nclass TRPO:\n    \"\"\" TRPO算法 \"\"\"\n    def __init__(self, hidden_dim, state_space, action_space, lmbda,\n                 kl_constraint, alpha, critic_lr, gamma, device):\n        state_dim = state_space.shape[0]\n        action_dim = action_space.n\n        # 策略网络参数不需要优化器更新\n        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=critic_lr)\n        self.gamma = gamma\n        self.lmbda = lmbda  # GAE参数\n        self.kl_constraint = kl_constraint  # KL距离最大限制\n        self.alpha = alpha  # 线性搜索参数\n        self.device = device\n\n    def take_action(self, state):\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        probs = self.actor(state)\n        action_dist = torch.distributions.Categorical(probs)\n        action = action_dist.sample()\n        return action.item()\n\n    def hessian_matrix_vector_product(self, states, old_action_dists, vector):\n        # 计算黑塞矩阵和一个向量的乘积\n        new_action_dists = torch.distributions.Categorical(self.actor(states))\n        kl = torch.mean(\n            torch.distributions.kl.kl_divergence(old_action_dists,\n                                                 new_action_dists))  # 计算平均KL距离\n        kl_grad = torch.autograd.grad(kl,\n                                      self.actor.parameters(),\n                                      create_graph=True)\n        kl_grad_vector = torch.cat([grad.view(-1) for grad in kl_grad])\n        # KL距离的梯度先和向量进行点积运算\n        kl_grad_vector_product = torch.dot(kl_grad_vector, vector)\n        grad2 = torch.autograd.grad(kl_grad_vector_product,\n                                    self.actor.parameters())\n        grad2_vector = torch.cat([grad.view(-1) for grad in grad2])\n        return grad2_vector\n\n    def conjugate_gradient(self, grad, states, old_action_dists):  # 共轭梯度法求解方程\n        x = torch.zeros_like(grad)\n        r = grad.clone()\n        p = grad.clone()\n        rdotr = torch.dot(r, r)\n        for i in range(10):  # 共轭梯度主循环\n            Hp = self.hessian_matrix_vector_product(states, old_action_dists,\n                                                    p)\n            alpha = rdotr / torch.dot(p, Hp)\n            x += alpha * p\n            r -= alpha * Hp\n            new_rdotr = torch.dot(r, r)\n            if new_rdotr &lt; 1e-10:\n                break\n            beta = new_rdotr / rdotr\n            p = r + beta * p\n            rdotr = new_rdotr\n        return x\n\n    def compute_surrogate_obj(self, states, actions, advantage, old_log_probs,\n                              actor):  # 计算策略目标\n        log_probs = torch.log(actor(states).gather(1, actions))\n        ratio = torch.exp(log_probs - old_log_probs)\n        return torch.mean(ratio * advantage)\n\n    def line_search(self, states, actions, advantage, old_log_probs,\n                    old_action_dists, max_vec):  # 线性搜索\n        old_para = torch.nn.utils.convert_parameters.parameters_to_vector(\n            self.actor.parameters())\n        old_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                             old_log_probs, self.actor)\n        for i in range(15):  # 线性搜索主循环\n            coef = self.alpha**i\n            new_para = old_para + coef * max_vec\n            new_actor = copy.deepcopy(self.actor)\n            torch.nn.utils.convert_parameters.vector_to_parameters(\n                new_para, new_actor.parameters())\n            new_action_dists = torch.distributions.Categorical(\n                new_actor(states))\n            kl_div = torch.mean(\n                torch.distributions.kl.kl_divergence(old_action_dists,\n                                                     new_action_dists))\n            new_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                                 old_log_probs, new_actor)\n            if new_obj &gt; old_obj and kl_div &lt; self.kl_constraint:\n                return new_para\n        return old_para\n\n    def policy_learn(self, states, actions, old_action_dists, old_log_probs,\n                     advantage):  # 更新策略函数\n        surrogate_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                                   old_log_probs, self.actor)\n        grads = torch.autograd.grad(surrogate_obj, self.actor.parameters())\n        obj_grad = torch.cat([grad.view(-1) for grad in grads]).detach()\n        # 用共轭梯度法计算x = H^(-1)g\n        descent_direction = self.conjugate_gradient(obj_grad, states,\n                                                    old_action_dists)\n\n        Hd = self.hessian_matrix_vector_product(states, old_action_dists,\n                                                descent_direction)\n        max_coef = torch.sqrt(2 * self.kl_constraint /\n                              (torch.dot(descent_direction, Hd) + 1e-8))\n        new_para = self.line_search(states, actions, advantage, old_log_probs,\n                                    old_action_dists,\n                                    descent_direction * max_coef)  # 线性搜索\n        torch.nn.utils.convert_parameters.vector_to_parameters(\n            new_para, self.actor.parameters())  # 用线性搜索后的参数更新策略\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n            self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n                                                                       dones)\n        td_delta = td_target - self.critic(states)\n        advantage = compute_advantage(self.gamma, self.lmbda,\n                                      td_delta.cpu()).to(self.device)\n        old_log_probs = torch.log(self.actor(states).gather(1,\n                                                            actions)).detach()\n        old_action_dists = torch.distributions.Categorical(\n            self.actor(states).detach())\n        critic_loss = torch.mean(\n            F.mse_loss(self.critic(states), td_target.detach()))\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()  # 更新价值函数\n        # 更新策略函数\n        self.policy_learn(states, actions, old_action_dists, old_log_probs,\n                          advantage)\n\n\n而如果是与连续动作交互的环境，需要对上面的代码做一些修改。对于策略网络，因为环境是连续动作的，所以策略网络分别输出表示动作分布的高斯分布的均值和标准差。\n\n\nCode\nclass PolicyNetContinuous(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(PolicyNetContinuous, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        mu = 2.0 * torch.tanh(self.fc_mu(x))\n        std = F.softplus(self.fc_std(x))\n        return mu, std  # 高斯分布的均值和标准差\n\n\nclass TRPOContinuous:\n    \"\"\" 处理连续动作的TRPO算法 \"\"\"\n    def __init__(self, hidden_dim, state_space, action_space, lmbda,\n                 kl_constraint, alpha, critic_lr, gamma, device):\n        state_dim = state_space.shape[0]\n        action_dim = action_space.shape[0]\n        self.actor = PolicyNetContinuous(state_dim, hidden_dim,\n                                         action_dim).to(device)\n        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=critic_lr)\n        self.gamma = gamma\n        self.lmbda = lmbda\n        self.kl_constraint = kl_constraint\n        self.alpha = alpha\n        self.device = device\n\n    def take_action(self, state):\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        mu, std = self.actor(state)\n        action_dist = torch.distributions.Normal(mu, std)\n        action = action_dist.sample()\n        return [action.item()]\n\n    def hessian_matrix_vector_product(self,\n                                      states,\n                                      old_action_dists,\n                                      vector,\n                                      damping=0.1):\n        mu, std = self.actor(states)\n        new_action_dists = torch.distributions.Normal(mu, std)\n        kl = torch.mean(\n            torch.distributions.kl.kl_divergence(old_action_dists,\n                                                 new_action_dists))\n        kl_grad = torch.autograd.grad(kl,\n                                      self.actor.parameters(),\n                                      create_graph=True)\n        kl_grad_vector = torch.cat([grad.view(-1) for grad in kl_grad])\n        kl_grad_vector_product = torch.dot(kl_grad_vector, vector)\n        grad2 = torch.autograd.grad(kl_grad_vector_product,\n                                    self.actor.parameters())\n        grad2_vector = torch.cat(\n            [grad.contiguous().view(-1) for grad in grad2])\n        return grad2_vector + damping * vector\n\n    def conjugate_gradient(self, grad, states, old_action_dists):\n        x = torch.zeros_like(grad)\n        r = grad.clone()\n        p = grad.clone()\n        rdotr = torch.dot(r, r)\n        for i in range(10):\n            Hp = self.hessian_matrix_vector_product(states, old_action_dists,\n                                                    p)\n            alpha = rdotr / torch.dot(p, Hp)\n            x += alpha * p\n            r -= alpha * Hp\n            new_rdotr = torch.dot(r, r)\n            if new_rdotr &lt; 1e-10:\n                break\n            beta = new_rdotr / rdotr\n            p = r + beta * p\n            rdotr = new_rdotr\n        return x\n\n    def compute_surrogate_obj(self, states, actions, advantage, old_log_probs,\n                              actor):\n        mu, std = actor(states)\n        action_dists = torch.distributions.Normal(mu, std)\n        log_probs = action_dists.log_prob(actions)\n        ratio = torch.exp(log_probs - old_log_probs)\n        return torch.mean(ratio * advantage)\n\n    def line_search(self, states, actions, advantage, old_log_probs,\n                    old_action_dists, max_vec):\n        old_para = torch.nn.utils.convert_parameters.parameters_to_vector(\n            self.actor.parameters())\n        old_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                             old_log_probs, self.actor)\n        for i in range(15):\n            coef = self.alpha**i\n            new_para = old_para + coef * max_vec\n            new_actor = copy.deepcopy(self.actor)\n            torch.nn.utils.convert_parameters.vector_to_parameters(\n                new_para, new_actor.parameters())\n            mu, std = new_actor(states)\n            new_action_dists = torch.distributions.Normal(mu, std)\n            kl_div = torch.mean(\n                torch.distributions.kl.kl_divergence(old_action_dists,\n                                                     new_action_dists))\n            new_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                                 old_log_probs, new_actor)\n            if new_obj &gt; old_obj and kl_div &lt; self.kl_constraint:\n                return new_para\n        return old_para\n\n    def policy_learn(self, states, actions, old_action_dists, old_log_probs,\n                     advantage):\n        surrogate_obj = self.compute_surrogate_obj(states, actions, advantage,\n                                                   old_log_probs, self.actor)\n        grads = torch.autograd.grad(surrogate_obj, self.actor.parameters())\n        obj_grad = torch.cat([grad.view(-1) for grad in grads]).detach()\n        descent_direction = self.conjugate_gradient(obj_grad, states,\n                                                    old_action_dists)\n        Hd = self.hessian_matrix_vector_product(states, old_action_dists,\n                                                descent_direction)\n        max_coef = torch.sqrt(2 * self.kl_constraint /\n                              (torch.dot(descent_direction, Hd) + 1e-8))\n        new_para = self.line_search(states, actions, advantage, old_log_probs,\n                                    old_action_dists,\n                                    descent_direction * max_coef)\n        torch.nn.utils.convert_parameters.vector_to_parameters(\n            new_para, self.actor.parameters())\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n        rewards = (rewards + 8.0) / 8.0  # 对奖励进行修改,方便训练\n        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n                                                                       dones)\n        td_delta = td_target - self.critic(states)\n        advantage = compute_advantage(self.gamma, self.lmbda,\n                                      td_delta.cpu()).to(self.device)\n        mu, std = self.actor(states)\n        old_action_dists = torch.distributions.Normal(mu.detach(),\n                                                      std.detach())\n        old_log_probs = old_action_dists.log_prob(actions)\n        critic_loss = torch.mean(\n            F.mse_loss(self.critic(states), td_target.detach()))\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n        self.policy_learn(states, actions, old_action_dists, old_log_probs,\n                          advantage)"
  },
  {
    "objectID": "index.html#简介-6",
    "href": "index.html#简介-6",
    "title": "强化学习基础",
    "section": "9.1 简介",
    "text": "9.1 简介\n上一章介绍的TRPO算法在很多场景上的应用都很成功，但是我们也发现它的计算过程非常复杂，每一步更新的运算量非常大。于是，TRPO 算法的改进版——PPO 算法在 2017 年被提出，PPO 基于 TRPO 的思想，但是其算法实现更加简单。并且大量的实验结果表明，与 TRPO 相比，PPO 能学习得一样好（甚至更快），这使得 PPO 成为非常流行的强化学习算法。如果我们想要尝试在一个新的环境中使用强化学习算法，那么 PPO 就属于可以首先尝试的算法。\n回忆一下TRPO的优化目标可以发现 TRPO 使用泰勒展开近似、共轭梯度、线性搜索等方法直接求解。PPO 的优化目标与 TRPO 相同，但 PPO 用了一些相对简单的方法来求解。具体来说，PPO 有两种形式，一是 PPO-惩罚，二是 PPO-截断，我们接下来对这两种形式进行介绍。"
  },
  {
    "objectID": "index.html#ppo-惩罚",
    "href": "index.html#ppo-惩罚",
    "title": "强化学习基础",
    "section": "9.2 PPO-惩罚",
    "text": "9.2 PPO-惩罚\nPPO-惩罚（PPO-Penalty）用拉格朗日乘数法直接将 KL 散度的限制放进了目标函数中，这就变成了一个无约束的优化问题，在迭代的过程中不断更新 KL 散度前的系数。即： \n\\underset{\\theta}{\\arg\\max}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim\\pi_{\\theta_k}(\\cdot|s)}\\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)-\\beta D_{KL}[\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta}(\\cdot|s)] \\right]\n\n令d_k=D_{KL}^{\\nu^{\\pi_{\\theta_k}}}(\\pi_{\\theta_k},\\pi_{\\theta}),\\beta的更新规则如下\n1. 如果d_k&lt;\\delta/1.5,那么\\beta_{k+1}=\\beta/2\n2. 如果d_k&lt;\\delta\\times 1.5,那么\\beta_{k+1}=\\beta\\times 2\n3. 否则\\beta_{k+1}=\\beta_k\n其中\\delta是事先设定的一个超参数，用于限制学习策略和之前一轮策略的差距。"
  },
  {
    "objectID": "index.html#ppo-截断",
    "href": "index.html#ppo-截断",
    "title": "强化学习基础",
    "section": "9.3 PPO-截断",
    "text": "9.3 PPO-截断\nPPO 的另一种形式 PPO-截断（PPO-Clip）更加直接，它在目标函数中进行限制，以保证新的参数和旧的参数的差距不会太大，即：\n\n\\underset{\\theta}{\\arg\\max}\\mathbb{E}_{s\\sim \\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim\\pi_{\\theta_k}(\\cdot|s)}\\left[\n    \\min \\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a),\n    \\text{clip}\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)},1-\\epsilon,1+\\epsilon \\right)A^{\\pi_{\\theta_k}}(s,a)\\right) \\right]\n 其中\\text{clip}(x,l,r):=\\max(\\min(x,r),l),即把x限制在[l,r]内。上式中\\epsilon是一个超参数，表示进行截断的范围。\n如果A^{\\pi_{\\theta_k}}(s,a)&gt;0说明这个动作的价值高于平均，最大化这个式子会增大\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)},但不会让其超过1+\\epsilo;反之，如果A^{\\pi_{\\theta_k}}(s,a)&lt;0说明这个动作的价值低于平均，最大化这个式子会减小\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)},但不会让其超过1-\\epsilon.如图 图 8 所示。\n\n\n\n\n\n\n图 8： PPO-截断示意图"
  },
  {
    "objectID": "index.html#倒立摆环境",
    "href": "index.html#倒立摆环境",
    "title": "强化学习基础",
    "section": "9.4 倒立摆环境",
    "text": "9.4 倒立摆环境\n下面我们介绍一下倒立摆（Inverted Pendulum）环境，该环境下有一个处于随机位置的倒立摆，如图 图 9 所示。环境的状态包括倒立摆角度的正弦值\\sin \\theta,余弦值\\cos \\theta,角速度\\dot{\\theta};动作为对倒立摆施加的力矩，详见 表 3 和 表 4 。每一步都会根据当前倒立摆的状态的好坏给予智能体不同的奖励，该环境的奖励函数为-(\\theta^2+0.1\\dot{\\theta}^2+0.001a^2),倒立摆向上保持直立不动时奖励为0，倒立摆在其他位置时奖励为负数，环境本身没有终止状态，运行200步后游戏自动结束。\n\n\n\n\n\n\n图 9： Pendulum环境示意图\n\n\n\n\n\n\n表 3： Pendulum环境的状态空间\n\n\n\n\n\n标号\n名称\n最小值\n最大值\n\n\n\n\n0\n\\cos\\theta\n-1.0\n1.0\n\n\n1\n\\sin\\theta\n-1.0\n1.0\n\n\n2\n\\dot{\\theta}\n-8.0\n8.0\n\n\n\n\n\n\n\n\n\n表 4： Pendulum环境的动作空间\n\n\n\n\n\n标号\n动作\n最小值\n最大值\n\n\n\n\n0\n力矩\n-2.0\n2.0\n\n\n\n\n\n\n力矩大小是在[-2,2]范围内的连续值。"
  },
  {
    "objectID": "index.html#ppo-代码实践",
    "href": "index.html#ppo-代码实践",
    "title": "强化学习基础",
    "section": "9.5 PPO 代码实践",
    "text": "9.5 PPO 代码实践\n下面我们在倒立摆环境中测试PPO算法，大量实验表明PPO-截断的表现总好于PPO-惩罚，因此我们下面专注于PPO-截断的代码实现。由于倒立摆是与连续动作交互的环境，同TRPO算法一样，我们让策略网络输出连续动作的高斯分布的均值和标准差，后续的连续动作则在该高斯分布中采样得到。\n\n\nCode\nimport gym\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport rl_utils\n\n\nclass PolicyNetContinuous(torch.nn.Module):\n    def __init__(self, state_dim, hidden_dim, action_dim):\n        super(PolicyNetContinuous, self).__init__()\n        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        mu = 2.0 * torch.tanh(self.fc_mu(x))\n        std = F.softplus(self.fc_std(x))\n        return mu, std\n\n\nclass PPOContinuous:\n    ''' 处理连续动作的PPO算法 '''\n    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n                 lmbda, epochs, eps, gamma, device):\n        self.actor = PolicyNetContinuous(state_dim, hidden_dim,\n                                         action_dim).to(device)\n        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n                                                lr=actor_lr)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n                                                 lr=critic_lr)\n        self.gamma = gamma\n        self.lmbda = lmbda\n        self.epochs = epochs\n        self.eps = eps\n        self.device = device\n\n    def take_action(self, state):\n        state = torch.tensor([state], dtype=torch.float).to(self.device)\n        mu, sigma = self.actor(state)\n        action_dist = torch.distributions.Normal(mu, sigma)\n        action = action_dist.sample()\n        return [action.item()]\n\n    def update(self, transition_dict):\n        states = torch.tensor(transition_dict['states'],\n                              dtype=torch.float).to(self.device)\n        actions = torch.tensor(transition_dict['actions'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        rewards = torch.tensor(transition_dict['rewards'],\n                               dtype=torch.float).view(-1, 1).to(self.device)\n        next_states = torch.tensor(transition_dict['next_states'],\n                                   dtype=torch.float).to(self.device)\n        dones = torch.tensor(transition_dict['dones'],\n                             dtype=torch.float).view(-1, 1).to(self.device)\n        rewards = (rewards + 8.0) / 8.0  # 和TRPO一样,对奖励进行修改,方便训练\n        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n                                                                       dones)\n        td_delta = td_target - self.critic(states)\n        advantage = rl_utils.compute_advantage(self.gamma, self.lmbda,\n                                               td_delta.cpu()).to(self.device)\n        mu, std = self.actor(states)\n        action_dists = torch.distributions.Normal(mu.detach(), std.detach())\n        # 动作是正态分布\n        old_log_probs = action_dists.log_prob(actions)\n\n        for _ in range(self.epochs):\n            mu, std = self.actor(states)\n            action_dists = torch.distributions.Normal(mu, std)\n            log_probs = action_dists.log_prob(actions)\n            ratio = torch.exp(log_probs - old_log_probs)\n            surr1 = ratio * advantage\n            surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage\n            actor_loss = torch.mean(-torch.min(surr1, surr2))\n            critic_loss = torch.mean(\n                F.mse_loss(self.critic(states), td_target.detach()))\n            self.actor_optimizer.zero_grad()\n            self.critic_optimizer.zero_grad()\n            actor_loss.backward()\n            critic_loss.backward()\n            self.actor_optimizer.step()\n            self.critic_optimizer.step()"
  }
]